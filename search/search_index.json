{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Software Engineering Fundamentals","text":"<p>There is nothing new in this site, that will not be anywhere else. In fact, a lot of the content here is being sourced from various authors online.</p> <p>I've been mindful of giving credit to the sources as I am making these notes. In case you feel there is a lack of credit in some place where it should have been, please feel free to point it out.</p>","tags":["software engineering","fundamentals","notes"]},{"location":"#index","title":"Index","text":"<ul> <li>Authentication</li> <li>Content Protection</li> <li>Databases</li> <li>Networking</li> </ul>","tags":["software engineering","fundamentals","notes"]},{"location":"programming-paradigms/","title":"A Journey Through Programming Paradigms","text":"<p>In the dynamic world of software development, understanding the foundations of programming is akin to wielding a versatile toolkit. Programming paradigms and algorithmic strategies serve as the guiding principles and problem-solving techniques that shape how code is written, organized, and executed. This blog embarks on a comprehensive journey through the diverse landscape of programming, exploring the significance of various paradigms and algorithmic strategies.</p>"},{"location":"programming-paradigms/#unraveling-the-concept-of-programming-paradigms","title":"Unraveling the Concept of Programming Paradigms","text":""},{"location":"programming-paradigms/#what-are-programming-paradigms","title":"What are Programming Paradigms?","text":"<p>Programming paradigms are overarching styles or approaches to software development that dictate how programmers structure and express their code. These paradigms provide a conceptual framework for solving problems, and each comes with its own set of principles, practices, and advantages.</p>"},{"location":"programming-paradigms/#the-pillars-of-programming-paradigms","title":"The Pillars of Programming Paradigms","text":""},{"location":"programming-paradigms/#1-imperative-programming-the-procedural-approach","title":"1. Imperative Programming: The Procedural Approach","text":"<p>Imperative programming guides the computer through a series of tasks step by step, akin to a recipe. C and Pascal exemplify this paradigm, offering a straightforward way to model processes.</p>"},{"location":"programming-paradigms/#2-declarative-programming-the-what-not-how","title":"2. Declarative Programming: The What, Not How","text":"<p>Declarative programming emphasizes expressing the desired outcome rather than the step-by-step instructions. SQL, HTML, and CSS are examples, allowing developers to specify what they want without detailing how to achieve it.</p>"},{"location":"programming-paradigms/#3-object-oriented-programming-oop-modeling-real-world-entities","title":"3. Object-Oriented Programming (OOP): Modeling Real-World Entities","text":"<p>OOP revolves around objects, encapsulating data and operations. Java, Python, and C++ embrace this paradigm, making it powerful for modeling complex systems.</p>"},{"location":"programming-paradigms/#4-functional-programming-treating-computation-as-mathematical-functions","title":"4. Functional Programming: Treating Computation as Mathematical Functions","text":"<p>Functional programming treats computation as mathematical functions, focusing on immutability and pure functions. Haskell, Lisp, and JavaScript (with functional features) are examples.</p>"},{"location":"programming-paradigms/#5-event-driven-programming-reacting-to-changes","title":"5. Event-Driven Programming: Reacting to Changes","text":"<p>Event-driven programming responds to events like user actions. JavaScript, especially in front-end development, embraces this paradigm with asynchronous operations and event listeners.</p>"},{"location":"programming-paradigms/#6-logic-programming-defining-rules-and-relationships","title":"6. Logic Programming: Defining Rules and Relationships","text":"<p>Logic programming expresses relationships and rules in logical statements. Prolog is an example, effective in rule-based systems and artificial intelligence.</p>"},{"location":"programming-paradigms/#7-aspect-oriented-programming-aop-modularizing-cross-cutting-concerns","title":"7. Aspect-Oriented Programming (AOP): Modularizing Cross-Cutting Concerns","text":"<p>AOP modularizes cross-cutting concerns like logging and security, enhancing code organization by separating core business logic from ancillary functionalities.</p>"},{"location":"programming-paradigms/#algorithmic-strategies-beyond-programming-paradigms","title":"Algorithmic Strategies: Beyond Programming Paradigms","text":""},{"location":"programming-paradigms/#dynamic-programming-unleashing-algorithmic-power","title":"Dynamic Programming: Unleashing Algorithmic Power","text":"<p>Dynamic Programming, while not a programming paradigm, is a vital algorithmic technique. It optimally solves complex problems by breaking them down into simpler overlapping subproblems, showcasing the power of algorithmic strategies.</p>"},{"location":"programming-paradigms/#a-multifaceted-toolbox","title":"A Multifaceted Toolbox","text":"<p>In the multifaceted world of problem-solving, developers often leverage both programming paradigms and algorithmic strategies like Dynamic Programming. These tools empower developers to craft efficient and elegant solutions to a wide array of problems, showcasing the multifaceted nature of the programming discipline.</p>"},{"location":"programming-paradigms/#conclusion-embracing-diversity-in-problem-solving","title":"Conclusion: Embracing Diversity in Problem Solving","text":"<p>In the ever-evolving landscape of software development, understanding both programming paradigms and algorithmic strategies equips developers with a diverse set of tools. Programming paradigms guide the structuring of code, and algorithmic strategies empower developers to craft efficient and elegant solutions, showcasing the multifaceted nature of the programming discipline. As we navigate this dynamic field, embracing this diversity allows us to tackle challenges with precision and creativity, contributing to the evolution of software development.</p>"},{"location":"programming-paradigms/#rest","title":"Rest","text":"<p>Object Oriented Programming -</p> <p>Functional Programing - more flexible, makes testing and debugging easier</p> <p>Core concepts:</p> <ol> <li>Immutability</li> <li>Stateless</li> </ol> <p>Functional Concepts</p> <ol> <li>Pure Functions - a function where the return value is only determined by its input values, without observable side effects</li> <li>Map, Reduce, Filter - Array helper methods, which take in functions and do not mutate the original array</li> <li>Currying - technique of translating the evaluation of a function that takes multiple arguments into evaluating a sequence of functions, each of a single argument</li> </ol> <p>Imperative Programming -</p> <p>Pure function is a function where the return value is only determined by its input values, without its observable side effects.</p> <p>Note:</p> <ul> <li>JS allows all three styles of programming</li> <li>React brings these functional programming concepts to UI</li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:aws","title":"AWS","text":"<ul> <li>            Object Storage          </li> </ul>"},{"location":"tags/#tag:azure","title":"Azure","text":"<ul> <li>            Object Storage          </li> </ul>"},{"location":"tags/#tag:gcp","title":"GCP","text":"<ul> <li>            Object Storage          </li> </ul>"},{"location":"tags/#tag:object-storage","title":"Object storage","text":"<ul> <li>            Object Storage          </li> </ul>"},{"location":"tags/#tag:storage-services","title":"Storage services","text":"<ul> <li>            Object Storage          </li> </ul>"},{"location":"tags/#tag:aws","title":"aws","text":"<ul> <li>            AWS EBS          </li> <li>            AWS EFS          </li> <li>            AWS FSx          </li> <li>            AWS File Cache          </li> </ul>"},{"location":"tags/#tag:fundamentals","title":"fundamentals","text":"<ul> <li>            Software Engineering Fundamentals          </li> </ul>"},{"location":"tags/#tag:notes","title":"notes","text":"<ul> <li>            Software Engineering Fundamentals          </li> </ul>"},{"location":"tags/#tag:software-engineering","title":"software engineering","text":"<ul> <li>            Software Engineering Fundamentals          </li> </ul>"},{"location":"tags/#tag:storage-services","title":"storage services","text":"<ul> <li>            AWS EBS          </li> <li>            AWS EFS          </li> <li>            AWS FSx          </li> <li>            AWS File Cache          </li> </ul>"},{"location":"authentication/","title":"API Authentication","text":"<p>All the doors of a house is locked, including the main door. You want a guest to be able to access the house even when you're not around, so you hand them a special key. Without this key no one can enter the house. And any person who has that special key can open any door in the house. Think of authentication the same way.</p> <p>API authentication is similar but in the world of software. You want clients to access your services through APIs, regardless of implementations like REST, gRPC, GraphQL, etc.</p> <p>Authentication can be needed for various reasons: including blocking unauthorized access completely, or even gating certain operations to specific clients only, while everyone else can access everything. </p> <p>But there's another part to this story: <code>authorization</code>. Think of it as managing who gets access to different parts of the digital space. Authentication is like having a key, and authorization is deciding which rooms you can enter once you have that key. Together, they shape how we secure and control digital interactions.</p> <p>It's important to note that authentication isn't necessary to protect against DDOS attacks. For that, techniques like <code>rate-limiting</code> requests can help.</p>"},{"location":"authentication/#index","title":"Index","text":"<ul> <li>'Basic' HTTP Authentication</li> <li>API Access Tokens</li> <li>OAuth with OpenID</li> <li>SAML Federated Identity</li> </ul>"},{"location":"authentication/#authentication-methods","title":"Authentication Methods","text":"<p>Adding authentication to APIs is crucial for securing access to resources and ensuring that only authorized users can interact with the API. There are several ways to implement authentication for APIs, depending on the requirements and the level of security needed. Here are different methods commonly used:</p> <ol> <li> <p>API Keys:</p> <ul> <li>How it works: Developers generate an API key and include it in the API request headers.</li> <li>Pros: Simple implementation, suitable for public APIs with lower security requirements.</li> <li>Cons: API keys can be easily exposed if not handled securely.</li> </ul> </li> <li> <p>OAuth 2.0:</p> <ul> <li>How it works: OAuth 2.0 is an authorization framework that enables a third-party application to obtain limited access to an HTTP service. It involves roles such as clients, resource servers, and authorization servers.</li> <li>Pros: Supports various grant types, including authorization code, implicit, client credentials, and more. Suitable for secure authentication in various scenarios.</li> <li>Cons: Requires more complex setup and understanding, might be overkill for simpler applications.</li> </ul> </li> <li> <p>JWT (JSON Web Tokens):</p> <ul> <li>How it works: A compact, URL-safe means of representing claims to be transferred between two parties. It is often used for authentication and authorization.</li> <li>Pros: Stateless, compact, and self-contained. Widely used for token-based authentication.</li> <li>Cons: Tokens can be decoded to reveal information, so they should not contain sensitive data.</li> </ul> </li> <li> <p>Basic Authentication:</p> <ul> <li>How it works: Username and password are Base64-encoded and included in the Authorization header.</li> <li>Pros: Simplicity and easy to implement.</li> <li>Cons: Credentials are sent with each request, making it vulnerable to interception if not used with HTTPS.</li> </ul> </li> <li> <p>Bearer Token:</p> <ul> <li>How it works: Similar to API keys but often used with OAuth 2.0. The token is sent in the Authorization header.</li> <li>Pros: Secure, can be easily revoked, and allows for fine-grained access control.</li> <li>Cons: Tokens need to be stored securely, and the expiration time must be managed.</li> </ul> </li> <li> <p>OpenID Connect:</p> <ul> <li>How it works: Built on top of OAuth 2.0, provides identity layer by adding an identity token.</li> <li>Pros: Standardized, widely adopted, and adds authentication capabilities to OAuth 2.0.</li> <li>Cons: Complexity, might be overkill for simple applications.</li> </ul> </li> <li> <p>Certificate-based Authentication:</p> <ul> <li>How it works: Clients present a certificate to authenticate their identity.</li> <li>Pros: Highly secure, suitable for situations where strong authentication is required.</li> <li>Cons: Complex setup and management of client certificates.</li> </ul> </li> </ol> <p>When choosing an authentication method, consider factors such as the sensitivity of data, user experience, and the level of security required for your API. It's often a good practice to use HTTPS in conjunction with any authentication method to ensure secure communication.</p>"},{"location":"authentication/#oauth-20-openid-connect","title":"OAuth 2.0 + OpenID Connect","text":"<p>OAuth 2.0 and OpenID Connect (OIDC) are often used together to provide a comprehensive identity and access management solution. OAuth 2.0 primarily deals with authorization, allowing third-party applications to access resources on behalf of a user, whereas OpenID Connect extends OAuth 2.0 to provide an identity layer.</p> <p>Here's a brief overview of how they work together:</p> <ol> <li> <p>OAuth 2.0: Authorization Framework</p> <ul> <li>Purpose: Allows secure authorization from one system to another without exposing credentials.</li> <li>Usage: Commonly used for delegated authorization scenarios, such as allowing a third-party application to access a user's resources (e.g., photos, contacts) on a resource server.</li> </ul> </li> <li> <p>OpenID Connect (OIDC): Identity Layer on Top of OAuth 2.0</p> <ul> <li>Purpose: Extends OAuth 2.0 to provide authentication and identity information.</li> <li>Usage: Focuses on user authentication and provides identity information about the end-user.</li> </ul> </li> </ol> <p>By combining OAuth 2.0 and OpenID Connect, you can achieve both authorization and authentication in a single flow. When a user wants to access a resource (OAuth flow), they are also authenticated (OIDC flow), and the application receives an ID token along with the access token. The ID token contains information about the user, providing details like their identity and potentially other profile information.</p> <p>This integration is particularly useful in scenarios where both access to resources and user identity information are required, such as in Single Sign-On (SSO) systems or when building applications that rely on user authentication and authorization. The use of OAuth 2.0 and OpenID Connect together is a common practice in modern authentication and authorization protocols.</p>"},{"location":"authentication/#todo","title":"TODO","text":"<ul> <li>API Authentication</li> <li>Ways to authentication<ul> <li>Issue with each method</li> <li>Uses</li> </ul> </li> <li>Authentication Protocols</li> </ul>"},{"location":"authentication/#protocols","title":"Protocols","text":"<ul> <li>SCRAM-SHA</li> <li>SASL-SSL</li> </ul>"},{"location":"authentication/#reference-articles","title":"Reference Articles","text":"<pre><code>https://community.getkisi.com/which-authentication-protocol-to-choose-ldap-kerberos-oauth2-saml-radius/\nhttps://learning.oreilly.com/library/view/restful-web-apis/9781449359713/ch11.html\n\nCommunication Protocols\nhttps://www.westuc.com/en-us/blog/hosted-voice-networks/ip-communication-protocols-101\nhttps://www.concise-courses.com/11-protocols/\nhttps://www.lifewire.com/definition-of-protocol-network-817949\nhttp://csfieldguide.org.nz/en/chapters/network-communication-protocols.html\nhttps://www.postscapes.com/internet-of-things-protocols/\nhttps://www.eclipse.org/community/eclipse_newsletter/2014/february/article2.php\nhttps://www.micrium.com/iot/internet-protocols/\n</code></pre>"},{"location":"authentication/01-basic-authentication/","title":"'Basic' HTTP Authentication Scheme","text":"<p>This is the most basic level of authentication. The key is <code>base64 encoded</code>, usually is derived from username and password. They key is sent in HTTP <code>Authorization</code> header of each API request, in the form <code>Basic xxxxxxxxx</code>.</p> <p>What's the point? I could've sent username and password as is. True. Anyone can base64 decode the key and extract the credentials. The reason for this authentication exists: - Something is better than nothing - Relies on the client for not sharing the key with anyone. Even though it doesn't guarantee that bad actors cannot somehow obtain the key</p> <p>Where could this authentication be used?</p>"},{"location":"authentication/01-basic-authentication/#key-rotation","title":"Key rotation","text":"<p>It is recommended to rotate your keys periodically. Some service provide this ability built-in. If the key is stolen by a bad actor</p>"},{"location":"authentication/01-basic-authentication/#replacing-key","title":"Replacing Key","text":""},{"location":"authentication/01-basic-authentication/#managing-activepassive-keys","title":"Managing active/passive keys","text":""},{"location":"authentication/01-basic-authentication/#deep-dive","title":"Deep Dive","text":"<p>HTTP Basic authentication is described in RFC 2617. It\u2019s a simple username/password scheme. The user of an API is supposed to set up a username and password ahead of time\u2014probably by registering an account on an affiliated website, or by sending an email requesting anAPI account. There\u2019s no standard for how to request a username and password for a given site.</p> <p>However it happens, once user has their username and password, they can make that original HTTP request again. This time they uses their username and password to generate a value for the request header Authorization.</p> <p>Basic Auth is simple, but it has two big problems. The first is that it\u2019s not secure. <code>YWxpY2U6cGFzc3dvcmQ=</code> looks like encrypted gibberish, but it\u2019s actually the string <code>alice:password</code> run through a simple, reversible transform called Base64. This means that anyone spying on User\u2019s Internet connection now knows their password. They can impersonate the user by sending HTTP requests that include <code>Authorization: Basic YWxpY2U6cGFzc3dvcmQ=</code></p> <p>This problem goes away if the API uses HTTPS instead of plain HTTP. Someone spying on user\u2019s Internet connection will see them open a connection, but the request and response will be encrypted by the SSL layer.</p> <p>RFC 2617 defines a second authentication method called Digest, which avoids this problem even when HTTPS is not in use. Digest and Basic share a second problem, which is not a big deal on the World Wide Web, but is very serious in the world of APIs: the people who use an API generally can\u2019t trust their clients.</p> <p>To make the problem obvious, imagine a very popular API such as the Twitter API. This API is so popular that user is using 10 different clients for this one API. There are a few on their mobile phone, a few on their desktop computer, and they\u2019re given permission to several different websites to use this API on their behalf. (This happens all the time.) Ten different clients. What happens when one of the clients goes rogue and starts posting spam to user\u2019s account? (This also happens frequently.) In the wake of the attack, User must change their password. They must do this so that the rogue client no longer has valid credentials. But they've given all 10 clients the same password. Nine of the clients are still trustworthy, but changing the password breaks all 10.</p>"},{"location":"authentication/01-basic-authentication/#rfc-7617","title":"RFC 7617","text":"<p>Defines the \"Basic\" Hypertext Transfer Protocol (HTTP) authentication scheme, which transmits credentials as user-id/password pairs, encoded using Base64.</p>"},{"location":"authentication/01-basic-authentication/#references","title":"References","text":"<ul> <li>RFC 7617 - The 'Basic' HTTP Authentication Scheme</li> <li>RFC 2617 - HTTP Authentication: Basic and Digest Access Authentication</li> </ul>"},{"location":"authentication/02-access-tokens/","title":"API Access Tokens","text":"<p>API keys have unique identifiers for each user and for every time they attempt to authenticate. Access tokens are suitable for applications where many users require access. Access tokens are secure and easy to work with from an end-user perspective.</p> <p><code>Tokens</code> are bits of information often in form of a random string. Tokens generally consist of a <code>header</code> that defines the type of token and security algorithm used; a <code>payload</code> that contains user information and metadata such as token duration and time of creation; and a <code>signature</code> to verify the sender's identity and the message's authenticity.</p>"},{"location":"authentication/02-access-tokens/#id-tokens","title":"ID Tokens","text":""},{"location":"authentication/02-access-tokens/#access-tokens","title":"Access Tokens","text":""},{"location":"authentication/02-access-tokens/#auth-tokens","title":"Auth Tokens","text":""},{"location":"authentication/02-access-tokens/#api-keys","title":"API Keys","text":""},{"location":"authentication/03-oauth/","title":"OAuth","text":"<p>OAuth (Open Authorization) is an open standard and protocol that facilitates secure authorization and access control for applications, enabling them to interact with each other on behalf of users without exposing their credentials. It is commonly used in scenarios where a third-party application needs access to a user's resources or data hosted on another service.</p> <p>The primary use case for OAuth is to allow users to grant limited access to their resources on one website or application to another, without sharing their credentials (e.g., username and password). OAuth works by providing a secure and standardized way for users to authorize third-party applications to access specific resources on their behalf.</p> <p>Here's a simplified breakdown of how OAuth works:</p> <ol> <li>User Initiates Authorization:</li> <li> <p>A user wants to grant a third-party application access to their resources on a service (e.g., social media data, photos, contacts).</p> </li> <li> <p>Requesting Authorization (OAuth Client):</p> </li> <li> <p>The third-party application, known as the OAuth client, requests authorization from the user to access specific resources.</p> </li> <li> <p>Authorization Server Comes into Play:</p> </li> <li> <p>The OAuth client redirects the user to the authorization server (which is often the service where the user's resources are hosted). The user is asked to log in and grant or deny access to the requested resources.</p> </li> <li> <p>User Grants Access:</p> </li> <li> <p>If the user approves, the authorization server issues an authorization grant to the OAuth client.</p> </li> <li> <p>OAuth Client Requests Access Token:</p> </li> <li> <p>The OAuth client uses the authorization grant to request an access token from the authorization server. This access token is a key that allows the client to access the user's resources.</p> </li> <li> <p>Access Token Granted:</p> </li> <li> <p>If the authorization server verifies the request and the user's approval, it issues an access token to the OAuth client.</p> </li> <li> <p>Accessing Resources:</p> </li> <li>The OAuth client uses the access token to authenticate itself and access the user's resources on the resource server (the server hosting the user's data).</li> </ol> <p>By using OAuth, users can grant specific permissions to third-party applications without exposing their login credentials. It enhances security, privacy, and user control over their data while enabling seamless interactions between different applications and services. OAuth 2.0 is the most widely adopted version, providing improvements and enhancements over the original OAuth 1.0.</p>"},{"location":"authentication/03-oauth/#oauth-20","title":"OAuth 2.0","text":"<p>OAuth 2.0 (Open Authorization 2.0) is a widely adopted and standardized protocol that facilitates secure authorization and delegation of access in the context of web and mobile applications. It is designed to allow third-party applications to access resources on behalf of a user without exposing their credentials. OAuth 2.0 is widely used for enabling secure API authorization and access control.</p> <p>Here's a high-level overview of how OAuth 2.0 works:</p> <ol> <li>Roles in OAuth 2.0:</li> <li>Resource Owner: The user who owns the resources (e.g., data, photos).</li> <li>Client: The third-party application requesting access to the user's resources.</li> <li>Authorization Server: The server that authenticates the user and issues access tokens.</li> <li> <p>Resource Server: The server hosting the user's resources.</p> </li> <li> <p>Authorization Grant Types:</p> </li> <li> <p>OAuth 2.0 defines several grant types that determine how the client obtains authorization. Common types include Authorization Code, Implicit, Resource Owner Password Credentials, and Client Credentials.</p> </li> <li> <p>Authorization Flow (Common Web Server Flow):</p> </li> <li>The client redirects the user to the authorization server for authentication.</li> <li>The user logs in and grants the client permission to access their resources.</li> <li> <p>The authorization server returns an authorization code to the client.</p> </li> <li> <p>Token Request:</p> </li> <li> <p>The client uses the authorization code to request an access token from the authorization server.</p> </li> <li> <p>Access Token Issued:</p> </li> <li> <p>If the authorization server verifies the request, it issues an access token to the client.</p> </li> <li> <p>Accessing Resources:</p> </li> <li> <p>The client uses the access token to authenticate itself when making requests to the resource server on behalf of the user.</p> </li> <li> <p>Refresh Token (Optional):</p> </li> <li>To obtain a new access token without involving the user, the client can use a refresh token if provided during the initial authorization.</li> </ol> <p>OAuth 2.0 provides a flexible and extensible framework, allowing implementations to tailor the protocol to specific use cases. It is widely used for securing API access in scenarios such as social media logins, allowing third-party applications to interact with user data without exposing sensitive information.</p> <p>It's important to note that while OAuth 2.0 focuses on authorization, it does not explicitly address authentication. OpenID Connect (OIDC) is an extension of OAuth 2.0 that adds an authentication layer, providing a comprehensive solution for both authorization and user authentication.</p>"},{"location":"authentication/03-oauth/#oauth-10-vs-20","title":"OAuth 1.0 vs 2.0","text":"<p>OAuth 1.0 and OAuth 2.0 are two versions of the OAuth protocol, each designed to address specific needs and challenges in securing authorization for web and mobile applications. Here are some key differences between OAuth 1.0 and OAuth 2.0:</p> <ol> <li>Security Mechanism:</li> <li>OAuth 1.0: Relies on a complex signature mechanism to ensure the integrity and authenticity of requests. It requires the use of cryptographic algorithms to sign each request, adding a layer of security.</li> <li> <p>OAuth 2.0: Simplifies the security model by relying on secure transport (e.g., HTTPS) for communication. It removes the complex signature mechanism used in OAuth 1.0, making it more straightforward to implement.</p> </li> <li> <p>Token Handling:</p> </li> <li>OAuth 1.0: Involves the use of both access tokens and request tokens. Request tokens are exchanged for access tokens, adding an extra step to the process.</li> <li> <p>OAuth 2.0: Streamlines the token handling process by using only access tokens. It eliminates the need for request tokens, making the flow more straightforward.</p> </li> <li> <p>Authorization Grant Types:</p> </li> <li>OAuth 1.0: Primarily relies on the \"three-legged\" authentication model, involving the resource owner, client, and server. It doesn't explicitly define different grant types.</li> <li> <p>OAuth 2.0: Introduces a variety of grant types, including Authorization Code, Implicit, Resource Owner Password Credentials, and Client Credentials. This provides flexibility for different use cases, such as web server flows, mobile app flows, and more.</p> </li> <li> <p>Scope of Standardization:</p> </li> <li>OAuth 1.0: Offers a comprehensive specification for secure authorization but lacks a standardized framework for token refresh and other aspects.</li> <li> <p>OAuth 2.0: Provides a more modular and extensible framework. It standardizes not only the authorization process but also includes specifications for token refresh, scope, and additional features, allowing for easier integration and adaptation to various scenarios.</p> </li> <li> <p>Complexity:</p> </li> <li>OAuth 1.0: Implementation can be more complex due to the signature mechanism, multiple token types, and a more intricate process.</li> <li> <p>OAuth 2.0: Simplifies the protocol, reducing complexity and making it more accessible for developers. However, the simplicity comes with a trade-off, as certain security responsibilities are shifted to the transport layer.</p> </li> <li> <p>Adoption and Compatibility:</p> </li> <li>OAuth 1.0: Used in various applications but has seen a decline in adoption in favor of OAuth 2.0 due to its complexity.</li> <li>OAuth 2.0: Widely adopted and considered the modern standard for secure authorization. It is the go-to choice for new applications and APIs.</li> </ol> <p>In summary, OAuth 2.0 is a more streamlined, flexible, and widely adopted version of the OAuth protocol, addressing the shortcomings and complexities of OAuth 1.0. It has become the standard choice for securing authorization in modern web and mobile applications.</p>"},{"location":"authentication/03-oauth/#oauth-10","title":"OAuth 1.0","text":"<p>Under OAuth, user gives each client an individual set of credentials. If they decides they don\u2019t like one of the clients, they can revokes its credentials, and the other nine clients are unaffected. If a client goes rogue and starts posting spam under the names of its users, the service provider can step in and revoke the credentials for every instance of that client\u2019s and everyone else\u2019s.</p> <p>There are two versions of OAuth. OAuth 1.0 (defined in RFC 5849) works well for allowing the developers of consumer-facing websites to integrate with your API. It starts falling apart when you want to allow the integration of desktop, mobile, or in-browser applications with your API. OAuth 2.0 is very similar to 1.0, but it defines ways to handle these scenarios.</p>"},{"location":"authentication/03-oauth/#references","title":"References","text":"<ul> <li>RFC 5849 | OAuth 1.0</li> <li>https://developer.box.com/reference/post-oauth2-token/</li> <li>https://developers.google.com/identity/protocols/oauth2</li> <li>https://www.oauth.com/oauth2-servers/access-tokens/</li> <li>https://learning.postman.com/docs/sending-requests/authorization/</li> <li>https://developer.twitter.com/en/docs/authentication/oauth-2-0/user-access-token</li> </ul>"},{"location":"authentication/authentication-protocols/","title":"A Dive into Authentication Protocols","text":"<p>Authentication in the digital realm is facilitated by a variety of protocols, each tailored to specific needs and scenarios. Let's explore some prominent authentication protocols that play crucial roles in securing digital interactions:</p> <ol> <li>OAuth (Open Authorization):</li> <li>Purpose: Enables third-party applications to access resources on behalf of a user securely.</li> <li> <p>Usage: Commonly employed for delegated authorization, allowing secure access to user data across services.</p> </li> <li> <p>OpenID Connect (OIDC):</p> </li> <li>Purpose: An extension of OAuth 2.0 that provides an identity layer for user authentication.</li> <li> <p>Usage: Often used in conjunction with OAuth 2.0, especially in Single Sign-On (SSO) scenarios, to obtain standardized user identity information.</p> </li> <li> <p>SAML (Security Assertion Markup Language):</p> </li> <li>Purpose: An XML-based, open-standard data format facilitating exchange of authentication and authorization data between parties, in particular, between an identity provider and a service provider, generally in web browser-based Single Sign-On (SSO) systems.</li> <li> <p>Usage: Widely implemented in enterprise environments for federated identity management.</p> </li> <li> <p>Kerberos:</p> </li> <li>Purpose: A network authentication protocol, ensuring secure authentication in client-server environments through the use of tickets.</li> <li> <p>Usage: Commonly utilized in enterprise networks, particularly in Microsoft Windows environments, for single sign-on and mutual authentication.</p> </li> <li> <p>LDAP (Lightweight Directory Access Protocol):</p> </li> <li>Purpose: A protocol for accessing and maintaining distributed directory information services, often used for user authentication.</li> <li> <p>Usage: Frequently employed in enterprise environments for centralized user management and authentication.</p> </li> <li> <p>JWT (JSON Web Tokens):</p> </li> <li>Purpose: A compact means of representing claims for secure information exchange and authentication.</li> <li> <p>Usage: Widely adopted in web development and microservices architectures for stateless authentication.</p> </li> <li> <p>RADIUS (Remote Authentication Dial-In User Service):</p> </li> <li>Purpose: A network protocol for remote user authentication and accounting, commonly used for network access. Provides centralized Authentication, Authorization, and Accounting (AAA or Triple A) management for users who connect and use a network service.</li> <li> <p>Usage: Deployed in networking scenarios, such as dial-up and VPN access.</p> </li> <li> <p>OAuth 2.0 Device Authorization Grant:</p> </li> <li>Purpose: Allows devices with limited input capabilities to obtain user authorization securely.</li> <li>Usage: Useful in scenarios involving devices like smart TVs or Internet of Things (IoT) devices that require user authentication.</li> </ol> <p>Additionally, the Simple Authentication and Security Layer (SASL) framework provides a flexible and extensible authentication mechanism, often incorporated into various protocols like LDAP, SMTP, IMAP, and XMPP. SASL supports multiple authentication mechanisms, ensuring adaptability to diverse authentication requirements.</p> <p>These protocols collectively form a robust authentication landscape, offering solutions for a wide range of use cases, from securing user access to facilitating secure data exchange in digital ecosystems.</p>"},{"location":"authentication/pkce/","title":"Proof Key for Code Exchange (PKCE)","text":"<p>Proof Key for Code Exchange (PKCE) is a security enhancement for the OAuth 2.0 authorization framework. It is specifically designed to address security concerns in scenarios where the traditional authorization code flow might be susceptible to interception attacks, particularly in mobile and native applications.</p> <p>Here's a breakdown of PKCE:</p> <ol> <li> <p>Background:    OAuth 2.0 authorization code flow involves exchanging an authorization code for an access token. However, in certain situations, malicious actors could intercept the authorization code during the callback to the client.</p> </li> <li> <p>Challenge-Response Mechanism:    PKCE introduces a challenge-response mechanism to secure the authorization code flow. Instead of sending the client secret with the authorization request (as in traditional OAuth flows), PKCE dynamically generates a secret for each authorization request.</p> </li> <li> <p>Code Verifier and Code Challenge:    Before initiating the authorization process, the client generates a random value called the \"code verifier.\" It then transforms this verifier into a \"code challenge\" using a one-way cryptographic function, commonly SHA-256.</p> </li> <li> <p>Authorization Request:    The client includes the code challenge in the authorization request sent to the authorization server.</p> </li> <li> <p>Authorization Code Response:    If the authorization server approves the request, it returns an authorization code to the client.</p> </li> <li> <p>Token Exchange:    When exchanging the authorization code for an access token, the client includes the original code verifier. The authorization server verifies that the code verifier matches the one used to create the code challenge.</p> </li> </ol> <p>By employing PKCE, even if an attacker intercepts the authorization code, they won't be able to exchange it for an access token without the corresponding code verifier. PKCE adds an extra layer of security, especially in situations where client secrets cannot be reliably protected (e.g., in mobile or native applications).</p> <p>In summary, PKCE is a security measure that helps prevent certain types of attacks in OAuth 2.0 authorization flows, making them more resilient in scenarios where code interception is a potential risk.</p>"},{"location":"authentication/pkce/#references","title":"References","text":"<ul> <li>PKCE RFC 7636</li> </ul>"},{"location":"authentication/sasl/","title":"SASL","text":"<p>SASL (<code>Simple Authentication and Security Layer</code>) is another authentication framework commonly used in various protocols to provide a flexible and extensible mechanism for authentication. SASL itself is not a standalone authentication protocol but a framework that allows different protocols to incorporate a variety of authentication mechanisms.</p> <p>SASL is often utilized in conjunction with protocols such as LDAP (Lightweight Directory Access Protocol), SMTP (Simple Mail Transfer Protocol), IMAP (Internet Message Access Protocol), and XMPP (Extensible Messaging and Presence Protocol), among others. It supports a wide range of authentication mechanisms, including:</p> <ol> <li>PLAIN: Basic username and password authentication.</li> <li>CRAM-MD5: Challenge-Response Authentication Mechanism with MD5 hashing.</li> <li>DIGEST-MD5: Similar to CRAM-MD5 but uses more secure digest hashing.</li> <li>GSSAPI (Generic Security Services Application Program Interface): Supports Kerberos authentication and other GSSAPI mechanisms.</li> <li>EXTERNAL: Allows a client to authenticate using credentials established by an external entity.</li> </ol> <p>The versatility of SASL lies in its ability to negotiate and support different authentication mechanisms based on the capabilities of the client and server. It enables secure authentication in various network protocols, making it a valuable component in many authentication scenarios.</p>"},{"location":"authentication/totp/","title":"TOTP","text":"<p><code>Time-based One-Time Password</code> (TOTP) is a type of <code>two-factor authentication</code> that involves the generation of one-time passwords based on a time factor. It's commonly used to enhance the security of user logins, especially in online services and applications.</p> <p>Here's how TOTP typically works:</p> <ol> <li>Secret Key Generation:</li> <li> <p>The user is initially provided with a secret key during the setup process.</p> </li> <li> <p>Time Factor:</p> </li> <li> <p>The TOTP algorithm uses the current time as a factor to generate a unique, time-dependent one-time password.</p> </li> <li> <p>Password Generation:</p> </li> <li> <p>The user's device (such as a mobile app) generates a new password based on the secret key and the current time.</p> </li> <li> <p>Authentication:</p> </li> <li> <p>The user enters this generated one-time password along with their regular credentials during the login process.</p> </li> <li> <p>Verification:</p> </li> <li>The server, which also knows the secret key, independently generates the expected one-time password based on the current time. If the entered password matches the expected one, authentication is successful.</li> </ol> <p>TOTP is commonly used in scenarios where an additional layer of security is desired. It doesn't involve any network communication in its basic form; instead, it relies on the synchronization of time between the user's device and the server. TOTP is often implemented in mobile apps or hardware tokens and is a part of multi-factor authentication strategies to enhance security.</p>"},{"location":"authentication/two-factor/","title":"Two-factor Authentication (2FA)","text":"<p>Strengthening Identity Verification with Diverse Layers</p> <p>In the realm of digital security, Two-Factor Authentication (2FA) stands as a robust mechanism, employing two distinct factors to fortify the verification process. Various types of 2FA exist, each adding an extra layer of protection:</p> <ol> <li> <p>SMS-Based 2FA:    Commonly adopted due to its simplicity and wide accessibility, despite some security concerns related to SIM swapping.</p> </li> <li> <p>Email-Based 2FA:    Widely used for its convenience, particularly in scenarios where users prefer not to rely on SMS or authenticator apps.</p> </li> <li> <p>TOTP (Time-based One-Time Password):    Gaining popularity for its security and ease of use, often implemented through authenticator apps like Google Authenticator or Authy.</p> </li> <li> <p>Biometric Authentication:    Increasingly popular, especially on mobile devices, leveraging fingerprints, facial recognition, or other biometric features.</p> </li> <li> <p>Push Notifications:    Common in mobile app authentication, providing a seamless user experience with prompt approval or denial.</p> </li> <li> <p>Hardware Tokens:    Trusted for its high level of security, particularly in corporate or high-risk environments, but less common for general consumer use.</p> </li> <li> <p>Backup Codes:    Used as a fallback option for other 2FA methods, ensuring access in case primary methods are unavailable.</p> </li> <li> <p>Smart Cards:    Frequently employed in enterprise environments with stringent security requirements, less common in everyday consumer scenarios.</p> </li> <li> <p>FIDO (Fast Identity Online):    Gaining traction, especially in passwordless authentication, but may vary in adoption depending on the platform and service.</p> </li> <li> <p>Location-Based Authentication:     Less common as a standalone method but may complement other forms of 2FA to enhance security.</p> </li> <li> <p>Behavioral Biometrics:     Emerging as a promising authentication method, but its widespread adoption is still in the early stages.</p> </li> <li> <p>Certificate-Based Authentication:     Common in enterprise settings, less prevalent in consumer-oriented applications.</p> </li> <li> <p>Knowledge-Based Authentication (KBA):     Often used as an additional layer, particularly for account recovery, but has limitations due to the potential for socially engineered attacks.</p> </li> </ol> <p>These diverse 2FA methods cater to different security needs and use cases, ensuring a multi-layered defense against unauthorized access. The combination of two factors, whether through something the user knows and something they have, enhances overall security, safeguarding user accounts across a spectrum of online services and applications. Combining different factors adds an extra layer of security by requiring attackers to compromise multiple elements to gain unauthorized access.</p>"},{"location":"cloud/aws-ebs/","title":"AWS EBS","text":"<p>Amazon <code>Elastic Block Store</code> (Amazon EBS) is an easy-to-use, scalable, high-performance block-storage service designed for Amazon Elastic Compute Cloud (Amazon EC2).</p>","tags":["aws","storage services"]},{"location":"cloud/aws-ebs/#references","title":"References","text":"<ul> <li>Official AWS EBS Overview</li> </ul>","tags":["aws","storage services"]},{"location":"cloud/aws-efs/","title":"AWS EFS","text":"<p>Amazon <code>Elastic File System</code> (EFS) automatically grows and shrinks as you add and remove files with no need for management or provisioning.</p>","tags":["aws","storage services"]},{"location":"cloud/aws-efs/#references","title":"References","text":"<ul> <li>Official AWS EFS Overview</li> </ul>","tags":["aws","storage services"]},{"location":"cloud/aws-file-cache/","title":"AWS File Cache","text":"<p>Amazon <code>File Cache</code> provides a high-speed cache on AWS that makes it easier to process file data, regardless of where it\u2019s stored. Amazon File Cache serves as temporary, high-performance storage for data on premises or on AWS. The service allows you to make dispersed datasets available to file-based applications on AWS with a unified view and high speeds.</p>","tags":["aws","storage services"]},{"location":"cloud/aws-file-cache/#references","title":"References","text":"<ul> <li>Official AWS File Cache Overview</li> </ul>","tags":["aws","storage services"]},{"location":"cloud/aws-fsx/","title":"AWS FSx","text":"<p>Amazon <code>FSx</code> makes it easy and cost effective to launch, run, and scale feature-rich, high-performance file systems in the cloud. It supports a wide range of workloads with its reliability, security, scalability, and broad set of capabilities. Amazon FSx is built on the latest AWS compute, networking, and disk technologies to provide high performance and lower TCO. And as a fully managed service, it handles hardware provisioning, patching, and backups -- freeing you up to focus on your applications, your end users, and your business.</p> <p>You can choose between four widely-used file systems: NetApp ONTAP, OpenZFS, Windows File Server, and Lustre.</p>","tags":["aws","storage services"]},{"location":"cloud/aws-fsx/#references","title":"References","text":"<ul> <li>Official AWS FSx Overview</li> </ul>","tags":["aws","storage services"]},{"location":"cloud/object-storage/","title":"Object Storage","text":"<p>TBD</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#use-cases","title":"Use Cases","text":"<p>TBD</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#cloud-offerings","title":"Cloud Offerings","text":"","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#aws-s3","title":"AWS S3","text":"<p>Amazon <code>Simple Storage Service</code> (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. With cost-effective storage classes and easy-to-use management features, you can optimize costs, organize data, and configure fine-tuned access controls to meet specific business, organizational, and compliance requirements.</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.</p> <p>Blob storage is massively scalable and secure object storage for cloud-native workloads, archives, data lakes, high-performance computing, and machine learning.</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#gcp-cloud-storage","title":"GCP Cloud Storage","text":"<p><code>Cloud Storage</code> is a managed service for storing unstructured data. Store any amount of data and retrieve it as often as you like.</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#storage-tiers","title":"Storage Tiers","text":"<p>TBD</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#common-features","title":"Common Features","text":"<p>TBD</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#security","title":"Security","text":"<p>TBD</p>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"cloud/object-storage/#references","title":"References","text":"<ul> <li>Official AWS S3 Overview</li> <li>Official Azure Blob Storage<ul> <li>Introduction to Azure Blob Storage</li> </ul> </li> <li>Official GCP Cloud Storage<ul> <li>What is Object storage? | GCP</li> </ul> </li> <li>What is Object storage? | IBM</li> <li>Object storage | Wiki</li> </ul>","tags":["Storage services","Object storage","AWS","GCP","Azure"]},{"location":"communication/","title":"Communication","text":"<p>\u201cNothing in life is more important than the ability to communicate effectively.\u201d \u2013 Gerald R. Ford, former United States president</p> <pre><code>_noun_\n\n1. the imparting or exchanging of information or news.\n   \"at the moment I am in communication with London\"\n\n2. means of sending or receiving information, such as phone lines or computers.\n   \"satellite communications\"\n</code></pre>"},{"location":"communication/#how-humans-communicate","title":"How humans communicate?","text":"<p>For a person to communicate with another, both of them need:</p> <ul> <li>a shared character set</li> <li>a shared dictionary</li> <li>Transmission mode</li> <li>a way to know who the person speaking is</li> <li>a way to know who the person listening is</li> </ul>"},{"location":"communication/#shared-char-set","title":"Shared char set","text":"<p>A character means nothing to a person when they don't understand it. It also means nothing  Character set is the lowest form  If you were to say \"H@ppy\"</p>"},{"location":"communication/#shared-dictionary","title":"Shared Dictionary","text":"<p>Just having a known char set doesn't help much in communication, unless there is a dictionary to refer to. If you were to say \"ajsgdyf\" to me, it wouldn't mean anything to me, but saying \"happy\" means something to me because you and I refer to the same english dictionary and speak/understand what it means.</p>"},{"location":"communication/#transmission-mode","title":"Transmission Mode","text":"<p>If person \"A\" writes a letter, but person \"B\" can only hear words, they'll never be able to communicate unless \"A\" changes the mode of transmitting its message. Both sender and receiver need to be able to agree upon the mode of transmission of the message.</p>"},{"location":"communication/#who-sent-the-message","title":"Who sent the message?","text":"<p>If person \"B\" receives a letter, there must be a way to know who sent the letter, or at the least a way to verify if it was \"A\", and really \"A\".</p>"},{"location":"communication/#did-you-get-my-message","title":"Did you get my message?","text":"<p>Just sending a message to \"B\" though agreed upon transmission mode does not guarantee \"A\" that \"B\" actually received the message. Maybe it got intercepted by \"C\" or \"A\" had the wrong address.</p>"},{"location":"communication/#how-machines-communicate","title":"How machines communicate?","text":"<p>Bytes (a set of binary numbers), put together, they may mean something to someone, another thing to someone else. When bytes are put together, it may be referred as data. A machine is capable of <code>transmitting/receiving</code> data in form of bytes to/fron another machine. It's same like we know characters. Characters put together make words, which makes sense just because we've learnt the dictionary.</p> <p>Transmitting data is one action. How would the data make sense to the one who is sending it or to the one receiving it?</p> <p>The only way a word may make sense to us is by knowing two things:</p> <ol> <li>Language that the word is written in</li> <li>Word exists in our dictionary (for the language), which we refer to make sense out of what the word mean.</li> </ol> <p>Similarly, for two entities/computers to communicate with each other and understand each other, they need a language to agree on. Think of it like a contract between systems, where one says \"X\", and it must mean something to another system. This language is what a <code>communication protocol</code> is. Fortunatley a computer's character set is defined and limited to 0's and 1's (binary data).</p>"},{"location":"communication/#forms-of-communication","title":"Forms of communication","text":"<p>There are indeed various forms of communication on internet including <code>email</code>, <code>text message</code> (SMS), <code>media-messaging</code> (MMS), <code>IRC chat</code>, <code>instant-message</code> (chat), <code>VoIP</code>, <code>video-calling</code>, <code>livestream</code>, etc. Telecommunication systems use <code>Simplex</code>, <code>Half-Duplex</code> and <code>Full-Duplex</code> channels. All of these are out of scope.</p> <p>Choosing the appropriate channel for communicating your message is critical to how that message will be received and acted upon.</p>"},{"location":"communication/#communication-components","title":"Communication Components","text":"<p>To work, communication requires a few core components:</p> <ol> <li>Destination address</li> <li>Transmission method</li> <li>Protocol</li> <li>Connection</li> </ol>"},{"location":"communication/#references","title":"References","text":"<ul> <li>https://www.kionetworks.com/en-us/blog/data-center/network-communication-protocols</li> <li>https://www.ibm.com/docs/en/aix/7.2?topic=communication-asynchronous-methods</li> <li>https://www.panopto.com/blog/learning-and-development-asynchronous-vs-synchronous-video-communications-whats-the-difference/</li> <li>https://www.loom.com/blog/synchronous-vs-asynchronous</li> <li>https://stackoverflow.com/questions/23419469/is-http-1-1-full-duplex</li> </ul>"},{"location":"communication/01-transmission-method/","title":"Transmission","text":""},{"location":"communication/01-transmission-method/#types","title":"Types","text":""},{"location":"communication/01-transmission-method/#simplex","title":"Simplex","text":""},{"location":"communication/01-transmission-method/#half-duplex","title":"Half-Duplex","text":""},{"location":"communication/01-transmission-method/#duplex-or-full-duplex","title":"Duplex or Full-Duplex","text":""},{"location":"communication/01-transmission-method/#synchronous-transmission","title":"Synchronous Transmission","text":"<p>The term <code>synchronous</code> is used to describe a continuous and consistent timed transfer of data blocks.</p> <p>These types of connections are used when large amounts of data must be transferred very quickly from one location to the other. The <code>speed</code> of the synchronous connection is attained by transferring data in large blocks instead of individual characters, which are then reassembled at the remote location.</p> <p>All devices on the synchronous link must be set to the same clocking.</p>"},{"location":"communication/01-transmission-method/#examples","title":"Examples","text":"<ul> <li>Chat Rooms</li> <li>Telephonic Conversations</li> <li>Video Conferencing </li> </ul>"},{"location":"communication/01-transmission-method/#asynchronous-transmission","title":"Asynchronous Transmission","text":"<p>In asynchronous aransmission, data is sent in form of byte or character. This transmission is the <code>half-duplex</code> type transmission.</p> <p>The term <code>asynchronous</code> is used to describe the process where transmitted data is encoded with start and stop bits, specifying the beginning and end of each character. These additional bits provide the timing or synchronization for the connection by indicating when a complete character has been sent or received; thus, timing for each character begins with the start bit and ends with the stop bit.</p>"},{"location":"communication/01-transmission-method/#examples_1","title":"Examples","text":"<ul> <li>Email</li> <li>Forums</li> <li>Letters</li> </ul>"},{"location":"communication/01-transmission-method/#references","title":"References","text":"<ul> <li>https://www.geeksforgeeks.org/difference-between-synchronous-and-asynchronous-transmission/</li> </ul>"},{"location":"communication/02-protocols/","title":"Communication Protocol","text":"<p>A communication protocol is a formal descriptions of digital message formats and rules, that allows two or more entities to transmit information of any kind. The protocol defines the rules, syntax, semantics and synchronization of communication and possible error recovery methods, (a.k.a. language). Communication protocols have to be agreed upon by the parties involved. Communicating systems use well-defined formats for exchanging various messages. Protocols may be implemented by hardware, software, or a combination of both.</p>"},{"location":"communication/02-protocols/#protocol-stack","title":"Protocol Stack","text":"<p>Multiple protocols can be used to describe different aspects of a single communication, known as a protocol stack/suite.</p> <p>Examples of Protocol stacks</p> Protocol Layer HTTP Application TCP Transport IP Internet or network Ethernet Link or data link IEEE 802.3ab Physical <p></p>"},{"location":"communication/02-protocols/#designing-a-protocol","title":"Designing a protocol","text":"<p>Let's say there is no protocol in the world that fits what I need. A protocol is just a contract that two systems agree to communicate on. Nothing stops me from building my own protocol.</p> <p>While designing a protocol is easy, however desire to have it's widespread adoption is equally challenging. The only scenarios were someone would go this route are:</p> <ol> <li>There is a real use case that requires a new protocol, that no one in the world has come across</li> <li>You want your properitary hardware to communicate over unidentifiable communication method.</li> </ol> <p>To complicate this further, none of the existing code/library will be able to read data based on the custom protocol, hence requiring a lot of code to be written from ground up.</p>"},{"location":"communication/02-protocols/#standards-governance","title":"Standards Governance","text":"<p>IETF - Internet Engineering Task Force is a standards organization for the internet and is responsible for the technical standards that make up the Internet protocol suite (TCP/IP) and publishes internet communication protocols.</p> <p>IEEE - Institute of Electrical and Electronics Engineers handles wired and wireless networking</p> <p>ISO - International Organization for Standardization handles other types</p> <p>ITU-T - handles telecommunications protocols and formats for the public switched telephone network (PSTN)</p>"},{"location":"communication/02-protocols/#organizations-to-know","title":"Organizations to know","text":"<ul> <li>IETF - Internet Engineering Task Force</li> <li>IESG - Internet Engineering Steering Group</li> <li>IAB - Internet Architecture Board</li> <li>IRTF - Internet Research Task Force</li> </ul>"},{"location":"communication/02-protocols/#references","title":"References","text":"<ul> <li>Wiki - Communication Protocol</li> <li>Techopedia Communication Protocol explained blog</li> <li>WebDAV Protocol</li> <li>Constrained Application Protocol overview</li> <li>Wiki - Constrained Application Protocol</li> <li>Designing a protocol</li> <li>National Cyber Security Centre - Protocol Design Principles whitepaper</li> <li>Designing your own protocol in 5 minutes blog</li> <li>Quora How to design protocol?</li> </ul>"},{"location":"communication/03-network-protocols/","title":"Network Protocols","text":"<p>When it comes to communication among applications, all of it is done on either TCP or UDP or even both protocols. Both of these protocols are at Transport layer level (L4).</p>"},{"location":"communication/03-network-protocols/#basics-of-protocols","title":"Basics of Protocols","text":""},{"location":"communication/03-network-protocols/#tcp-transport-control-protocol-connection-oriented-data-delivery-is-guaranteed","title":"TCP - Transport Control Protocol. Connection-oriented. Data delivery is guaranteed.","text":"<ul> <li>FTP [20/21] (File Transfer Protocol) - </li> <li>TFTP [69] (Trivial File Transfer Protocol) - Does not ask for user-id pwd</li> <li>SFTP [22] (Secure File Transfer Protocol) - Information is sent encrypted</li> <li>SSH [22] (Secure Shell) - To connect to another computer in a secured way</li> <li>Telnet [23] - To connect to another computer in unsecured way</li> <li>SMTP [25/465] (Simple Mail Transfer Protocol) - Mail protocol. Mail server to mail server.</li> <li>Second port is used when using secured connection with encryption TLS/SSL</li> <li>IMAP4 [143/993] (Internet Message Access Protocol) - Mail protocol. Downloads a copy of message from the server</li> <li>POP3 [110/995] (Post Office Protocol) - Mail protocol. Downloads original message from the server, deleting from the server.</li> <li>HTTP [80] (Hypertext Transfer Protocol) - </li> <li>HTTPS [443] (Hypertext Transfer Protocol Secure) - Uses SSL over HTTP</li> </ul>"},{"location":"communication/03-network-protocols/#udp-user-datagram-protocol-connection-less-data-delivery-is-not-guaranteed","title":"UDP - User Datagram Protocol. Connection-less. Data delivery is not guaranteed.","text":"<ul> <li>SNMP [161/162] (Simple Network Management Protocol) - Used to sends infrastructure operation information</li> <li>NTP [123] (Network Time Protocol) - Server-sync is done over it</li> <li>SIP [5060/5061] (Session Initiation Protocol) - Video and Voice</li> <li>RTSP [554] (Real Time Streaming Protocol) - Stream media like YouTube or Hulu</li> <li>DHCP [67/68] (Dynamic host Configuration Protocol) - Provides an IP address to a computer/hardware</li> </ul>"},{"location":"communication/03-network-protocols/#both-tcp-andor-udp","title":"Both (TCP and/or UDP)","text":"<ul> <li>LDAP [389] (Lightweight Directory Access Protocol) - </li> <li>RDP [3389] (Remote Desktop Protocol) - Uses Windows. </li> <li>DNS [53] (Domain Name System) - </li> </ul>"},{"location":"communication/03-network-protocols/#list-of-protocols","title":"List of Protocols","text":"<ul> <li>Ethernet Protocol - Link</li> <li>IP - Internet<ul> <li>UDP - Transport</li> <li>TCP - Transport</li> <li>HTTP - Application</li> <li>POP3 - Application</li> </ul> </li> </ul> <p>TLS (Transport Layer Security)</p> <p>SSL (Secure Socket Layer)</p>"},{"location":"communication/03-network-protocols/#communication-protocols","title":"Communication Protocols","text":"<ul> <li>P4</li> <li>LDAP</li> <li>Telnet</li> <li>HTTP</li> <li>RFC</li> <li>JDBC</li> <li>Session</li> </ul>"},{"location":"communication/03-network-protocols/#api-protocols","title":"API Protocols","text":"<ul> <li>SOAP</li> <li>REST</li> <li>JSON-RPC</li> <li>gRPC</li> <li>GraphQL</li> <li>Thrift</li> </ul>"},{"location":"communication/03-network-protocols/#iot-protocols","title":"IoT Protocols","text":"<ul> <li>MQTT</li> <li>AMQP</li> <li>REST</li> <li>OPC UA</li> <li>DHTP</li> </ul>"},{"location":"communication/04-osi-model/","title":"OSI Model","text":"<p>Open Systems Interconnection (OSI) provides a common basis for system interconnection. In the OSI reference model, the communication between computing systems are split into seven different abstraction layers.</p> <p></p> <p>At each layer, two entities exchange data by means of a protocol.</p>"},{"location":"communication/04-osi-model/#how-data-processing-works","title":"How Data Processing works?","text":"<p>Data processing by two communicating OSI-compatible devices proceeds as follows:</p> <ol> <li>The data to be transmitted is composed at the topmost layer of the transmitting device (layer N) into a protocol data unit (PDU).</li> <li>The PDU is passed to layer N-1, where it is known as the service data unit (SDU).</li> <li>At layer N-1 the SDU is concatenated with a header, a footer, or both, producing a layer N-1 PDU. It is then passed to layer N-2.</li> <li>The process continues until reaching the lowermost level, from which the data is transmitted to the receiving device.</li> <li>At the receiving device the data is passed from the lowest to the highest layer as a series of SDUs while being successively stripped from each layer's header or footer until reaching the topmost layer, where the last of the data is consumed.</li> </ol> <p>Note: The lesser number of layers a protocol makes the data travel, it'll have reducing effect on the overall latency.</p>"},{"location":"communication/04-osi-model/#abstraction-layers-tabular","title":"Abstraction Layers (tabular)","text":"Classification Packet Data Unit Layer Description Protocols Host Layers Data Application Layer (L7) Network process to application HTTP, FTP, DNS, SNMP, Telnet, SMTP, IMAP, POP3, DCHP Host Layers Data Presentation Layer Data representation, Compression and Encryption TLS, SSL, MPEG, ASCII chars, Compression Host Layers Data Session Layer Interhost Communication NetBIOS, PPTP, SAP, RPC, SQL Host Layers Segments Transport Layer (L4) End-to-End connections and Reliability TCP, UDP Media Layers Packets Network Layer Path Determination and IP (Logical Addressing) IPV4, IPV6, ARP, ICMP (ping), IPSec, MPLS Media Layers Frames Data Link Layer MAC and LLC (Physical Addressing) PPP, ATM, Ethernet, MPLS, 802.1x, FDDI, MAC address, Fiber Channel Media Layers Bits Physical Layer (L1) Media, Signal and Binary transmission Ethernet, USB, Bluetooth, IEEE802.11, Cables, Connectors, Hubs"},{"location":"communication/04-osi-model/#layer-architecture","title":"Layer Architecture","text":"Layer Protocol Data Unit Function L7 Application Data High-level protocols such as for resource sharing or remote file access, e.g. HTTP. L6 Presentation Data Translation of data between a networking service and an application; including character encoding, data compression and encryption/decryption L5 Session Data Managing communication sessions, i.e., continuous exchange of information in the form of multiple back-and-forth transmissions between two nodes L4 Transport Segment, Datagram Reliable transmission of data segments between points on a network, including segmentation, acknowledgement and multiplexing L3 Network Packet Structuring and managing a multi-node network, including addressing, routing and traffic control L2 Data Link Frame Transmission of data frames between two nodes connected by a physical layer L1 Physical Bit, Symbol Transmission and reception of raw bit streams over a physical medium <p>Read more about each layer from the Wiki</p>"},{"location":"communication/04-osi-model/#references","title":"References","text":"<ul> <li>Wiki - OSI Model</li> </ul>"},{"location":"communication/05-ip-tcp/","title":"IP / TCP","text":"<p>Transmission Control Protocol (TCP) is a standard that defines how to establish and maintain a network conversation by which applications can exchange data.</p>"},{"location":"communication/05-ip-tcp/#references","title":"References","text":"<ul> <li>https://www.techtarget.com/searchnetworking/definition/TCP</li> <li>https://www.geeksforgeeks.org/what-is-transmission-control-protocol-tcp/</li> </ul>"},{"location":"communication/11-request/","title":"HTTP Request","text":""},{"location":"communication/11-request/#terms","title":"Terms","text":""},{"location":"communication/11-request/#querystring","title":"QueryString","text":"<p>The HTTP query string is specified by the values following the question mark (?).</p> <p>Query strings are contained in request headers. It is wise to not trust the data that is contained in headers, as this information can be falsified by malicious users.</p> <p>As a security precaution, always encode header data or user input before using it. Alternatively, you can validate header data and user input with a short function.</p>"},{"location":"communication/11-request/#references","title":"References","text":"<ul> <li>Book : Writing Secure Code, 2nd edition</li> <li>Microsoft | Request object</li> </ul>"},{"location":"data-engineering/","title":"Data Engineering","text":"<p>Data engineering is a field within data science that focuses on the practical application of data collection and analysis. It involves the design and construction of systems and architecture for extracting, storing, and analyzing large volumes of data. Data engineers play a crucial role in enabling organizations to make data-driven decisions by ensuring that data is properly stored, processed, and made accessible to data scientists and analysts.</p> <p>Scope of Data Engineering:</p> <ol> <li> <p>Data Collection and Ingestion:    Data engineers work on collecting and ingesting data from various sources, including databases, APIs, logs, and external datasets. They design processes to ensure the efficient and reliable flow of data into storage systems.</p> </li> <li> <p>Data Storage:    Determining the appropriate storage infrastructure for different types of data is a key aspect of data engineering. This includes choosing databases, data warehouses, or data lakes based on the organization's needs and requirements.</p> </li> <li> <p>Data Processing:    Data engineers develop processes for transforming and processing raw data into a format suitable for analysis. This involves cleaning, aggregating, and structuring data to meet the specific needs of downstream applications.</p> </li> <li> <p>Data Modeling:    Building data models that represent the structure and relationships within the data is crucial for effective analysis. Data engineers design and implement these models to ensure accurate representation and efficient querying.</p> </li> <li> <p>Data Quality and Governance:    Ensuring the quality and integrity of data is a priority for data engineers. They implement mechanisms for data validation, error handling, and governance to maintain high data quality standards.</p> </li> <li> <p>Big Data Technologies:    With the rise of big data, data engineers often work with technologies such as Apache Hadoop, Spark, and other distributed computing frameworks to handle and process large datasets.</p> </li> <li> <p>Data Pipelines:    Designing and building data pipelines is a fundamental task in data engineering. These pipelines automate the flow of data from source to destination, ensuring a streamlined and reliable process.</p> </li> <li> <p>Real-time Data Processing:    In scenarios where real-time insights are critical, data engineers may work on implementing solutions for streaming data processing. This involves handling data as it arrives, allowing for immediate analysis.</p> </li> <li> <p>Scalability and Performance:    Data engineers must consider the scalability and performance of data systems. This includes optimizing queries, choosing appropriate hardware, and implementing strategies to handle growing volumes of data.</p> </li> <li> <p>Collaboration with Data Science:     Data engineers collaborate closely with data scientists and analysts to understand their data requirements and ensure that the infrastructure supports advanced analytics and machine learning initiatives.</p> </li> <li> <p>Security and Compliance:     Implementing security measures to protect sensitive data and ensuring compliance with data protection regulations are critical aspects of data engineering.</p> </li> </ol> <p>The scope of data engineering is dynamic and continually evolving as new technologies and data challenges emerge. It is an integral part of the data ecosystem, providing the foundation for effective data analysis and decision-making within organizations.</p>"},{"location":"data-engineering/#references","title":"References","text":"<ul> <li>databricks | Databricks Data Science &amp; Engineering guide</li> <li>databricks | Work with Delta Lake table history a.k.a. Time Travel</li> <li>databricks | Data skipping with Z-order indexes for Delta Lake</li> <li>databricks | GDPR and CCPA compliance with Delta Lake<ul> <li>Wikipedia | Pseudonymization</li> </ul> </li> </ul>"},{"location":"data-engineering/cdc/","title":"Unraveling the Power of Change Data Capture","text":"<p>In the fast-paced realm of data management, organizations are continually seeking innovative ways to stay ahead of the curve, adapt to changes, and harness the power of evolving information. One such transformative technology making waves in the data landscape is Change Data Capture (CDC). Let's delve into the world of CDC and explore how it revolutionizes the way we capture, understand, and utilize dynamic data changes.</p>"},{"location":"data-engineering/cdc/#decoding-change-data-capture-cdc","title":"Decoding Change Data Capture (CDC)","text":"<p>Change Data Capture is a sophisticated method that identifies and captures changes made to data in databases. It's a game-changer in the realm of data integration and analytics, providing organizations with real-time insights into alterations within their datasets. Unlike traditional methods that often involve processing entire datasets, CDC focuses on the incremental changes, optimizing efficiency and reducing processing overhead.</p>"},{"location":"data-engineering/cdc/#how-cdc-works","title":"How CDC Works","text":"<ol> <li> <p>Capture Changes in Real-Time: CDC captures changes as they occur, offering a real-time perspective on data evolution. Whether it's an update, insertion, or deletion, CDC tracks modifications at the granular level.</p> </li> <li> <p>Log-Based or Trigger-Based Approach: Two primary methods govern CDC implementation. The log-based approach involves extracting changes from transaction logs, while the trigger-based method employs triggers to identify modifications directly within the database tables.</p> </li> <li> <p>Efficient Data Replication: By pinpointing changes, CDC facilitates efficient data replication. Instead of replicating entire datasets, only the modified data is transferred, ensuring a streamlined and resource-efficient process.</p> </li> </ol>"},{"location":"data-engineering/cdc/#the-benefits-of-change-data-capture","title":"The Benefits of Change Data Capture","text":"<ol> <li> <p>Real-Time Insights: CDC provides organizations with a dynamic view of their data, enabling real-time insights into changes. This agility is invaluable for decision-making, analytics, and staying abreast of evolving trends.</p> </li> <li> <p>Reduced Processing Overhead: By focusing on incremental changes, CDC minimizes processing overhead. This efficiency is particularly advantageous in large datasets, promoting quicker data replication and integration.</p> </li> <li> <p>Enhanced Data Quality: Accurate and timely information is essential for making informed decisions. CDC ensures data quality by capturing changes promptly, reducing the likelihood of working with outdated or inaccurate information.</p> </li> <li> <p>Improved Data Integration: Integrating data across different systems becomes more seamless with CDC. The ability to capture changes efficiently ensures that disparate datasets can be synchronized without cumbersome manual interventions.</p> </li> <li> <p>Compliance and Auditing: For organizations operating in regulated industries, CDC plays a crucial role in compliance and auditing. The ability to track changes provides a robust audit trail, enhancing accountability and transparency.</p> </li> </ol>"},{"location":"data-engineering/cdc/#use-cases-and-implementation","title":"Use Cases and Implementation","text":"<ol> <li> <p>E-Commerce and Retail: In the dynamic landscape of e-commerce, product updates, inventory changes, and pricing modifications are constant. CDC ensures that the latest information is seamlessly integrated into various systems.</p> </li> <li> <p>Financial Services: In the financial sector, where accuracy and speed are paramount, CDC facilitates real-time updates on transactions, account changes, and regulatory modifications.</p> </li> <li> <p>Healthcare: In healthcare settings, patient records, treatment plans, and medical histories undergo continuous updates. CDC ensures that healthcare providers have the latest and most accurate information at their fingertips.</p> </li> </ol>"},{"location":"data-engineering/cdc/#navigating-the-future-with-cdc","title":"Navigating the Future with CDC","text":"<p>As organizations continue to grapple with ever-expanding datasets and the need for real-time insights, Change Data Capture emerges as a linchpin technology. Embracing CDC not only streamlines data integration but also empowers organizations to adapt swiftly to changing business landscapes.</p> <p>In a world where data is a driving force, Change Data Capture stands as a beacon of efficiency, providing organizations with the tools they need to thrive in an era of constant change. As technology continues to evolve, the transformative capabilities of CDC are set to play an increasingly pivotal role in shaping the future of data management.</p>"},{"location":"data-engineering/cdc/#references","title":"References","text":"<ul> <li>https://developers.redhat.com/blog/2020/05/08/change-data-capture-with-debezium-a-simple-how-to-part-1</li> <li>https://www.startdataengineering.com/post/change-data-capture-using-debezium-kafka-and-pg/</li> <li>https://www.confluent.io/resources/kafka-summit-2020/change-data-capture-pipelines-with-debezium-and-kafka-streams/<ul> <li>https://www.arcion.io/blog/debezium-alternatives</li> <li>https://www.reddit.com/r/dataengineering/comments/y3zj27/debeziumkafka_alternatives_for_cdc_stream/</li> <li>https://hevodata.com/learn/7-best-cdc-tools/</li> <li>https://slashdot.org/software/p/Debezium/alternatives</li> </ul> </li> <li>https://itnext.io/change-data-capture-with-debezium-on-serverless-kafka-e9e4bfa9944e</li> </ul>"},{"location":"data-engineering/data-lake/","title":"Data Lake","text":"<p>A data lake is a centralized repository that allows organizations to store vast volumes of <code>raw data</code> in its native format until it is needed. Unlike traditional databases or data warehouses, a data lake can store structured data, semi-structured data, and unstructured data. This flexibility makes it a valuable resource for organizations dealing with diverse and large datasets.</p> <p>A <code>hierarchical data warehouse</code> stores data in files or folders, whereas a <code>data lake</code> uses a flat architecture and object storage to store the data.\u200d</p> <p>Key characteristics of a data lake include:</p> <ol> <li> <p>Storage of Raw Data:    Data lakes store raw, unprocessed data in its original form. This includes data from various sources such as logs, social media, sensors, and more.</p> </li> <li> <p>Scalability:    Data lakes are designed to handle massive amounts of data, providing scalability to accommodate the growing volume and variety of information.</p> </li> <li> <p>Schema-on-Read:    Unlike traditional databases that use a schema-on-write approach, data lakes typically employ a schema-on-read strategy. This means the structure is applied when the data is analyzed rather than when it's ingested.</p> </li> <li> <p>Diverse Data Types:    Data lakes can store structured, semi-structured, and unstructured data. This includes relational data, JSON, XML, images, videos, and more.</p> </li> <li> <p>Cost-Effective Storage:    Storage costs in data lakes are often more economical compared to traditional databases. Cloud-based data lakes, in particular, offer pay-as-you-go pricing models.</p> </li> <li> <p>Flexibility:    Data lakes provide flexibility for data exploration and analysis. Users can access and analyze data without predefined schemas, facilitating discovery and experimentation.</p> </li> <li> <p>Integration with Big Data Technologies:    Data lakes are commonly integrated with big data technologies such as Apache Hadoop and Apache Spark, enabling the processing of large datasets in a distributed computing environment.</p> </li> <li> <p>Support for Advanced Analytics:    Data lakes support advanced analytics, machine learning, and artificial intelligence applications by providing a rich source of diverse and raw data for training and analysis.</p> </li> <li> <p>Data Governance Challenges:    Managing metadata, ensuring data quality, and implementing proper governance are challenges associated with data lakes. As the volume of data grows, maintaining control over data assets becomes crucial.</p> </li> <li> <p>Security Considerations:     Securing data in a data lake is a critical aspect. Access controls, encryption, and proper authentication mechanisms are essential to protect sensitive information.</p> </li> <li> <p>Tooling Ecosystem:     A variety of tools and frameworks are available to work with data lakes, including data preparation tools, analytics platforms, and machine learning frameworks.</p> </li> </ol> <p>Common platforms for building data lakes include cloud services like Amazon S3, Microsoft Azure Data Lake Storage, and Google Cloud Storage. Organizations can leverage these platforms to build scalable, cost-effective, and flexible repositories for their diverse data needs.</p>"},{"location":"data-engineering/data-lake/#references","title":"References","text":"<ul> <li>databricks | Introduction to Data Lakes</li> </ul>"},{"location":"data-engineering/data-lakehouse/","title":"Data Lakehouse","text":"<p>A Data Lakehouse is an architectural approach that combines the benefits of both <code>data lakes</code> and <code>data warehouses</code>. It seeks to address some of the limitations and challenges associated with traditional data warehouses and data lakes, providing a unified and versatile platform for managing and analyzing data.</p> <p>Here are the key components and characteristics of a Data Lakehouse:</p> <ol> <li> <p>Unified Storage:    In a Data Lakehouse, data is stored in a unified manner, typically using a cloud-based object store. This storage layer accommodates both structured and semi-structured data, allowing for the flexibility of a data lake.</p> </li> <li> <p>Schema Enforcement:    Unlike traditional data lakes, a Data Lakehouse incorporates schema enforcement for structured data. This means that while it retains the schema flexibility of a data lake, it also enforces a schema on the data when necessary for structured analysis.</p> </li> <li> <p>Optimized for Analytics:    The architecture is designed to optimize data for analytics and querying. This includes the ability to organize and index data efficiently, making it suitable for analytical workloads.</p> </li> <li> <p>Support for ACID Transactions:    Data Lakehouse introduces support for ACID (Atomicity, Consistency, Isolation, Durability) transactions. This ensures data consistency and reliability, which are critical for traditional data warehouse use cases.</p> </li> <li> <p>Query Performance:    With optimizations for analytical query performance, a Data Lakehouse allows users to perform complex analytics on large datasets without compromising on speed or efficiency.</p> </li> <li> <p>Separation of Compute and Storage:    Similar to cloud data warehouses, a Data Lakehouse often separates compute and storage, allowing organizations to scale their computational resources independently of storage capacity.</p> </li> <li> <p>Compatibility with Data Warehousing Tools:    A Data Lakehouse is compatible with existing data warehousing tools and query languages. This means that organizations can leverage their existing analytics and business intelligence tools without significant modifications.</p> </li> <li> <p>Data Governance and Security:    Robust data governance and security features are integral to a Data Lakehouse. This includes access controls, encryption, auditing, and other measures to ensure data security and compliance.</p> </li> <li> <p>Flexibility and Agility:    The flexibility of a Data Lakehouse allows organizations to ingest and analyze diverse types of data, making it suitable for handling both traditional structured data and newer, less-structured formats.</p> </li> <li> <p>Incremental Migration:     Organizations can incrementally migrate their existing data lake or data warehouse workloads to a Data Lakehouse, providing a transition path that aligns with their evolving data strategy.</p> </li> </ol> <p>The concept of a Data Lakehouse is particularly relevant in the context of modern data architectures, where organizations seek to break down silos, improve data agility, and support a wide range of analytics use cases. By combining elements of both data lakes and data warehouses, a Data Lakehouse aims to provide a comprehensive solution that caters to the evolving needs of data-driven organizations.</p>"},{"location":"data-engineering/data-lakehouse/#lakehouse-over-lake-and-warehouse","title":"Lakehouse over lake and warehouse","text":""},{"location":"data-engineering/data-lakehouse/#limitations-and-challenges-of-traditional-data-warehouses","title":"Limitations and Challenges of Traditional Data Warehouses","text":"<ol> <li> <p>Scalability:    Traditional data warehouses may face scalability challenges when dealing with massive volumes of data. Scaling up can be expensive, and scaling out may introduce complexities.</p> </li> <li> <p>Structured Data Focus:    They are optimized for structured data, making it challenging to handle semi-structured or unstructured data commonly found in modern diverse datasets.</p> </li> <li> <p>Cost:    The cost of storage and processing in traditional data warehouses can be high, especially as data volumes increase. Scaling to accommodate more data can result in escalating costs.</p> </li> <li> <p>Complexity in Data Ingestion:    Ingesting data into traditional data warehouses can be complex, especially when dealing with real-time or streaming data. Loading large datasets can also be time-consuming.</p> </li> <li> <p>Data Latency:    Traditional data warehouses may struggle with real-time data processing, and there might be a delay in making the latest data available for analysis.</p> </li> <li> <p>Limited Flexibility:    Adapting to changes in data structures or adding new data sources may be challenging due to the rigid schemas and structures inherent in traditional warehouses.</p> </li> <li> <p>Inability to Handle Big Data:    Traditional data warehouses may lack the capabilities to handle the scale and variety of data associated with big data. This can limit their effectiveness in today's data landscape.</p> </li> </ol>"},{"location":"data-engineering/data-lakehouse/#limitations-and-challenges-of-traditional-data-lakes","title":"Limitations and Challenges of Traditional Data Lakes","text":"<ol> <li> <p>Schema-on-Read Complexity:    Data lakes typically follow a schema-on-read approach, which means the structure is determined at the time of analysis. This can lead to complexities and challenges in understanding the data.</p> </li> <li> <p>Data Quality and Governance:    Maintaining data quality and governance in data lakes can be challenging. The absence of a predefined schema can result in issues related to data accuracy and consistency.</p> </li> <li> <p>Cost Management:    While storage costs in data lakes are relatively low, managing and processing large volumes of data for analytics can become costly, especially when scaling computational resources.</p> </li> <li> <p>Query Performance:    Analyzing data in data lakes may suffer from slower query performance compared to traditional data warehouses, especially without proper optimization and indexing.</p> </li> <li> <p>Security Concerns:    Security and access control in data lakes can be complex. Managing permissions and ensuring data security requires careful planning and implementation.</p> </li> <li> <p>Lack of Transaction Support:    Traditional data lakes may lack transactional capabilities, which can be crucial for ensuring data consistency, especially in scenarios where multiple transactions need to be coordinated.</p> </li> <li> <p>Metadata Management:    Maintaining metadata and ensuring proper documentation in data lakes can be challenging. The sheer volume and diversity of data may result in difficulties in tracking and understanding the available datasets.</p> </li> <li> <p>Tooling and Integration:    Integrating data lakes with existing analytics and business intelligence tools may require additional effort, as these tools are often designed with traditional data warehouse paradigms in mind.</p> </li> </ol> <p>Overcoming Limitations with Data Lakehouses:</p> <p>A Data Lakehouse attempts to address some of the limitations of traditional data warehouses and data lakes by combining their strengths. It provides a unified storage architecture that supports both structured and semi-structured data, enforces schema when needed, and optimizes for analytics while offering scalability and flexibility.</p>"},{"location":"data-engineering/data-lakehouse/#references","title":"References","text":"<ul> <li>databricks | Data Lakehouse</li> <li>databricks | What Is a Lakehouse?</li> </ul>"},{"location":"data-engineering/data-mesh/","title":"Data Mesh","text":"<p>Data Mesh is a relatively recent concept in the field of data architecture and management that was introduced by Zhamak Dehghani. It represents a paradigm shift in how organizations approach the scalability and democratization of data within large enterprises. Instead of having a centralized monolithic approach to data, Data Mesh promotes a decentralized, domain-oriented strategy.</p>"},{"location":"data-engineering/data-mesh/#key-principles-of-data-mesh","title":"Key Principles of Data Mesh","text":"<ol> <li> <p>Domain-Oriented Data Ownership:    Data is owned by the domain to which it is most relevant. Each business domain has its own data team responsible for the data's quality, governance, and accessibility.</p> </li> <li> <p>Data as a Product:    Treat data as a product with its own lifecycle, including design, implementation, and ongoing support. This emphasizes the importance of creating data products that are valuable, usable, and maintainable.</p> </li> <li> <p>Decentralized Data Architecture:    Data Mesh advocates for a decentralized approach to data infrastructure. Instead of a centralized data lake or warehouse, each domain operates its own data infrastructure, fostering scalability and autonomy.</p> </li> <li> <p>Federated Data Mesh:    The federated data mesh model involves connecting multiple data domains through a set of shared principles, standards, and protocols. This enables interoperability and collaboration while maintaining autonomy.</p> </li> <li> <p>Data Product Teams:    Each domain has its own cross-functional data product team, which includes data engineers, data scientists, domain experts, and other relevant roles. These teams are responsible for the end-to-end delivery of data products.</p> </li> <li> <p>Self-serve Data Infrastructure:    Providing self-serve data infrastructure tools and platforms empowers domain teams to manage their data infrastructure, reducing dependencies on a central data team.</p> </li> <li> <p>APIs for Data Products:    Data products are exposed through well-defined APIs, making them discoverable and usable across different domains. This ensures a standardized and interoperable approach to data access.</p> </li> <li> <p>Data Ownership and Governance:    Data ownership and governance are distributed across domain teams, ensuring that those closest to the data understand and manage its quality, compliance, and security aspects.</p> </li> <li> <p>Data Platform as a Product:    The centralized data platform is treated as a product in itself, with a focus on delivering features that enhance the productivity and effectiveness of domain data teams.</p> </li> </ol>"},{"location":"data-engineering/data-mesh/#benefits-of-data-mesh","title":"Benefits of Data Mesh","text":"<ol> <li> <p>Scalability:    Scalability is improved through decentralization, allowing each domain to manage its own data infrastructure independently.</p> </li> <li> <p>Autonomy:    Domains have autonomy over their data, allowing them to make decisions that best suit their specific business needs.</p> </li> <li> <p>Flexibility:    The federated model allows for flexibility and adaptability, as new domains can be added or modified without disrupting the entire data architecture.</p> </li> <li> <p>Responsibility and Accountability:    By assigning ownership to domain teams, there is a clear line of responsibility and accountability for the data quality and outcomes.</p> </li> <li> <p>Improved Collaboration:    Collaboration is facilitated through well-defined APIs and standards, enabling different domains to share and leverage each other's data products.</p> </li> </ol> <p>Data Mesh represents a departure from traditional centralized approaches to data management, aiming to address the challenges posed by the increasing complexity and scale of data within modern enterprises. It emphasizes principles of autonomy, ownership, and treating data as a product to unlock the potential of data-driven decision-making across diverse business domains.</p>"},{"location":"data-engineering/data-mesh/#references","title":"References","text":"<ul> <li>Data Mesh Architecture</li> <li>Martin Flowler | Data Mesh Principles and Logical Architecture</li> <li>What is a Data Mesh \u2014 and How Not to Mesh it Up blog</li> </ul>"},{"location":"data-engineering/data-warehouse/","title":"Data Warehouse","text":"<p>A data warehouse is a centralized repository that stores and manages structured data from one or more sources. It is designed for efficient querying, reporting, and analysis, providing a consolidated view of an organization's data for decision-making purposes. Data warehouses play a crucial role in business intelligence and analytics, enabling organizations to derive insights and make informed decisions based on historical and current data.</p> <p>Key characteristics of a data warehouse include:</p> <ol> <li> <p>Structured Data Storage:    Data warehouses primarily store structured data, organized into tables with rows and columns. This structured format facilitates efficient querying and analysis.</p> </li> <li> <p>Integration of Data:    Data warehouses integrate data from multiple sources within an organization, such as transactional databases, CRM systems, and other data repositories. This integration helps provide a unified view of business operations.</p> </li> <li> <p>Historical Data Storage:    Data warehouses store historical data, allowing users to analyze trends and changes over time. This historical perspective is valuable for decision-making and trend analysis.</p> </li> <li> <p>Query and Analysis Optimization:    The architecture of data warehouses is optimized for complex querying and analysis. It typically involves indexing, partitioning, and other techniques to enhance query performance.</p> </li> <li> <p>Business Intelligence and Reporting:    Data warehouses serve as the foundation for business intelligence (BI) and reporting tools. Users can run queries and generate reports to gain insights into business performance.</p> </li> <li> <p>Data Transformation and Cleansing:    Before being loaded into the data warehouse, data often undergoes a process of transformation and cleansing. This ensures data quality and consistency within the warehouse.</p> </li> <li> <p>OLAP (Online Analytical Processing) Support:    Data warehouses support OLAP, allowing users to perform multidimensional analysis. OLAP enables the exploration of data from different dimensions and perspectives.</p> </li> <li> <p>Data Mart Creation:    Data warehouses can be structured as a single, centralized repository or as a collection of data marts. Data marts are smaller, subject-specific subsets of the data warehouse tailored to specific business units or departments.</p> </li> <li> <p>Concurrency and Scalability:    To support multiple users querying the data simultaneously, data warehouses are designed for concurrency. Scalability features are also incorporated to handle growing volumes of data.</p> </li> <li> <p>Data Governance and Security:     Data warehouses implement robust data governance practices to maintain data accuracy, consistency, and security. Access controls and encryption are commonly used to protect sensitive information.</p> </li> <li> <p>ETL Processes:     Extract, Transform, Load (ETL) processes are employed to extract data from source systems, transform it into a suitable format, and load it into the data warehouse.</p> </li> </ol> <p>Common technologies used for building data warehouses include Amazon Redshift, Google BigQuery, Microsoft Azure Synapse Analytics, and traditional on-premises solutions like Oracle Data Warehouse and Teradata.</p> <p>Data warehouses provide a structured and organized approach to managing and analyzing data, supporting the needs of decision-makers across various departments within an organization.</p>"},{"location":"data-engineering/medallion-architecture/","title":"Medallion Architecture","text":"<p>A medallion architecture is a data design pattern used to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze \u21d2 Silver \u21d2 Gold layer tables). Medallion architectures are sometimes also referred to as \"multi-hop\" architectures.</p>"},{"location":"data-engineering/medallion-architecture/#bronze-layer","title":"Bronze layer","text":"<p>Raw data.</p>"},{"location":"data-engineering/medallion-architecture/#silver-layer","title":"Silver layer","text":"<p>Cleansed and conformed data.</p>"},{"location":"data-engineering/medallion-architecture/#gold-layer","title":"Gold layer","text":"<p>Curated business-level tables.</p>"},{"location":"data-engineering/medallion-architecture/#references","title":"References","text":"<ul> <li>databricks | Medallion Architecture</li> <li>Medium | Medallion architecture: best practices for managing Bronze, Silver and Gold</li> <li>LinkedIn | Consider Gold, Silver, and Bronze for your Data, not just the Olympics</li> </ul>"},{"location":"data-mining/","title":"Data Mining","text":"<p>Data mining is the automated gathering of data from the internet. Also commonly known as screen scraping, web scraping, web harvesting, etc.</p>"},{"location":"data-mining/#table-of-contents","title":"Table of contents","text":"<ul> <li>Crawling vs Scraping</li> <li>Ways it is done?</li> <li>[Legalities]</li> <li>[Ethics]</li> <li>Tasks</li> <li>Finding</li> <li>Parsing</li> <li>Crawling</li> </ul>"},{"location":"data-mining/#classifying-the-project","title":"Classifying the project","text":"<ul> <li><code>crawling</code> or <code>scraping</code></li> <li><code>broad</code> or <code>targeted</code></li> <li><code>one-time</code> or <code>long running</code></li> <li>One-time projects result in producing files in formats like csv, json, etc</li> <li>Long running projects involve monitoring, re-scanning for new data, update data, etc</li> <li>Data needed for collection or more in-depth analysis</li> </ul>"},{"location":"data-mining/#using-python","title":"Using Python","text":"<p>Python has a default recursion limit (the number of times a program can recursively call itself) of 1,000. Because Wikipedia\u2019s network of links is extremely large, the program will eventually hit that recursion limit and stop, unless you put in a recursion counter or something to prevent that from happening.</p> <p>With server-side redirects, you usually don\u2019t have to worry. If you\u2019re using the <code>urllib</code> library with Python 3.x, it handles redirects automatically! If you\u2019re using the Requests library, make sure to set the <code>allow_redirects</code> flag to True:</p> <pre><code>r = requests.get('http://github.com', allow_redirects=True)\n</code></pre>"},{"location":"data-mining/#helpful-links","title":"Helpful links","text":""},{"location":"data-mining/#affiliates","title":"Affiliates","text":"<ul> <li>Amazon promotions API</li> <li>Walmart affiliate program</li> <li>Target partners</li> <li>eBay partner network</li> <li>Etsy affiliates program</li> <li>Nike affiliate program</li> <li>Advanced Auto Parts affiliate program</li> <li>AliExpress</li> </ul>"},{"location":"data-mining/#3rd-party-tools-apis","title":"3rd party tools &amp; APIs","text":"<ul> <li>Google Search Central - to learn more about how search engines find and tag the data that they display in special search result features and enhancements.</li> <li>WebScraping API</li> </ul>"},{"location":"data-mining/#examples-sites-to-scrape","title":"Examples sites to scrape","text":"<ul> <li>https://records.nhl.com/</li> <li>https://www.nfl.com/stats/player-stats/</li> <li>Wikipedia API</li> <li>Six Degrees of Kevin Bacon</li> <li>The Oracle of Bacon</li> </ul>"},{"location":"data-mining/dark-web/","title":"Dark Web","text":"<p>The Dark Web isn\u2019t particularly vast, it\u2019s not 90 percent of the Internet, and it\u2019s not even particularly secret. In fact, the Dark Web is a collection of websites that are publicly visible, yet hide the IP addresses of the servers that run them. That means anyone can visit a Dark Web site, but it can be very difficult to figure out where they\u2019re hosted\u2014or by whom.</p> <p>The Dark Web hides server IP addresses through a combination of encryption, anonymization techniques, and the use of specialized networks like Tor (The Onion Router). Here's a brief overview of how this works:</p> <ol> <li> <p>Tor Network: The Tor network is a decentralized network of volunteer-operated servers that allows users to browse the internet anonymously. When you access a website on the Tor network, your traffic is routed through multiple encrypted nodes (relays) before reaching its destination. Each relay only knows the IP address of the previous and next relay, making it difficult to trace the origin of the connection.</p> </li> <li> <p>Onion Routing: Tor uses onion routing to encrypt and route traffic through multiple relays in a layered manner. Each relay only decrypts one layer of encryption, so no single relay can see both the source and destination of the traffic. This provides a high level of anonymity for users and servers on the Tor network.</p> </li> <li> <p>Hidden Services: Websites on the Dark Web often use Tor's hidden services feature to conceal their server IP addresses. Hidden services have .onion domain names and are accessible only through the Tor network. When you access a hidden service, your connection is routed through Tor relays, and the server's IP address remains hidden from both users and observers.</p> </li> <li> <p>Encryption: Dark Web servers may use encryption protocols like HTTPS (SSL/TLS) to secure communications between clients and servers. This helps protect the confidentiality and integrity of data transmitted over the network and prevents eavesdropping by third parties.</p> </li> <li> <p>Obfuscation Techniques: Some Dark Web operators may employ obfuscation techniques to further conceal their server IP addresses. This might include using proxy servers, virtual private networks (VPNs), or other privacy-enhancing tools to obscure the origin of network traffic.</p> </li> </ol> <p>While these measures provide a significant degree of anonymity, it's important to note that they are not foolproof. Advanced adversaries, such as government agencies or skilled hackers, may still be able to deanonymize users or servers on the Dark Web through various means, including traffic analysis, compromised endpoints, or vulnerabilities in the Tor network itself.</p>"},{"location":"data-mining/dark-web/#resources","title":"Resources","text":"<ul> <li>Hacker Lexicon: What Is the Dark Web?</li> </ul>"},{"location":"data-mining/ethics/","title":"Ethics","text":""},{"location":"data-mining/ethics/#trespass-to-chattels","title":"Trespass to Chattels","text":"<p>Trespass to chattels is fundamentally different from what we think of as \u201ctrespassing laws\u201d in that it applies not to real estate or land but to movable property, or chattels in legal parlance. It applies when your property is interfered with in some way that does not allow you to access or use it.</p> <p>Checking out a website via your browser is fine; launching a full-scale Distributed Denial of Service (DDOS) attack against it obviously is not.</p> <p>Three criteria need to be met for a web scraper to violate trespass to chattels:</p> <ol> <li>Lack of consent: Because web servers are open to everyone, they are generally \u201cgiving consent\u201d to web scrapers as well. However, many websites\u2019 Terms of Service agreements specifically prohibit the use of scrapers. In addition, any cease and desist notices delivered to you may revoke this consent.</li> <li>Actual harm: Servers are costly. In addition to server costs, if your scrapers take a website down, or limit its ability to serve other users, this can add to the \u201charm\u201d you cause.</li> <li>Intentionality: If you\u2019re writing the code, you know what it does! Arguing a lack of intention would likely not go well when defending your web scraper.</li> </ol>"},{"location":"data-mining/ethics/#computer-fraud-and-abuse-act","title":"Computer Fraud and Abuse Act","text":"<p>Computer Fraud and Abuse Act (CFAA) applies to only a stereotypical version of a malicious hacker unleashing viruses, the act has strong implications for web scrapers as well.</p> <p>In short: stay away from protected computers, do not access computers (including web servers) that you are not given access to, and especially, stay away from government or financial computers.</p>"},{"location":"data-mining/ethics/#robots-exclusion-protocol-robotstxt","title":"Robots Exclusion Protocol (robots.txt)","text":"<p>If you go to just about any large website and look for its <code>robots.txt</code> file, you will find it in the <code>root web folder</code>. robots.txt can be parsed and used by automated programs extremely easily. It is merely a sign that says <code>Please don\u2019t go to these parts of the site</code>.</p> <p>There is no official governing body for the syntax of robots.txt. Example below:</p> <pre><code># Welcome to my robots.txt file!\nUser-agent: *\nDisallow: *\n\n# Google Search Engine Robot\nUser-agent: Googlebot\nAllow: *\nAllow: /search?q=%23\nDisallow: /private\n</code></pre>"},{"location":"data-mining/ethics/#considerations","title":"Considerations","text":"<ul> <li>Don't be a jerk</li> <li>Do not scrape any user's personal information, even if it is publicly available</li> <li>Respect rules in robots.txt, defined by the domain owner</li> <li>It\u2019s best to leave the bot running slowly and during the night, than hammering the site hard in a short time.</li> <li>If you're crawling multiple websites, it is ok to do that in parallel. The load needs to be reasonable for each individual remote server.</li> </ul>"},{"location":"data-mining/ethics/#interesting-links","title":"Interesting links","text":"<ul> <li>The Google web cache | Every site that Google's bot crawl, they make a copy of the site and host it on the internet. If a website you are searching for, or scraping, is unavailable, you might want to check there to see if a usable copy exists!</li> </ul>"},{"location":"data-mining/legalities/","title":"Legalities","text":""},{"location":"data-mining/legalities/#intellectual-property","title":"Intellectual Property","text":"<p>There are three basic types of intellectual property: <code>trademarks</code> (indicated by a \u2122 or \u00ae symbol), <code>copyrights</code> (the ubiquitous \u00a9), and <code>patents</code> (sometimes indicated by text noting that the invention is patent protected or a patent number but often by nothing at all).</p> <p>Patents are used to declare ownership over inventions only. You cannot patent images, text, or any information itself.</p> <p>A trademark is a word, phrase, symbol, and/or design that identifies and distinguishes the source of the goods of one party from those of others. A service mark is a word, phrase, symbol, and/or design that identifies and distinguishes the source of a service rather than goods. The term \u201ctrademark\u201d is often used to refer to both trademarks and service marks.</p> <p>Unlike with patents, the ownership of a trademark depends heavily on the context in which it is used. For example, if I wish to publish a blog post with an accompanying picture of the Coca-Cola logo, I could do that, as long as I wasn\u2019t implying that my blog post was sponsored by, or published by, Coca-Cola.</p>"},{"location":"data-mining/legalities/#fair-use","title":"Fair use","text":"<p>Storing or displaying a trademark as a reference to the brand it represents is fine. Using a trademark in a way that might mislead the consumer is not. The concept of \u201cfair use\u201d does not apply to patents, however.</p>"},{"location":"data-mining/legalities/#copyright","title":"Copyright","text":"<p>Both trademarks and patents have something in common in that they have to be formally registered in order to be recognized. This is not true with copyrighted material.</p> <p>The Berne Convention for the Protection of Literary and Artistic Works, Switzerland, is the international standard for copyright. In practice, this means that, as a US citizen, you can be held accountable in the United States for violating the copyright of material written by someone in, say, France (and vice versa).</p> <p>What makes images, text, music, etc., copyrighted? Every piece of material you create is automatically subject to copyright law as soon as you bring it into existence. Copyright protection extends to creative works only. It does not cover statistics or facts.</p>"},{"location":"data-mining/legalities/#usage","title":"Usage","text":"<p>Even copyrighted content can be used directly, within reason, under the Digital Millennium Copyright Act of 1988. The DMCA outlines some rules for the automated handling of copyrighted material, with two main points:</p> <ul> <li>Under the <code>safe harbor</code> protection, if you scrape material from a source, you are protected as long as you removed the copyrighted material when notified.</li> <li>You cannot circumvent security measures (such as password protection) in order to gather content.</li> </ul> <p>In short, you should never directly publish copyrighted material without permission from the original author or copyright holder. If you are storing copyrighted material that you have free access to in your own nonpublic database for the purposes of analysis, that is fine. If you are publishing that database to your website for viewing or download, that is not fine. If you are analyzing that database and publishing statistics about word counts, a list of authors by prolificacy, or some other meta-analysis of the data, that is fine. If you are accompanying that meta-analysis with a few select quotes, or brief samples of data to make your point, that is likely also fine, but you might want to examine the fair-use clause in the US Code to make sure.</p>"},{"location":"data-structure/","title":"Data Structures","text":""},{"location":"data-structure/#what-are-data-structures","title":"What are Data Structures?","text":"<p>Data structures are specialized formats for organizing, managing, and storing data in a computer so that it can be accessed and modified efficiently. They provide a means to manage large amounts of data efficiently for various uses, such as databases, internet indexing services, and large-scale computations.</p>"},{"location":"data-structure/#types-of-data-structures","title":"Types of Data Structures","text":"<p>Data structures can be broadly categorized into two types:</p> <ol> <li> <p>Primitive Data Structures</p> <ul> <li>These are the basic structures built into the programming language.</li> <li>Examples: Integers, floats, characters, pointers.</li> </ul> </li> <li> <p>Non-Primitive Data Structures</p> <ul> <li>These are more complex structures that utilize primitive data types.</li> <li>Examples: Arrays, linked lists, stacks, queues, trees, graphs, hash tables.</li> </ul> </li> </ol>"},{"location":"data-structure/#common-data-structures","title":"Common Data Structures","text":"<ol> <li> <p>Arrays</p> <ul> <li>A collection of elements identified by index or key.</li> <li>Useful for quick access to elements by index.</li> <li>Fixed size and homogeneous elements.</li> </ul> </li> <li> <p>Linked Lists</p> <ul> <li>A sequence of nodes where each node contains data and a reference to the next node.</li> <li>Efficient for insertions and deletions.</li> <li>Types: Singly linked list, doubly linked list, circular linked list.</li> </ul> </li> <li> <p>Stacks</p> <ul> <li>A collection of elements with Last In, First Out (LIFO) access.</li> <li>Operations: push (add), pop (remove).</li> <li>Useful for undo mechanisms, expression parsing.</li> </ul> </li> <li> <p>Queues</p> <ul> <li>A collection of elements with First In, First Out (FIFO) access.</li> <li>Operations: enqueue (add), dequeue (remove).</li> <li>Useful for scheduling tasks, buffering.</li> </ul> </li> <li> <p>Trees</p> <ul> <li>Hierarchical data structure with a root element and child nodes.</li> <li>Types: Binary trees, binary search trees, AVL trees, B-trees.</li> <li>Useful for hierarchical data, fast search, insert, and delete operations.</li> </ul> </li> <li> <p>Graphs</p> <ul> <li>A collection of nodes (vertices) and edges connecting them.</li> <li>Types: Directed, undirected, weighted, unweighted.</li> <li>Useful for representing networks, social connections, and pathfinding algorithms.</li> </ul> </li> <li> <p>Hash Tables</p> <ul> <li>A collection of key-value pairs, where keys are hashed to produce an index.</li> <li>Provides average-case constant time complexity for insertions, deletions, and lookups.</li> <li>Useful for fast data retrieval, indexing.</li> </ul> </li> </ol>"},{"location":"data-structure/#why-data-structures-are-useful","title":"Why Data Structures are Useful","text":"<ol> <li> <p>Efficiency</p> <ul> <li>Data structures allow efficient data management and retrieval.</li> <li>Example: Hash tables provide constant time complexity for lookup operations.</li> </ul> </li> <li> <p>Organization</p> <ul> <li>Data structures provide a systematic way to organize data.</li> <li>Example: Trees can represent hierarchical relationships, making them suitable for file systems.</li> </ul> </li> <li> <p>Scalability</p> <ul> <li>Proper data structures help handle large datasets effectively.</li> <li>Example: B-trees are used in databases to manage large amounts of data and maintain balance.</li> </ul> </li> <li> <p>Optimization</p> <ul> <li>Data structures optimize operations like searching, sorting, and indexing.</li> <li>Example: Binary search trees enable efficient searching and sorting operations.</li> </ul> </li> <li> <p>Memory Management</p> <ul> <li>Data structures help in efficient memory allocation and deallocation.</li> <li>Example: Linked lists dynamically allocate memory, reducing wastage.</li> </ul> </li> <li> <p>Algorithm Support</p> <ul> <li>Many algorithms are built on specific data structures.</li> <li>Example: Graph algorithms like Dijkstra\u2019s use graphs to find the shortest path.</li> </ul> </li> </ol>"},{"location":"data-structure/#examples-of-use-cases","title":"Examples of Use Cases","text":"<ol> <li> <p>Databases</p> <ul> <li>Use B-trees and hash tables for indexing and efficient data retrieval.</li> </ul> </li> <li> <p>Operating Systems</p> <ul> <li>Use queues for process scheduling, stacks for function call management.</li> </ul> </li> <li> <p>Networking</p> <ul> <li>Use graphs to represent and analyze network topologies.</li> </ul> </li> <li> <p>Web Development</p> <ul> <li>Use arrays and hash tables for managing and accessing data quickly.</li> </ul> </li> <li> <p>Artificial Intelligence</p> <ul> <li>Use trees and graphs for decision-making processes and representing knowledge.</li> </ul> </li> </ol>"},{"location":"data-structure/#conclusion","title":"Conclusion","text":"<p>Data structures are fundamental to computer science and software engineering. They provide the building blocks for creating efficient and scalable programs and applications. Understanding and utilizing the right data structures is crucial for optimizing performance and ensuring that systems can handle the required operations effectively.</p>"},{"location":"data-structure/array/","title":"Array","text":""},{"location":"data-structure/array/#what-is-an-array","title":"What is an Array?","text":"<p>An array is a fixed-size data structure that consists of a collection of elements of same type, each identified by an index. Arrays are one of the simplest and most widely used data structures in programming. They store elements of the same type in contiguous memory locations, allowing efficient access to any element using its index. The only downside is that the memory location has to be reserved at the time of creation and cannot be changed dynamically.</p> <p></p>"},{"location":"data-structure/array/#key-characteristics","title":"Key Characteristics","text":"<ol> <li>Fixed Size</li> <li>Homogeneous Elements</li> <li>Contiguous Memory Allocation</li> <li>Indexed Access</li> </ol>"},{"location":"data-structure/array/#types-of-arrays","title":"Types of Arrays","text":"<ol> <li> <p>One-Dimensional Array: A linear array where elements are accessed using a single index.</p> <ul> <li>Example: <code>int arr[5] = {1, 2, 3, 4, 5};</code></li> </ul> </li> <li> <p>Multi-Dimensional Array: Arrays with more than one dimension, such as two-dimensional arrays (matrices).</p> <ul> <li>Example: <code>int matrix[3][3] = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};</code></li> </ul> </li> <li> <p>Dynamic Arrays: Arrays that can change size dynamically during runtime, such as <code>vectors</code> in C++ or <code>ArrayList</code> in Java.</p> </li> </ol>"},{"location":"data-structure/array/#operations-on-arrays","title":"Operations on Arrays","text":"<ol> <li> <p>Accessing Elements</p> <ul> <li>Elements are accessed using their index.</li> <li>Example: <code>arr[2]</code> accesses the third element in the array <code>arr</code>.</li> </ul> </li> <li> <p>Insertion</p> <ul> <li>Adding an element at a specific index requires shifting elements to make space.</li> <li>Example: To insert an element at index 2, shift elements from index 2 onwards and then place the new element.</li> </ul> </li> <li> <p>Deletion</p> <ul> <li>Removing an element requires shifting elements to fill the gap.</li> <li>Example: To delete an element at index 2, shift elements from index 3 onwards one position to the left.</li> </ul> </li> <li> <p>Traversal</p> <ul> <li>Visiting each element of the array for processing.</li> <li>Example: Using a loop to print all elements of the array.</li> </ul> </li> <li> <p>Searching</p> <ul> <li>Finding the index of an element in the array.</li> <li>Linear Search: O(n) time complexity.</li> <li>Binary Search: O(log n) time complexity (requires sorted array).</li> </ul> </li> <li> <p>Sorting</p> <ul> <li>Arranging elements in a specific order (ascending or descending).</li> <li>Example: Using sorting algorithms like Quick Sort, Merge Sort, or Bubble Sort.</li> </ul> </li> </ol>"},{"location":"data-structure/array/#advantages-of-arrays","title":"Advantages of Arrays","text":"<ol> <li>Constant-Time Access: Elements can be accessed in O(1) time using their index.</li> <li>Memory Efficiency: Arrays have a low memory overhead compared to other data structures like linked lists.</li> <li>Cache-Friendly: Contiguous memory allocation makes arrays more cache-friendly, leading to better performance in terms of memory access speed.</li> </ol>"},{"location":"data-structure/array/#disadvantages-of-arrays","title":"Disadvantages of Arrays","text":"<ol> <li>Fixed Size: The size of the array is fixed and cannot be changed after its creation, leading to potential wastage of memory or insufficient space.</li> <li>Inefficient Insertion/Deletion: Inserting or deleting elements, especially in the middle, can be inefficient as it requires shifting elements.</li> <li>Homogeneous Data: Arrays can only store elements of the same type, limiting flexibility.</li> </ol>"},{"location":"data-structure/array/#applications-of-arrays","title":"Applications of Arrays","text":"<ol> <li>Storing and Accessing Data: Arrays are used to store data that needs to be accessed quickly using indices.</li> <li>Implementation of Other Data Structures: Arrays are the foundation for implementing other data structures like stacks, queues, and hash tables.</li> <li>Mathematical Computations: Arrays are used in scientific computations and matrix operations.</li> <li>Graphics and Image Processing: Arrays are used to store pixel values and manipulate images.</li> </ol>"},{"location":"data-structure/array/implementation/","title":"Implementation","text":""},{"location":"data-structure/array/implementation/#example","title":"Example","text":"<p>Here is a simple example of how arrays are used in C++:</p> <pre><code>#include &lt;iostream&gt;\nusing namespace std;\n\nint main() {\n    // Declare and initialize an array\n    int arr[5] = {1, 2, 3, 4, 5};\n\n    // Access elements\n    cout &lt;&lt; \"Element at index 2: \" &lt;&lt; arr[2] &lt;&lt; endl; // Output: 3\n\n    // Insert an element (inefficient)\n    int n = 5; // Current size of the array\n    int newArr[6];\n    for (int i = 0; i &lt; n; i++) {\n        newArr[i] = arr[i];\n    }\n    newArr[2] = 10; // Inserting 10 at index 2\n    for (int i = 2; i &lt; n; i++) {\n        newArr[i + 1] = arr[i];\n    }\n    n++;\n    cout &lt;&lt; \"Array after insertion: \";\n    for (int i = 0; i &lt; n; i++) {\n        cout &lt;&lt; newArr[i] &lt;&lt; \" \";\n    }\n    cout &lt;&lt; endl;\n\n    // Delete an element (inefficient)\n    for (int i = 2; i &lt; n - 1; i++) {\n        newArr[i] = newArr[i + 1];\n    }\n    n--;\n    cout &lt;&lt; \"Array after deletion: \";\n    for (int i = 0; i &lt; n; i++) {\n        cout &lt;&lt; newArr[i] &lt;&lt; \" \";\n    }\n    cout &lt;&lt; endl;\n\n    // Traverse the array\n    cout &lt;&lt; \"Traversing the array: \";\n    for (int i = 0; i &lt; n; i++) {\n        cout &lt;&lt; newArr[i] &lt;&lt; \" \";\n    }\n    cout &lt;&lt; endl;\n\n    return 0;\n}\n</code></pre> <p>In this example: - We declare and initialize an array. - We access an element using its index. - We insert a new element at a specific index, which requires shifting elements. - We delete an element, which also requires shifting elements. - We traverse the array and print each element.</p>"},{"location":"data-structure/graph/","title":"Understanding Graphs","text":"<p>A graph is a collection of nodes (also called vertices) and edges that connect pairs of nodes. Graphs can be: - Directed or Undirected: In a directed graph, edges have a direction, while in an undirected graph, edges do not. - Weighted or Unweighted: In a weighted graph, edges have weights or costs associated with them. In an unweighted graph, all edges are considered equal. - Sparse or Dense: A sparse graph has relatively few edges compared to the number of nodes, while a dense graph has many edges.</p> <p>Graphs are used to represent various real-world systems such as social networks, transportation networks, and communication networks.</p> <p></p>"},{"location":"data-structure/graph/implementation/","title":"Implementation","text":""},{"location":"data-structure/graph/implementation/#java-implementation-of-a-graph","title":"Java Implementation of a Graph","text":"<p>Here\u2019s a simple implementation of a graph in Java using an adjacency list representation.</p>"},{"location":"data-structure/graph/implementation/#graph-class","title":"Graph Class","text":"<pre><code>import java.util.*;\n\nclass Graph {\n    private int numVertices;\n    private LinkedList&lt;Integer&gt;[] adjLists;\n\n    // Constructor\n    public Graph(int vertices) {\n        numVertices = vertices;\n        adjLists = new LinkedList[vertices];\n        for (int i = 0; i &lt; vertices; i++) {\n            adjLists[i] = new LinkedList&lt;&gt;();\n        }\n    }\n\n    // Add edge\n    public void addEdge(int src, int dest) {\n        adjLists[src].add(dest);\n        adjLists[dest].add(src); // For undirected graph, add both connections\n    }\n\n    // Print the graph\n    public void printGraph() {\n        for (int i = 0; i &lt; numVertices; i++) {\n            System.out.print(\"Vertex \" + i + \":\");\n            for (Integer node : adjLists[i]) {\n                System.out.print(\" -&gt; \" + node);\n            }\n            System.out.println();\n        }\n    }\n\n    // Depth First Search (DFS) traversal\n    public void DFS(int vertex) {\n        boolean[] visited = new boolean[numVertices];\n        DFSUtil(vertex, visited);\n    }\n\n    private void DFSUtil(int vertex, boolean[] visited) {\n        visited[vertex] = true;\n        System.out.print(vertex + \" \");\n\n        for (int adj : adjLists[vertex]) {\n            if (!visited[adj]) {\n                DFSUtil(adj, visited);\n            }\n        }\n    }\n\n    // Breadth First Search (BFS) traversal\n    public void BFS(int startVertex) {\n        boolean[] visited = new boolean[numVertices];\n        LinkedList&lt;Integer&gt; queue = new LinkedList&lt;&gt;();\n\n        visited[startVertex] = true;\n        queue.add(startVertex);\n\n        while (queue.size() != 0) {\n            int vertex = queue.poll();\n            System.out.print(vertex + \" \");\n\n            for (int adj : adjLists[vertex]) {\n                if (!visited[adj]) {\n                    visited[adj] = true;\n                    queue.add(adj);\n                }\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        Graph graph = new Graph(5);\n\n        graph.addEdge(0, 1);\n        graph.addEdge(0, 4);\n        graph.addEdge(1, 2);\n        graph.addEdge(1, 3);\n        graph.addEdge(1, 4);\n        graph.addEdge(2, 3);\n        graph.addEdge(3, 4);\n\n        graph.printGraph();\n\n        System.out.println(\"Depth First Search starting from vertex 0:\");\n        graph.DFS(0);\n\n        System.out.println(\"\\nBreadth First Search starting from vertex 0:\");\n        graph.BFS(0);\n    }\n}\n</code></pre>"},{"location":"data-structure/graph/implementation/#explanation","title":"Explanation","text":"<p>Graph Class:</p> <pre><code>class Graph {\n    private int numVertices;\n    private LinkedList&lt;Integer&gt;[] adjLists;\n\n    // Constructor\n    public Graph(int vertices) {\n        numVertices = vertices;\n        adjLists = new LinkedList[vertices];\n        for (int i = 0; i &lt; vertices; i++) {\n            adjLists[i] = new LinkedList&lt;&gt;();\n        }\n    }\n}\n</code></pre> <p>The <code>Graph</code> class represents a graph with a specified number of vertices. The adjacency list is used to store the graph, with each vertex having a linked list of its adjacent vertices.</p> <p>Add Edge Function:</p> <pre><code>public void addEdge(int src, int dest) {\n    adjLists[src].add(dest);\n    adjLists[dest].add(src); // For undirected graph, add both connections\n}\n</code></pre> <p>This function adds an edge between two vertices in the graph. For an undirected graph, edges are added in both directions.</p> <p>Print Graph Function:</p> <pre><code>public void printGraph() {\n    for (int i = 0; i &lt; numVertices; i++) {\n        System.out.print(\"Vertex \" + i + \":\");\n        for (Integer node : adjLists[i]) {\n            System.out.print(\" -&gt; \" + node);\n        }\n        System.out.println();\n    }\n}\n</code></pre> <p>This function prints the graph's adjacency list representation.</p> <p>Depth First Search (DFS) Function:</p> <pre><code>public void DFS(int vertex) {\n    boolean[] visited = new boolean[numVertices];\n    DFSUtil(vertex, visited);\n}\n\nprivate void DFSUtil(int vertex, boolean[] visited) {\n    visited[vertex] = true;\n    System.out.print(vertex + \" \");\n\n    for (int adj : adjLists[vertex]) {\n        if (!visited[adj]) {\n            DFSUtil(adj, visited);\n        }\n    }\n}\n</code></pre> <p>The DFS function recursively traverses the graph starting from a given vertex. It uses a boolean array to keep track of visited vertices.</p> <p>Breadth First Search (BFS) Function:</p> <pre><code>public void BFS(int startVertex) {\n    boolean[] visited = new boolean[numVertices];\n    LinkedList&lt;Integer&gt; queue = new LinkedList&lt;&gt;();\n\n    visited[startVertex] = true;\n    queue.add(startVertex);\n\n    while (queue.size() != 0) {\n        int vertex = queue.poll();\n        System.out.print(vertex + \" \");\n\n        for (int adj : adjLists[vertex]) {\n            if (!visited[adj]) {\n                visited[adj] = true;\n                queue.add(adj);\n            }\n        }\n    }\n}\n</code></pre> <p>The BFS function iteratively traverses the graph starting from a given vertex using a queue to manage the traversal.</p> <p>Main Method:</p> <pre><code>public static void main(String[] args) {\n    Graph graph = new Graph(5);\n\n    graph.addEdge(0, 1);\n    graph.addEdge(0, 4);\n    graph.addEdge(1, 2);\n    graph.addEdge(1, 3);\n    graph.addEdge(1, 4);\n    graph.addEdge(2, 3);\n    graph.addEdge(3, 4);\n\n    graph.printGraph();\n\n    System.out.println(\"Depth First Search starting from vertex 0:\");\n    graph.DFS(0);\n\n    System.out.println(\"\\nBreadth First Search starting from vertex 0:\");\n    graph.BFS(0);\n}\n</code></pre>"},{"location":"data-structure/graph/implementation/#conclusion","title":"Conclusion","text":"<p>This example demonstrates how to implement a graph in Java using an adjacency list. The <code>Graph</code> class includes methods to add edges, print the graph, and perform both Depth First Search (DFS) and Breadth First Search (BFS) traversals. This basic implementation can be extended to handle more complex operations and types of graphs, such as weighted and directed graphs.</p>"},{"location":"data-structure/hash-table/","title":"Hash Table","text":""},{"location":"data-structure/hash-table/#what-is-a-hash-table","title":"What is a Hash Table?","text":"<p>A hash table (or hash map) is a data structure that provides an efficient way to store and retrieve data. It uses a technique called hashing to map keys to values, allowing for average-case constant time complexity (<code>O(1)</code>) for operations like insertion, deletion, and lookup.</p>"},{"location":"data-structure/hash-table/#key-concepts","title":"Key Concepts","text":"<ol> <li> <p>Hash Function</p> <ul> <li>Purpose: A hash function takes an input (or key) and returns an integer known as a hash code. The hash code is then used as an index into an array where the value corresponding to the key is stored.</li> <li>Properties: A good hash function should distribute keys uniformly across the hash table to minimize collisions.</li> </ul> </li> <li> <p>Buckets</p> <ul> <li>Purpose: The array in a hash table is often referred to as a series of buckets, where each bucket can hold multiple key-value pairs.</li> <li>Structure: Each bucket corresponds to an index in the array, derived from the hash code of the key.</li> </ul> </li> <li> <p>Collisions</p> <ul> <li>Definition: Collisions occur when two keys hash to the same index in the array.</li> <li>Handling Collisions: There are several strategies to handle collisions, including chaining and open addressing.</li> </ul> </li> </ol>"},{"location":"data-structure/hash-table/#collision-handling-techniques","title":"Collision Handling Techniques","text":"<ol> <li> <p>Chaining</p> <ul> <li>Description: Each bucket in the hash table contains a list (or another data structure like a linked list) of key-value pairs. When a collision occurs, the new key-value pair is added to the list at the appropriate bucket.</li> <li>Pros: Simple to implement and handles collisions effectively.</li> <li>Cons: Can degrade to O(n) time complexity if many collisions occur, resulting in long lists.</li> </ul> </li> <li> <p>Open Addressing</p> <ul> <li>Description: Instead of storing multiple elements in a single bucket, open addressing places each element directly into the array. If a collision occurs, the algorithm probes (or searches) for the next empty slot in the array.</li> <li>Probing Methods:<ul> <li>Linear Probing: Increment the index by one until an empty slot is found.</li> <li>Quadratic Probing: Use a quadratic function to determine the next slot.</li> <li>Double Hashing: Use a second hash function to determine the step size for probing.</li> </ul> </li> <li>Pros: Can lead to better cache performance due to fewer linked lists and reduced memory overhead.</li> <li>Cons: More complex implementation and can suffer from clustering, where groups of occupied slots create long probe sequences.</li> </ul> </li> </ol>"},{"location":"data-structure/hash-table/#operations-on-hash-tables","title":"Operations on Hash Tables","text":"<ol> <li> <p>Insertion</p> <ul> <li>Calculate the hash code of the key.</li> <li>Determine the index from the hash code.</li> <li>Place the key-value pair at the appropriate bucket (or probe to find an empty slot in open addressing).</li> </ul> </li> <li> <p>Deletion</p> <ul> <li>Calculate the hash code of the key.</li> <li>Determine the index from the hash code.</li> <li>Remove the key-value pair from the bucket or mark the slot as deleted in open addressing.</li> </ul> </li> <li> <p>Lookup</p> <ul> <li>Calculate the hash code of the key.</li> <li>Determine the index from the hash code.</li> <li>Retrieve the value from the appropriate bucket or probe for the key in open addressing.</li> </ul> </li> </ol>"},{"location":"data-structure/hash-table/#advantages-of-hash-tables","title":"Advantages of Hash Tables","text":"<ul> <li>Speed: Average-case constant time complexity (O(1)) for insertion, deletion, and lookup operations.</li> <li>Efficiency: Efficient memory usage when the load factor (number of elements / number of buckets) is maintained.</li> </ul>"},{"location":"data-structure/hash-table/#disadvantages-of-hash-tables","title":"Disadvantages of Hash Tables","text":"<ul> <li>Collisions: Handling collisions can add complexity and degrade performance.</li> <li>Memory Overhead: Requires a balance between speed and memory usage, as too many empty buckets waste space, while too few increase collisions.</li> <li>Complexity: Implementation can be complex, especially with open addressing and probing strategies.</li> </ul>"},{"location":"data-structure/hash-table/#use-cases","title":"Use Cases","text":"<ul> <li>Databases: Hash tables are used in indexing and quick data retrieval.</li> <li>Caches: Storing recently accessed data for quick lookups.</li> <li>Symbol Tables: In compilers and interpreters to store variable bindings.</li> <li>Sets and Maps: Implementing abstract data types like sets and dictionaries.</li> </ul>"},{"location":"data-structure/hash-table/implementation/","title":"Implementation","text":""},{"location":"data-structure/hash-table/implementation/#example","title":"Example","text":"<p>Here's a simple implementation of a hash table using chaining in Python:</p> <pre><code>class HashTable:\n    def __init__(self, size=10):\n        self.size = size\n        self.table = [[] for _ in range(size)]\n\n    def hash_function(self, key):\n        return hash(key) % self.size\n\n    def insert(self, key, value):\n        index = self.hash_function(key)\n        for pair in self.table[index]:\n            if pair[0] == key:\n                pair[1] = value\n                return\n        self.table[index].append([key, value])\n\n    def lookup(self, key):\n        index = self.hash_function(key)\n        for pair in self.table[index]:\n            if pair[0] == key:\n                return pair[1]\n        return None\n\n    def delete(self, key):\n        index = self.hash_function(key)\n        for i, pair in enumerate(self.table[index]):\n            if pair[0] == key:\n                del self.table[index][i]\n                return\n\n# Example usage:\nhash_table = HashTable()\nhash_table.insert(\"apple\", 10)\nprint(hash_table.lookup(\"apple\"))  # Output: 10\nhash_table.delete(\"apple\")\nprint(hash_table.lookup(\"apple\"))  # Output: None\n</code></pre> <p>In this example, the <code>HashTable</code> class uses a list of lists to handle collisions through chaining. The <code>hash_function</code> method computes the index for a given key, and the <code>insert</code>, <code>lookup</code>, and <code>delete</code> methods manage the key-value pairs accordingly.</p>"},{"location":"data-structure/linked-list/","title":"Linked List","text":""},{"location":"data-structure/linked-list/#what-is-a-linked-list","title":"What is a Linked List?","text":"<p>A linked list is a linear data structure in which elements, called nodes, are connected using pointers. Each node contains two parts: the data and a reference (or pointer) to the next node in the sequence. Unlike arrays, linked lists do not store elements in contiguous memory locations.</p>"},{"location":"data-structure/linked-list/#key-characteristics","title":"Key Characteristics","text":"<ol> <li>Dynamic Size: The size of a linked list can grow or shrink dynamically as nodes are added or removed.</li> <li>Non-contiguous Memory Allocation: Nodes are not stored in contiguous memory locations; each node points to the next node.</li> <li>Pointer-based Structure: Each node contains a pointer/reference to the next node in the list.</li> </ol>"},{"location":"data-structure/linked-list/#types-of-linked-lists","title":"Types of Linked Lists","text":"<ol> <li> <p>Singly Linked List</p> <ul> <li>Each node points to the next node.</li> <li>The last node points to <code>null</code> (or <code>None</code> in Python).</li> <li>Example:  <code>plaintext  Head -&gt; [Data|Next] -&gt; [Data|Next] -&gt; [Data|Next] -&gt; null</code></li> </ul> </li> <li> <p>Doubly Linked List</p> <ul> <li>Each node contains a pointer to both the next and the previous node.</li> <li>Provides bidirectional traversal.</li> <li>Example:  <code>plaintext  Head &lt;-&gt; [Prev|Data|Next] &lt;-&gt; [Prev|Data|Next] &lt;-&gt; [Prev|Data|Next] &lt;-&gt; null</code></li> </ul> </li> <li> <p>Circular Linked List</p> <ul> <li>The last node points back to the first node, forming a circle.</li> <li>Can be singly or doubly linked.</li> <li>Example (Singly):  <code>plaintext  Head -&gt; [Data|Next] -&gt; [Data|Next] -&gt; [Data|Next] -&gt; Head</code></li> </ul> </li> </ol>"},{"location":"data-structure/linked-list/#advantages-of-linked-lists","title":"Advantages of Linked Lists","text":"<ol> <li>Dynamic Size: Can grow and shrink in size during runtime.</li> <li>Ease of Insertion/Deletion: Inserting and deleting nodes does not require shifting elements, unlike arrays.</li> <li>Efficient Memory Usage: Memory is allocated as needed.</li> </ol>"},{"location":"data-structure/linked-list/#disadvantages-of-linked-lists","title":"Disadvantages of Linked Lists","text":"<ol> <li>Memory Overhead: Requires extra memory for pointers.</li> <li>Sequential Access: Nodes must be accessed sequentially from the head, making random access inefficient.</li> <li>Complexity: More complex to implement and manage compared to arrays.</li> </ol>"},{"location":"data-structure/linked-list/#applications-of-linked-lists","title":"Applications of Linked Lists","text":"<ol> <li>Dynamic Memory Allocation: Used in applications where dynamic memory allocation is required.</li> <li>Implementation of Other Data Structures: Used to implement stacks, queues, and graphs.</li> <li>Navigation Systems: Used in navigation systems where items need to be frequently added or removed.</li> </ol>"},{"location":"data-structure/linked-list/implementation/","title":"Implementation","text":""},{"location":"data-structure/linked-list/implementation/#operations-on-linked-lists","title":"Operations on Linked Lists","text":""},{"location":"data-structure/linked-list/implementation/#insertion","title":"Insertion","text":""},{"location":"data-structure/linked-list/implementation/#at-the-beginning","title":"At the Beginning","text":"<p>Add a new node at the start of the list.</p> <pre><code>public void insertAtBeginning(int data) {\n    Node newNode = new Node(data);\n    newNode.next = head;\n    head = newNode;\n}\n</code></pre>"},{"location":"data-structure/linked-list/implementation/#at-the-end","title":"At the End","text":"<p>Add a new node at the end of the list.</p> <pre><code>public void insertAtEnd(int data) {\n    Node newNode = new Node(data);\n    if (head == null) {\n        head = newNode;\n        return;\n    }\n    Node last = head;\n    while (last.next != null) {\n        last = last.next;\n    }\n    last.next = newNode;\n}\n</code></pre>"},{"location":"data-structure/linked-list/implementation/#at-a-specific-position","title":"At a Specific Position","text":"<p>Insert a new node after a given node.</p> <pre><code>public void insertAfter(Node prevNode, int data) {\nif (prevNode == null) {\n    return;\n}\nNode newNode = new Node(data);\nif (head == null) {\n    head = newNode;\n    return;\n}\nNode last = head;\nwhile (last.val != prevNode.val) {\n    last = last.next;\n}\nNode temp = last.next;\nlast.next = newNode;\nnewNode.next = temp;\n}\n</code></pre>"},{"location":"data-structure/linked-list/implementation/#deletion","title":"Deletion","text":""},{"location":"data-structure/linked-list/implementation/#by-a-key","title":"By a key","text":"<p>Remove a node with a given key.</p> <pre><code>public void deleteNode(int key) {\n    Node temp = head;\n    Node prev = null;\n\n    // If head node holds the key to be deleted\n    if (temp != null &amp;&amp; temp.data == key) {\n        head = temp.next;\n        return;\n    }\n\n    // Search for the key to be deleted\n    while (temp != null &amp;&amp; temp.data != key) {\n        prev = temp;\n        temp = temp.next;\n    }\n\n    // If key is not present\n    if (temp == null) return;\n\n    // Unlink the node from linked list\n    prev.next = temp.next;\n}\n</code></pre>"},{"location":"data-structure/linked-list/implementation/#traversal","title":"Traversal","text":"<p>Visiting each node of the list to process the data.</p> <pre><code>void printList(Node* node) {\n    while (node != NULL) {\n        cout &lt;&lt; node-&gt;data &lt;&lt; \" \";\n        node = node-&gt;next;\n    }\n}\n</code></pre>"},{"location":"data-structure/linked-list/implementation/#searching","title":"Searching","text":"<p>Finding a node with a specific value.</p> <pre><code>bool search(Node* head, int key) {\n    Node* current = head;\n    while (current != NULL) {\n        if (current-&gt;data == key) return true;\n        current = current-&gt;next;\n    }\n    return false;\n}\n</code></pre>"},{"location":"data-structure/linked-list/implementation/#example","title":"Example","text":"<p>Here is a simple example of a singly linked list in C++:</p> <pre><code>class Node {\n    int data;\n    Node next;\n    Node(int data) {\n        this.data = data;\n        this.next = null;\n    }\n}\n\npublic class LinkedList {\n    Node head;\n\n    // Insert at the beginning\n    public void insertAtBeginning(int data) {\n        Node newNode = new Node(data);\n        newNode.next = head;\n        head = newNode;\n    }\n\n    // Insert at the end\n    public void insertAtEnd(int data) {\n        Node newNode = new Node(data);\n        if (head == null) {\n            head = newNode;\n            return;\n        }\n        Node last = head;\n        while (last.next != null) {\n            last = last.next;\n        }\n        last.next = newNode;\n    }\n\n    // Delete a node by key\n    public void deleteNode(int key) {\n        Node temp = head;\n        Node prev = null;\n\n        // If head node holds the key to be deleted\n        if (temp != null &amp;&amp; temp.data == key) {\n            head = temp.next;\n            return;\n        }\n\n        // Search for the key to be deleted\n        while (temp != null &amp;&amp; temp.data != key) {\n            prev = temp;\n            temp = temp.next;\n        }\n\n        // If key is not present\n        if (temp == null) return;\n\n        // Unlink the node from linked list\n        prev.next = temp.next;\n    }\n\n    // Print the linked list\n    public void printList() {\n        Node temp = head;\n        while (temp != null) {\n            System.out.print(temp.data + \" \");\n            temp = temp.next;\n        }\n        System.out.println();\n    }\n\n    public static void main(String[] args) {\n        LinkedList llist = new LinkedList();\n\n        llist.insertAtBeginning(1);\n        llist.insertAtEnd(2);\n        llist.insertAtEnd(3);\n        llist.insertAtBeginning(0);\n\n        System.out.println(\"Linked List:\");\n        llist.printList();\n\n        llist.deleteNode(2);\n        System.out.println(\"Linked List after deletion of 2:\");\n        llist.printList();\n    }\n}\n</code></pre> <p>In this example: - A singly linked list is created with nodes inserted at the beginning and end. - A node is deleted based on its value. - The list is printed before and after deletion to show the changes.</p>"},{"location":"data-structure/queue/","title":"Understanding Queue","text":"<p>A queue is a linear data structure that follows the First-In-First-Out (FIFO) principle, where elements are inserted from one end called the rear (or back) and removed from the other end called the front. This makes queues ideal for scenarios where the order of operations matters, such as task scheduling, handling requests in a web server, or managing tasks in an operating system.</p>"},{"location":"data-structure/queue/implementation/","title":"Implementation","text":""},{"location":"data-structure/queue/implementation/#java-implementation-of-queue","title":"Java Implementation of Queue","text":"<p>Java provides a built-in interface for queues in the <code>java.util</code> package. The most common implementation of a queue is the <code>LinkedList</code> class, which implements the <code>Queue</code> interface.</p> <p>Here's a basic example of using a queue in Java:</p>"},{"location":"data-structure/queue/implementation/#using-linkedlist-to-implement-a-queue","title":"Using LinkedList to Implement a Queue","text":"<pre><code>import java.util.LinkedList;\nimport java.util.Queue;\n\npublic class QueueExample {\n    public static void main(String[] args) {\n        // Create a Queue using LinkedList\n        Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;();\n\n        // Add elements to the queue (Enqueue operation)\n        queue.add(10);\n        queue.add(20);\n        queue.add(30);\n        queue.add(40);\n\n        // Display the elements in the queue\n        System.out.println(\"Queue: \" + queue);\n\n        // Remove an element from the queue (Dequeue operation)\n        int removedElement = queue.remove();\n        System.out.println(\"Removed Element: \" + removedElement);\n\n        // Display the elements in the queue after dequeue\n        System.out.println(\"Queue after Dequeue: \" + queue);\n\n        // Peek at the front element of the queue without removing it\n        int frontElement = queue.peek();\n        System.out.println(\"Front Element: \" + frontElement);\n\n        // Check if the queue is empty\n        boolean isEmpty = queue.isEmpty();\n        System.out.println(\"Is the Queue empty? \" + isEmpty);\n    }\n}\n</code></pre>"},{"location":"data-structure/queue/implementation/#explanation","title":"Explanation:","text":"<p>Import the Queue Interface and LinkedList Class:</p> <pre><code>import java.util.LinkedList;\nimport java.util.Queue;\n</code></pre> <p>Create a Queue:</p> <pre><code>Queue&lt;Integer&gt; queue = new LinkedList&lt;&gt;();\n</code></pre> <p>Here, we create a queue of integers using the <code>LinkedList</code> class, which implements the <code>Queue</code> interface.</p> <p>Add Elements to the Queue (Enqueue Operation):</p> <pre><code>queue.add(10);\nqueue.add(20);\nqueue.add(30);\nqueue.add(40);\n</code></pre> <p>We use the <code>add()</code> method to insert elements into the queue. Elements are added to the rear of the queue.</p> <p>Display the Queue:</p> <pre><code>System.out.println(\"Queue: \" + queue);\n</code></pre> <p>This prints the current elements in the queue.</p> <p>Remove an Element from the Queue (Dequeue Operation):</p> <pre><code>int removedElement = queue.remove();\nSystem.out.println(\"Removed Element: \" + removedElement);\n</code></pre> <p>The <code>remove()</code> method removes and returns the front element of the queue.</p> <p>Display the Queue after Dequeue:</p> <pre><code>System.out.println(\"Queue after Dequeue: \" + queue);\n</code></pre> <p>This prints the queue after removing an element.</p> <p>Peek at the Front Element:</p> <pre><code>int frontElement = queue.peek();\nSystem.out.println(\"Front Element: \" + frontElement);\n</code></pre> <p>The <code>peek()</code> method returns the front element without removing it from the queue.</p> <p>Check if the Queue is Empty:</p> <pre><code>boolean isEmpty = queue.isEmpty();\nSystem.out.println(\"Is the Queue empty? \" + isEmpty);\n</code></pre> <p>The <code>isEmpty()</code> method checks if the queue is empty and returns a boolean value.</p>"},{"location":"data-structure/queue/implementation/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the basic operations of a queue, including adding elements (enqueue), removing elements (dequeue), peeking at the front element, and checking if the queue is empty. The <code>LinkedList</code> class in Java provides an easy way to implement queues and leverage the powerful features of the Java Collections Framework.</p>"},{"location":"data-structure/stack/","title":"Understanding Stack","text":"<p>A stack is a linear data structure that follows the Last-In-First-Out (LIFO) principle, where elements are added (pushed) and removed (popped) from the same end, referred to as the top of the stack. Stacks are used in various applications such as expression evaluation, backtracking algorithms, and function call management in programming languages.</p>"},{"location":"data-structure/stack/implementation/","title":"Implementation","text":""},{"location":"data-structure/stack/implementation/#java-implementation-of-stack","title":"Java Implementation of Stack","text":"<p>Java provides a built-in class <code>Stack</code> in the <code>java.util</code> package. Here\u2019s how to use it:</p>"},{"location":"data-structure/stack/implementation/#using-the-stack-class","title":"Using the Stack Class","text":"<pre><code>import java.util.Stack;\n\npublic class StackExample {\n    public static void main(String[] args) {\n        // Create a Stack\n        Stack&lt;Integer&gt; stack = new Stack&lt;&gt;();\n\n        // Push elements onto the stack\n        stack.push(10);\n        stack.push(20);\n        stack.push(30);\n        stack.push(40);\n\n        // Display the elements in the stack\n        System.out.println(\"Stack: \" + stack);\n\n        // Pop an element from the stack\n        int poppedElement = stack.pop();\n        System.out.println(\"Popped Element: \" + poppedElement);\n\n        // Display the stack after pop\n        System.out.println(\"Stack after Pop: \" + stack);\n\n        // Peek at the top element of the stack without removing it\n        int topElement = stack.peek();\n        System.out.println(\"Top Element: \" + topElement);\n\n        // Check if the stack is empty\n        boolean isEmpty = stack.isEmpty();\n        System.out.println(\"Is the Stack empty? \" + isEmpty);\n    }\n}\n</code></pre>"},{"location":"data-structure/stack/implementation/#explanation","title":"Explanation:","text":"<p>Import the Stack Class:</p> <pre><code>import java.util.Stack;\n</code></pre> <p>Create a Stack:</p> <pre><code>Stack&lt;Integer&gt; stack = new Stack&lt;&gt;();\n</code></pre> <p>Here, we create a stack of integers.</p> <p>Push Elements onto the Stack:</p> <pre><code>stack.push(10);\nstack.push(20);\nstack.push(30);\nstack.push(40);\n</code></pre> <p>We use the <code>push()</code> method to add elements to the top of the stack.</p> <p>Display the Stack:</p> <pre><code>System.out.println(\"Stack: \" + stack);\n</code></pre> <p>This prints the current elements in the stack.</p> <p>Pop an Element from the Stack:</p> <pre><code>int poppedElement = stack.pop();\nSystem.out.println(\"Popped Element: \" + poppedElement);\n</code></pre> <p>The <code>pop()</code> method removes and returns the top element of the stack.</p> <p>Display the Stack after Pop:</p> <pre><code>System.out.println(\"Stack after Pop: \" + stack);\n</code></pre> <p>This prints the stack after removing an element.</p> <p>Peek at the Top Element:</p> <pre><code>int topElement = stack.peek();\nSystem.out.println(\"Top Element: \" + topElement);\n</code></pre> <p>The <code>peek()</code> method returns the top element without removing it from the stack.</p> <p>Check if the Stack is Empty:</p> <pre><code>boolean isEmpty = stack.isEmpty();\nSystem.out.println(\"Is the Stack empty? \" + isEmpty);\n</code></pre> <p>The <code>isEmpty()</code> method checks if the stack is empty and returns a boolean value.</p>"},{"location":"data-structure/stack/implementation/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the basic operations of a stack, including pushing elements, popping elements, peeking at the top element, and checking if the stack is empty. The <code>Stack</code> class in Java provides a convenient way to implement stacks and use their core functionalities in applications.</p>"},{"location":"data-structure/trees/tree-balanced-tree/","title":"Tree balanced tree","text":""},{"location":"data-structure/trees/tree-balanced-tree/#understanding-balanced-trees","title":"Understanding Balanced Trees","text":"<p>A balanced tree is a type of binary tree in which the height of the left and right subtrees of any node differs by no more than one. Balanced trees ensure that operations such as insertion, deletion, and lookup can be performed in O(log n) time. Common types of balanced trees include AVL trees and Red-Black trees.</p>"},{"location":"data-structure/trees/tree-balanced-tree/#java-implementation-of-avl-tree","title":"Java Implementation of AVL Tree","text":"<p>An AVL (Adelson-Velsky and Landis) tree is a self-balancing binary search tree where the difference in heights between the left and right subtrees (balance factor) of any node is at most one. If the balance factor of any node becomes more than one, rotations are performed to balance the tree.</p> <p>Here\u2019s how you can implement an AVL tree in Java:</p>"},{"location":"data-structure/trees/tree-balanced-tree/#node-class","title":"Node Class","text":"<pre><code>class Node {\n    int key, height;\n    Node left, right;\n\n    Node(int d) {\n        key = d;\n        height = 1;\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-balanced-tree/#avltree-class-with-basic-operations","title":"AVLTree Class with Basic Operations","text":"<pre><code>public class AVLTree {\n    Node root;\n\n    // A utility function to get the height of the tree\n    int height(Node N) {\n        if (N == null)\n            return 0;\n        return N.height;\n    }\n\n    // A utility function to get the maximum of two integers\n    int max(int a, int b) {\n        return (a &gt; b) ? a : b;\n    }\n\n    // A utility function to right rotate subtree rooted with y\n    Node rightRotate(Node y) {\n        Node x = y.left;\n        Node T2 = x.right;\n\n        // Perform rotation\n        x.right = y;\n        y.left = T2;\n\n        // Update heights\n        y.height = max(height(y.left), height(y.right)) + 1;\n        x.height = max(height(x.left), height(x.right)) + 1;\n\n        // Return new root\n        return x;\n    }\n\n    // A utility function to left rotate subtree rooted with x\n    Node leftRotate(Node x) {\n        Node y = x.right;\n        Node T2 = y.left;\n\n        // Perform rotation\n        y.left = x;\n        x.right = T2;\n\n        // Update heights\n        x.height = max(height(x.left), height(x.right)) + 1;\n        y.height = max(height(y.left), height(y.right)) + 1;\n\n        // Return new root\n        return y;\n    }\n\n    // Get Balance factor of node N\n    int getBalance(Node N) {\n        if (N == null)\n            return 0;\n        return height(N.left) - height(N.right);\n    }\n\n    Node insert(Node node, int key) {\n        // Perform the normal BST rotation\n        if (node == null)\n            return (new Node(key));\n\n        if (key &lt; node.key)\n            node.left = insert(node.left, key);\n        else if (key &gt; node.key)\n            node.right = insert(node.right, key);\n        else // Duplicate keys not allowed\n            return node;\n\n        // Update height of this ancestor node\n        node.height = 1 + max(height(node.left), height(node.right));\n\n        // Get the balance factor of this ancestor node to check whether this node became unbalanced\n        int balance = getBalance(node);\n\n        // If this node becomes unbalanced, then there are 4 cases\n\n        // Left Left Case\n        if (balance &gt; 1 &amp;&amp; key &lt; node.left.key)\n            return rightRotate(node);\n\n        // Right Right Case\n        if (balance &lt; -1 &amp;&amp; key &gt; node.right.key)\n            return leftRotate(node);\n\n        // Left Right Case\n        if (balance &gt; 1 &amp;&amp; key &gt; node.left.key) {\n            node.left = leftRotate(node.left);\n            return rightRotate(node);\n        }\n\n        // Right Left Case\n        if (balance &lt; -1 &amp;&amp; key &lt; node.right.key) {\n            node.right = rightRotate(node.right);\n            return leftRotate(node);\n        }\n\n        // return the (unchanged) node pointer\n        return node;\n    }\n\n    // A utility function to print preorder traversal of the tree\n    // The function also prints height of every node\n    void preOrder(Node node) {\n        if (node != null) {\n            System.out.print(node.key + \" \");\n            preOrder(node.left);\n            preOrder(node.right);\n        }\n    }\n\n    public static void main(String[] args) {\n        AVLTree tree = new AVLTree();\n\n        /* Constructing tree given in the above figure */\n        tree.root = tree.insert(tree.root, 10);\n        tree.root = tree.insert(tree.root, 20);\n        tree.root = tree.insert(tree.root, 30);\n        tree.root = tree.insert(tree.root, 40);\n        tree.root = tree.insert(tree.root, 50);\n        tree.root = tree.insert(tree.root, 25);\n\n        /* The constructed AVL Tree would be\n                 30\n                /  \\\n              20    40\n             /  \\    \\\n           10  25    50\n        */\n        System.out.println(\"Preorder traversal\" +\n                \" of constructed tree is : \");\n        tree.preOrder(tree.root);\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-balanced-tree/#explanation","title":"Explanation","text":"<p>Node Class:</p> <pre><code>class Node {\n    int key, height;\n    Node left, right;\n\n    Node(int d) {\n        key = d;\n        height = 1;\n    }\n}\n</code></pre> <p>This class represents a node in the AVL tree, containing an integer <code>key</code>, height of the node, and pointers to the left and right children.</p> <ol> <li>AVLTree Class:</li> <li> <p>Height Function: <code>java      int height(Node N) {          if (N == null)              return 0;          return N.height;      }</code>      This utility function returns the height of the tree rooted at a given node.</p> </li> <li> <p>Maximum Function: <code>java      int max(int a, int b) {          return (a &gt; b) ? a : b;      }</code></p> </li> <li> <p>Right Rotate Function:      ```java      Node rightRotate(Node y) {          Node x = y.left;          Node T2 = x.right;</p> <pre><code> x.right = y;\n y.left = T2;\n\n y.height = max(height(y.left), height(y.right)) + 1;\n x.height = max(height(x.left), height(x.right)) + 1;\n\n return x;\n</code></pre> <p>}  ```  This function performs a right rotation on the subtree rooted with y.</p> </li> <li> <p>Left Rotate Function:      ```java      Node leftRotate(Node x) {          Node y = x.right;          Node T2 = y.left;</p> <pre><code> y.left = x;\n x.right = T2;\n\n x.height = max(height(x.left), height(x.right)) + 1;\n y.height = max(height(y.left), height(y.right)) + 1;\n\n return y;\n</code></pre> <p>}  ```  This function performs a left rotation on the subtree rooted with x.</p> </li> <li> <p>Get Balance Function: <code>java      int getBalance(Node N) {          if (N == null)              return 0;          return height(N.left) - height(N.right);      }</code>      This function returns the balance factor of a node.</p> </li> <li> <p>Insert Function:      ```java      Node insert(Node node, int key) {          if (node == null)              return (new Node(key));</p> <pre><code> if (key &lt; node.key)\n     node.left = insert(node.left, key);\n else if (key &gt; node.key)\n     node.right = insert(node.right, key);\n else\n     return node;\n\n node.height = 1 + max(height(node.left), height(node.right));\n int balance = getBalance(node);\n\n if (balance &gt; 1 &amp;&amp; key &lt; node.left.key)\n     return rightRotate(node);\n\n if (balance &lt; -1 &amp;&amp; key &gt; node.right.key)\n     return leftRotate(node);\n\n if (balance &gt; 1 &amp;&amp; key &gt; node.left.key) {\n     node.left = leftRotate(node.left);\n     return rightRotate(node);\n }\n\n if (balance &lt; -1 &amp;&amp; key &lt; node.right.key) {\n     node.right = rightRotate(node.right);\n     return leftRotate(node);\n }\n\n return node;\n</code></pre> <p>}  ```  This function inserts a new key into the AVL tree and balances it if necessary.</p> </li> <li> <p>Preorder Traversal: <code>java      void preOrder(Node node) {          if (node != null) {              System.out.print(node.key + \" \");              preOrder(node.left);              preOrder(node.right);          }      }</code></p> </li> <li> <p>Main Method:    ```java    public static void main(String[] args) {        AVLTree tree = new AVLTree();</p> <p>tree.root = tree.insert(tree.root, 10);    tree.root = tree.insert(tree.root, 20);    tree.root = tree.insert(tree.root, 30);    tree.root = tree.insert(tree.root, 40);    tree.root = tree.insert(tree.root, 50);    tree.root = tree.insert(tree.root, 25);</p> <p>System.out.println(\"Preorder traversal\" +            \" of constructed tree is : \");    tree.preOrder(tree.root);    }    ```</p> </li> </ol>"},{"location":"data-structure/trees/tree-balanced-tree/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the basic operations of an AVL tree, a type of balanced binary search tree, including insertion and balancing through rotations. The <code>AVLTree</code> class in Java provides a clear and efficient way to implement and work with balanced trees, ensuring that the tree remains balanced after every insertion operation, which allows for efficient search, insertion, and deletion operations.</p>"},{"location":"data-structure/trees/tree-binary-search-tree/","title":"Tree binary search tree","text":""},{"location":"data-structure/trees/tree-binary-search-tree/#understanding-binary-search-tree-bst","title":"Understanding Binary Search Tree (BST)","text":"<p>A Binary Search Tree (BST) is a specialized type of binary tree that maintains a sorted order of elements. In a BST: - The left subtree of a node contains only nodes with keys less than the node\u2019s key. - The right subtree of a node contains only nodes with keys greater than the node\u2019s key. - Both left and right subtrees must also be binary search trees.</p> <p>This property allows efficient search, insertion, and deletion operations, typically with a time complexity of O(log n) in a balanced BST.</p>"},{"location":"data-structure/trees/tree-binary-search-tree/#java-implementation-of-binary-search-tree","title":"Java Implementation of Binary Search Tree","text":"<p>Here's how you can implement a BST in Java:</p>"},{"location":"data-structure/trees/tree-binary-search-tree/#node-class","title":"Node Class","text":"<pre><code>class Node {\n    int data;\n    Node left, right;\n\n    public Node(int item) {\n        data = item;\n        left = right = null;\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-binary-search-tree/#binarysearchtree-class-with-basic-operations","title":"BinarySearchTree Class with Basic Operations","text":"<pre><code>public class BinarySearchTree {\n    Node root;\n\n    public BinarySearchTree() {\n        root = null;\n    }\n\n    // Insert a new node\n    void insert(int data) {\n        root = insertRec(root, data);\n    }\n\n    // Recursive insert function\n    Node insertRec(Node root, int data) {\n        if (root == null) {\n            root = new Node(data);\n            return root;\n        }\n\n        if (data &lt; root.data)\n            root.left = insertRec(root.left, data);\n        else if (data &gt; root.data)\n            root.right = insertRec(root.right, data);\n\n        return root;\n    }\n\n    // Search for a value in the BST\n    boolean search(int data) {\n        return searchRec(root, data);\n    }\n\n    // Recursive search function\n    boolean searchRec(Node root, int data) {\n        if (root == null) {\n            return false;\n        }\n        if (root.data == data) {\n            return true;\n        }\n\n        if (data &lt; root.data) {\n            return searchRec(root.left, data);\n        } else {\n            return searchRec(root.right, data);\n        }\n    }\n\n    // Inorder traversal\n    void inorder() {\n        inorderRec(root);\n    }\n\n    // Recursive inorder function\n    void inorderRec(Node root) {\n        if (root != null) {\n            inorderRec(root.left);\n            System.out.print(root.data + \" \");\n            inorderRec(root.right);\n        }\n    }\n\n    // Delete a node\n    void delete(int data) {\n        root = deleteRec(root, data);\n    }\n\n    // Recursive delete function\n    Node deleteRec(Node root, int data) {\n        if (root == null) {\n            return root;\n        }\n\n        if (data &lt; root.data) {\n            root.left = deleteRec(root.left, data);\n        } else if (data &gt; root.data) {\n            root.right = deleteRec(root.right, data);\n        } else {\n            // Node with only one child or no child\n            if (root.left == null) {\n                return root.right;\n            } else if (root.right == null) {\n                return root.left;\n            }\n\n            // Node with two children: Get the inorder successor (smallest in the right subtree)\n            root.data = minValue(root.right);\n\n            // Delete the inorder successor\n            root.right = deleteRec(root.right, root.data);\n        }\n\n        return root;\n    }\n\n    int minValue(Node root) {\n        int minValue = root.data;\n        while (root.left != null) {\n            minValue = root.left.data;\n            root = root.left;\n        }\n        return minValue;\n    }\n\n    public static void main(String[] args) {\n        BinarySearchTree bst = new BinarySearchTree();\n\n        // Insert nodes into the BST\n        bst.insert(50);\n        bst.insert(30);\n        bst.insert(20);\n        bst.insert(40);\n        bst.insert(70);\n        bst.insert(60);\n        bst.insert(80);\n\n        // Print the BST using inorder traversal\n        System.out.println(\"Inorder traversal of the given tree:\");\n        bst.inorder();\n\n        // Search for a value in the BST\n        System.out.println(\"\\n\\nSearch for value 40 in the BST:\");\n        System.out.println(bst.search(40) ? \"Value found\" : \"Value not found\");\n\n        // Delete a node from the BST\n        System.out.println(\"\\nDelete 20:\");\n        bst.delete(20);\n        System.out.println(\"Inorder traversal of the modified tree:\");\n        bst.inorder();\n\n        System.out.println(\"\\nDelete 30:\");\n        bst.delete(30);\n        System.out.println(\"Inorder traversal of the modified tree:\");\n        bst.inorder();\n\n        System.out.println(\"\\nDelete 50:\");\n        bst.delete(50);\n        System.out.println(\"Inorder traversal of the modified tree:\");\n        bst.inorder();\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-binary-search-tree/#explanation","title":"Explanation","text":"<ol> <li> <p>Node Class:    ```java    class Node {        int data;        Node left, right;</p> <p>public Node(int item) {        data = item;        left = right = null;    }    }    <code>``    This class represents a node in the BST, containing an integer</code>data`, and pointers to the left and right children.</p> </li> <li> <p>BinarySearchTree Class:</p> </li> <li> <p>Insert Method: <code>java      void insert(int data) {          root = insertRec(root, data);      }</code>      This method inserts a new node into the BST.</p> </li> <li> <p>Recursive Insert Function:      ```java      Node insertRec(Node root, int data) {          if (root == null) {              root = new Node(data);              return root;          }</p> <pre><code> if (data &lt; root.data)\n     root.left = insertRec(root.left, data);\n else if (data &gt; root.data)\n     root.right = insertRec(root.right, data);\n\n return root;\n</code></pre> <p>}  ```</p> </li> <li> <p>Search Method: <code>java      boolean search(int data) {          return searchRec(root, data);      }</code></p> </li> <li> <p>Recursive Search Function:      ```java      boolean searchRec(Node root, int data) {          if (root == null) {              return false;          }          if (root.data == data) {              return true;          }</p> <pre><code> if (data &lt; root.data) {\n     return searchRec(root.left, data);\n } else {\n     return searchRec(root.right, data);\n }\n</code></pre> <p>}  ```</p> </li> <li> <p>Inorder Traversal: <code>java      void inorder() {          inorderRec(root);      }</code></p> </li> <li> <p>Recursive Inorder Function: <code>java      void inorderRec(Node root) {          if (root != null) {              inorderRec(root.left);              System.out.print(root.data + \" \");              inorderRec(root.right);          }      }</code></p> </li> <li> <p>Delete Method: <code>java      void delete(int data) {          root = deleteRec(root, data);      }</code></p> </li> <li> <p>Recursive Delete Function:      ```java      Node deleteRec(Node root, int data) {          if (root == null) {              return root;          }</p> <pre><code> if (data &lt; root.data) {\n     root.left = deleteRec(root.left, data);\n } else if (data &gt; root.data) {\n     root.right = deleteRec(root.right, data);\n } else {\n     if (root.left == null) {\n         return root.right;\n     } else if (root.right == null) {\n         return root.left;\n     }\n\n     root.data = minValue(root.right);\n     root.right = deleteRec(root.right, root.data);\n }\n\n return root;\n</code></pre> <p>}  ```</p> </li> <li> <p>Find Minimum Value in the Right Subtree: <code>java      int minValue(Node root) {          int minValue = root.data;          while (root.left != null) {              minValue = root.left.data;              root = root.left;          }          return minValue;      }</code></p> </li> <li> <p>Main Method:    ```java    public static void main(String[] args) {        BinarySearchTree bst = new BinarySearchTree();</p> <p>bst.insert(50);    bst.insert(30);    bst.insert(20);    bst.insert(40);    bst.insert(70);    bst.insert(60);    bst.insert(80);</p> <p>System.out.println(\"Inorder traversal of the given tree:\");    bst.inorder();</p> <p>System.out.println(\"\\n\\nSearch for value 40 in the BST:\");    System.out.println(bst.search(40) ? \"Value found\" : \"Value not found\");</p> <p>System.out.println(\"\\nDelete 20:\");    bst.delete(20);    System.out.println(\"Inorder traversal of the modified tree:\");    bst.inorder();</p> <p>System.out.println(\"\\nDelete 30:\");    bst.delete(30);    System.out.println(\"Inorder traversal of the modified tree:\");    bst.inorder();</p> <p>System.out.println(\"\\nDelete 50:\");    bst.delete(50);    System.out.println(\"Inorder traversal of the modified tree:\");    bst.inorder();    }    ```</p> </li> </ol>"},{"location":"data-structure/trees/tree-binary-search-tree/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the basic operations of a Binary Search Tree (BST) in Java, including insertion, search, deletion, and traversal (inorder). The <code>BinarySearchTree</code> class provides a clear and efficient way to implement and work with BSTs, allowing for organized and efficient data storage and retrieval.</p>"},{"location":"data-structure/trees/tree-binary-tree/","title":"Tree binary tree","text":""},{"location":"data-structure/trees/tree-binary-tree/#understanding-binary-tree","title":"Understanding Binary Tree","text":"<p>A binary tree is a hierarchical data structure in which each node has at most two children, referred to as the left child and the right child. Binary trees are used in various applications such as representing hierarchical data, searching and sorting, and in algorithms like Huffman coding and expression parsing.</p>"},{"location":"data-structure/trees/tree-binary-tree/#java-implementation-of-a-binary-tree","title":"Java Implementation of a Binary Tree","text":"<p>Here's how you can implement and use a binary tree in Java:</p>"},{"location":"data-structure/trees/tree-binary-tree/#node-class","title":"Node Class","text":"<pre><code>class Node {\n    int data;\n    Node left, right;\n\n    public Node(int item) {\n        data = item;\n        left = right = null;\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-binary-tree/#binarytree-class-with-basic-operations","title":"BinaryTree Class with Basic Operations","text":"<pre><code>public class BinaryTree {\n    Node root;\n\n    public BinaryTree() {\n        root = null;\n    }\n\n    // Inorder Traversal\n    void inorder(Node node) {\n        if (node == null)\n            return;\n\n        // Recur on the left child\n        inorder(node.left);\n\n        // Print the data of the node\n        System.out.print(node.data + \" \");\n\n        // Recur on the right child\n        inorder(node.right);\n    }\n\n    // Preorder Traversal\n    void preorder(Node node) {\n        if (node == null)\n            return;\n\n        // Print the data of the node\n        System.out.print(node.data + \" \");\n\n        // Recur on the left child\n        preorder(node.left);\n\n        // Recur on the right child\n        preorder(node.right);\n    }\n\n    // Postorder Traversal\n    void postorder(Node node) {\n        if (node == null)\n            return;\n\n        // Recur on the left child\n        postorder(node.left);\n\n        // Recur on the right child\n        postorder(node.right);\n\n        // Print the data of the node\n        System.out.print(node.data + \" \");\n    }\n\n    // Wrapper for inorder traversal\n    void inorder() {\n        inorder(root);\n    }\n\n    // Wrapper for preorder traversal\n    void preorder() {\n        preorder(root);\n    }\n\n    // Wrapper for postorder traversal\n    void postorder() {\n        postorder(root);\n    }\n\n    // Insert a new node\n    void insert(int data) {\n        root = insertRec(root, data);\n    }\n\n    // Recursive insert function\n    Node insertRec(Node root, int data) {\n        if (root == null) {\n            root = new Node(data);\n            return root;\n        }\n\n        if (data &lt; root.data)\n            root.left = insertRec(root.left, data);\n        else if (data &gt; root.data)\n            root.right = insertRec(root.right, data);\n\n        return root;\n    }\n\n    public static void main(String[] args) {\n        BinaryTree tree = new BinaryTree();\n\n        /* Let us create the following BST\n                  50\n               /     \\\n              30      70\n             /  \\    /  \\\n            20   40  60   80 */\n        tree.insert(50);\n        tree.insert(30);\n        tree.insert(20);\n        tree.insert(40);\n        tree.insert(70);\n        tree.insert(60);\n        tree.insert(80);\n\n        System.out.println(\"Inorder traversal:\");\n        tree.inorder();\n\n        System.out.println(\"\\nPreorder traversal:\");\n        tree.preorder();\n\n        System.out.println(\"\\nPostorder traversal:\");\n        tree.postorder();\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-binary-tree/#explanation","title":"Explanation","text":"<ol> <li> <p>Node Class:    ```java    class Node {        int data;        Node left, right;</p> <p>public Node(int item) {        data = item;        left = right = null;    }    }    <code>``    This class represents a node in the binary tree, containing an integer</code>data`, and pointers to the left and right children.</p> </li> <li> <p>BinaryTree Class:</p> </li> <li> <p>Inorder Traversal: <code>java      void inorder(Node node) {          if (node == null)              return;          inorder(node.left);          System.out.print(node.data + \" \");          inorder(node.right);      }</code>      This method prints the nodes of the tree in an inorder sequence (left-root-right).</p> </li> <li> <p>Preorder Traversal: <code>java      void preorder(Node node) {          if (node == null)              return;          System.out.print(node.data + \" \");          preorder(node.left);          preorder(node.right);      }</code>      This method prints the nodes of the tree in a preorder sequence (root-left-right).</p> </li> <li> <p>Postorder Traversal: <code>java      void postorder(Node node) {          if (node == null)              return;          postorder(node.left);          postorder(node.right);          System.out.print(node.data + \" \");      }</code>      This method prints the nodes of the tree in a postorder sequence (left-right-root).</p> </li> <li> <p>Insert Method: <code>java      void insert(int data) {          root = insertRec(root, data);      }</code>      This method inserts a new node into the binary tree.</p> </li> <li> <p>Recursive Insert Function: <code>java      Node insertRec(Node root, int data) {          if (root == null) {              root = new Node(data);              return root;          }          if (data &lt; root.data)              root.left = insertRec(root.left, data);          else if (data &gt; root.data)              root.right = insertRec(root.right, data);          return root;      }</code>      This is a helper method for inserting a new node recursively.</p> </li> <li> <p>Main Method:    ```java    public static void main(String[] args) {        BinaryTree tree = new BinaryTree();</p> <p>tree.insert(50);    tree.insert(30);    tree.insert(20);    tree.insert(40);    tree.insert(70);    tree.insert(60);    tree.insert(80);</p> <p>System.out.println(\"Inorder traversal:\");    tree.inorder();</p> <p>System.out.println(\"\\nPreorder traversal:\");    tree.preorder();</p> <p>System.out.println(\"\\nPostorder traversal:\");    tree.postorder();    }    ```    This method creates a binary tree, inserts nodes, and performs inorder, preorder, and postorder traversals, printing the results.</p> </li> </ol>"},{"location":"data-structure/trees/tree-binary-tree/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the basic operations of a binary tree, including insertion and different types of traversal (inorder, preorder, and postorder). The <code>BinaryTree</code> class in Java provides a clear and simple way to implement and work with binary trees, allowing for efficient data storage and retrieval.</p>"},{"location":"data-structure/trees/tree-heap/","title":"Tree heap","text":""},{"location":"data-structure/trees/tree-heap/#understanding-heap","title":"Understanding Heap","text":"<p>A heap is a specialized tree-based data structure that satisfies the heap property: - Max-Heap: In a max-heap, the key at a parent node is always greater than or equal to the keys of its children, and the largest key is at the root. - Min-Heap: In a min-heap, the key at a parent node is always less than or equal to the keys of its children, and the smallest key is at the root.</p> <p>Heaps are commonly used to implement priority queues and for efficient sorting algorithms such as Heap Sort.</p>"},{"location":"data-structure/trees/tree-heap/#java-implementation-of-a-max-heap","title":"Java Implementation of a Max-Heap","text":"<p>Here's how you can implement a Max-Heap in Java:</p>"},{"location":"data-structure/trees/tree-heap/#maxheap-class-with-basic-operations","title":"MaxHeap Class with Basic Operations","text":"<pre><code>import java.util.Arrays;\n\npublic class MaxHeap {\n    private int[] heap;\n    private int size;\n    private int maxSize;\n\n    private static final int FRONT = 1;\n\n    public MaxHeap(int maxSize) {\n        this.maxSize = maxSize;\n        this.size = 0;\n        heap = new int[this.maxSize + 1];\n        heap[0] = Integer.MAX_VALUE;\n    }\n\n    // Function to return the position of the parent for the node currently at pos\n    private int parent(int pos) {\n        return pos / 2;\n    }\n\n    // Function to return the position of the left child for the node currently at pos\n    private int leftChild(int pos) {\n        return (2 * pos);\n    }\n\n    // Function to return the position of the right child for the node currently at pos\n    private int rightChild(int pos) {\n        return (2 * pos) + 1;\n    }\n\n    // Function to swap two nodes of the heap\n    private void swap(int fpos, int spos) {\n        int tmp;\n        tmp = heap[fpos];\n        heap[fpos] = heap[spos];\n        heap[spos] = tmp;\n    }\n\n    // Function to heapify the node at pos\n    private void maxHeapify(int pos) {\n        if (pos &gt; (size / 2) &amp;&amp; pos &lt;= size)\n            return;\n\n        if (heap[pos] &lt; heap[leftChild(pos)] ||\n            heap[pos] &lt; heap[rightChild(pos)]) {\n\n            if (heap[leftChild(pos)] &gt; heap[rightChild(pos)]) {\n                swap(pos, leftChild(pos));\n                maxHeapify(leftChild(pos));\n            } else {\n                swap(pos, rightChild(pos));\n                maxHeapify(rightChild(pos));\n            }\n        }\n    }\n\n    // Function to insert a node into the heap\n    public void insert(int element) {\n        if (size &gt;= maxSize) {\n            return;\n        }\n        heap[++size] = element;\n        int current = size;\n\n        while (heap[current] &gt; heap[parent(current)]) {\n            swap(current, parent(current));\n            current = parent(current);\n        }\n    }\n\n    // Function to print the contents of the heap\n    public void print() {\n        for (int i = 1; i &lt;= size / 2; i++) {\n            System.out.print(\" PARENT : \" + heap[i] +\n                    \" LEFT CHILD : \" + heap[2 * i] +\n                    \" RIGHT CHILD :\" + heap[2 * i + 1]);\n            System.out.println();\n        }\n    }\n\n    // Function to remove and return the maximum element from the heap\n    public int extractMax() {\n        int popped = heap[FRONT];\n        heap[FRONT] = heap[size--];\n        maxHeapify(FRONT);\n        return popped;\n    }\n\n    public static void main(String[] arg) {\n        System.out.println(\"The Max Heap is \");\n        MaxHeap maxHeap = new MaxHeap(15);\n        maxHeap.insert(5);\n        maxHeap.insert(3);\n        maxHeap.insert(17);\n        maxHeap.insert(10);\n        maxHeap.insert(84);\n        maxHeap.insert(19);\n        maxHeap.insert(6);\n        maxHeap.insert(22);\n        maxHeap.insert(9);\n\n        maxHeap.print();\n        System.out.println(\"The max val is \" + maxHeap.extractMax());\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-heap/#explanation","title":"Explanation","text":"<ol> <li>Node Class:    ```java    import java.util.Arrays;</li> </ol> <p>public class MaxHeap {        private int[] heap;        private int size;        private int maxSize;</p> <pre><code>   private static final int FRONT = 1;\n</code></pre> <p>}    <code>``    This class defines the structure of the Max-Heap with an array</code>heap<code>, the current size of the heap</code>size<code>, and the maximum size of the heap</code>maxSize<code>. The</code>FRONT` constant denotes the index of the root node in the heap array.</p> <ol> <li>Utility Functions:</li> <li> <p>Parent, Left Child, and Right Child:      ```java      private int parent(int pos) {          return pos / 2;      }</p> <p>private int leftChild(int pos) {      return (2 * pos);  }</p> <p>private int rightChild(int pos) {      return (2 * pos) + 1;  }  ```  These functions return the position of the parent, left child, and right child of a node at the given position.</p> </li> <li> <p>Swap Function: <code>java      private void swap(int fpos, int spos) {          int tmp;          tmp = heap[fpos];          heap[fpos] = heap[spos];          heap[spos] = tmp;      }</code>      This function swaps the nodes at the given positions.</p> </li> <li> <p>Max Heapify Function:      ```java      private void maxHeapify(int pos) {          if (pos &gt;= (size / 2) &amp;&amp; pos &lt;= size)              return;</p> <pre><code> if (heap[pos] &lt; heap[leftChild(pos)] ||\n     heap[pos] &lt; heap[rightChild(pos)]) {\n\n     if (heap[leftChild(pos)] &gt; heap[rightChild(pos)]) {\n         swap(pos, leftChild(pos));\n         maxHeapify(leftChild(pos));\n     } else {\n         swap(pos, rightChild(pos));\n         maxHeapify(rightChild(pos));\n     }\n }\n</code></pre> <p>}  ```  This function ensures the max-heap property is maintained starting from the node at the given position.</p> </li> <li> <p>Insert Function:    ```java    public void insert(int element) {        if (size &gt;= maxSize) {            return;        }        heap[++size] = element;        int current = size;</p> <p>while (heap[current] &gt; heap[parent(current)]) {        swap(current, parent(current));        current = parent(current);    }    }    ```    This function inserts a new element into the heap and maintains the max-heap property.</p> </li> <li> <p>Print Function: <code>java    public void print() {        for (int i = 1; i &lt;= size / 2; i++) {            System.out.print(\" PARENT : \" + heap[i] +                    \" LEFT CHILD : \" + heap[2 * i] +                    \" RIGHT CHILD :\" + heap[2 * i + 1]);            System.out.println();        }    }</code>    This function prints the contents of the heap in a readable format.</p> </li> <li> <p>Extract Max Function: <code>java    public int extractMax() {        int popped = heap[FRONT];        heap[FRONT] = heap[size--];        maxHeapify(FRONT);        return popped;    }</code>    This function removes and returns the maximum element from the heap while maintaining the max-heap property.</p> </li> <li> <p>Main Method:    ```java    public static void main(String[] arg) {        System.out.println(\"The Max Heap is \");        MaxHeap maxHeap = new MaxHeap(15);        maxHeap.insert(5);        maxHeap.insert(3);        maxHeap.insert(17);        maxHeap.insert(10);        maxHeap.insert(84);        maxHeap.insert(19);        maxHeap.insert(6);        maxHeap.insert(22);        maxHeap.insert(9);</p> <p>maxHeap.print();    System.out.println(\"The max val is \" + maxHeap.extractMax());    }    ```</p> </li> </ol>"},{"location":"data-structure/trees/tree-heap/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the basic operations of a Max-Heap in Java, including insertion, heapification, and extraction of the maximum element. The <code>MaxHeap</code> class provides a clear and efficient way to implement and work with heaps, allowing for organized and efficient data storage and retrieval.</p>"},{"location":"data-structure/trees/tree-trie/","title":"Tree trie","text":""},{"location":"data-structure/trees/tree-trie/#understanding-trie","title":"Understanding Trie","text":"<p>A trie (pronounced \"try\"), also known as a prefix tree, is a type of search tree used to store a dynamic set or associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key it is associated with. All descendants of a node have a common prefix of the string associated with that node.</p>"},{"location":"data-structure/trees/tree-trie/#key-features-of-a-trie","title":"Key Features of a Trie","text":"<ul> <li>Insertion: Efficiently insert words.</li> <li>Search: Quickly search for a word or prefix.</li> <li>Deletion: Remove words from the trie.</li> </ul>"},{"location":"data-structure/trees/tree-trie/#java-implementation-of-a-trie","title":"Java Implementation of a Trie","text":"<p>Here's how you can implement a Trie in Java:</p>"},{"location":"data-structure/trees/tree-trie/#trienode-class","title":"TrieNode Class","text":"<pre><code>class TrieNode {\n    TrieNode[] children;\n    boolean isEndOfWord;\n\n    public TrieNode() {\n        children = new TrieNode[26]; // Assuming only lowercase English letters\n        isEndOfWord = false;\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-trie/#trie-class-with-basic-operations","title":"Trie Class with Basic Operations","text":"<pre><code>public class Trie {\n    private TrieNode root;\n\n    public Trie() {\n        root = new TrieNode();\n    }\n\n    // Function to insert a word into the trie\n    public void insert(String word) {\n        TrieNode node = root;\n        for (char ch : word.toCharArray()) {\n            int index = ch - 'a';\n            if (node.children[index] == null) {\n                node.children[index] = new TrieNode();\n            }\n            node = node.children[index];\n        }\n        node.isEndOfWord = true;\n    }\n\n    // Function to search for a word in the trie\n    public boolean search(String word) {\n        TrieNode node = root;\n        for (char ch : word.toCharArray()) {\n            int index = ch - 'a';\n            if (node.children[index] == null) {\n                return false;\n            }\n            node = node.children[index];\n        }\n        return node != null &amp;&amp; node.isEndOfWord;\n    }\n\n    // Function to check if there is any word in the trie that starts with the given prefix\n    public boolean startsWith(String prefix) {\n        TrieNode node = root;\n        for (char ch : prefix.toCharArray()) {\n            int index = ch - 'a';\n            if (node.children[index] == null) {\n                return false;\n            }\n            node = node.children[index];\n        }\n        return true;\n    }\n\n    public static void main(String[] args) {\n        Trie trie = new Trie();\n\n        trie.insert(\"apple\");\n        System.out.println(trie.search(\"apple\"));   // returns true\n        System.out.println(trie.search(\"app\"));     // returns false\n        System.out.println(trie.startsWith(\"app\")); // returns true\n        trie.insert(\"app\");   \n        System.out.println(trie.search(\"app\"));     // returns true\n    }\n}\n</code></pre>"},{"location":"data-structure/trees/tree-trie/#explanation","title":"Explanation","text":"<ol> <li> <p>TrieNode Class:    ```java    class TrieNode {        TrieNode[] children;        boolean isEndOfWord;</p> <p>public TrieNode() {        children = new TrieNode[26]; // Assuming only lowercase English letters        isEndOfWord = false;    }    }    <code>``    The</code>TrieNode` class represents each node in the Trie. Each node contains an array of child nodes for each character in the alphabet (assuming lowercase English letters), and a boolean to indicate if the node represents the end of a word.</p> </li> <li> <p>Trie Class:</p> </li> <li> <p>Constructor: <code>java      public Trie() {          root = new TrieNode();      }</code>      The <code>Trie</code> class contains a root node, which is the starting point of the Trie.</p> </li> <li> <p>Insert Function: <code>java      public void insert(String word) {          TrieNode node = root;          for (char ch : word.toCharArray()) {              int index = ch - 'a';              if (node.children[index] == null) {                  node.children[index] = new TrieNode();              }              node = node.children[index];          }          node.isEndOfWord = true;      }</code>      This function inserts a word into the Trie. It iterates over each character of the word, calculates the index for the child array, and creates a new node if necessary. Finally, it marks the last node as the end of a word.</p> </li> <li> <p>Search Function: <code>java      public boolean search(String word) {          TrieNode node = root;          for (char ch : word.toCharArray()) {              int index = ch - 'a';              if (node.children[index] == null) {                  return false;              }              node = node.children[index];          }          return node != null &amp;&amp; node.isEndOfWord;      }</code>      This function searches for a word in the Trie. It iterates over each character of the word and checks if the corresponding child node exists. If the word is found and the last node is marked as the end of a word, it returns true.</p> </li> <li> <p>StartsWith Function: <code>java      public boolean startsWith(String prefix) {          TrieNode node = root;          for (char ch : prefix.toCharArray()) {              int index = ch - 'a';              if (node.children[index] == null) {                  return false;              }              node = node.children[index];          }          return true;      }</code>      This function checks if there is any word in the Trie that starts with the given prefix. It iterates over each character of the prefix and checks if the corresponding child node exists. If all characters of the prefix are found, it returns true.</p> </li> <li> <p>Main Method:    ```java    public static void main(String[] args) {        Trie trie = new Trie();</p> <p>trie.insert(\"apple\");    System.out.println(trie.search(\"apple\"));   // returns true    System.out.println(trie.search(\"app\"));     // returns false    System.out.println(trie.startsWith(\"app\")); // returns true    trie.insert(\"app\");     System.out.println(trie.search(\"app\"));     // returns true    }    ```</p> </li> </ol>"},{"location":"data-structure/trees/tree-trie/#conclusion","title":"Conclusion","text":"<p>This example demonstrates the basic operations of a Trie in Java, including insertion, searching for a word, and checking if a prefix exists in the Trie. The <code>Trie</code> class provides an efficient way to store and search strings, making it a powerful tool for various applications such as autocomplete, spell checker, and IP routing.</p>"},{"location":"data-structure/trees/trie/","title":"Trie","text":""},{"location":"data-structure/trees/trie/#what-is-a-trie","title":"What is a Trie?","text":"<p>A trie, also known as a prefix tree or digital tree, is a specialized tree-like data structure that is used to store a dynamic set of strings, where the keys are usually strings. It is particularly effective for solving problems involving retrieval of keys, such as autocomplete and spell checker functionalities.</p>"},{"location":"data-structure/trees/trie/#key-concepts","title":"Key Concepts","text":"<ol> <li>Nodes and Edges</li> <li>Nodes: Each node in a trie represents a single character of a string.</li> <li> <p>Edges: Edges connect nodes and represent the transition from one character to the next in the strings being stored.</p> </li> <li> <p>Root</p> </li> <li> <p>The root node represents the beginning of the trie and does not contain any character.</p> </li> <li> <p>Children</p> </li> <li> <p>Each node can have multiple children, representing the next character in the possible strings.</p> </li> <li> <p>End of Word Marker</p> </li> <li>Some nodes are marked to indicate that they represent the end of a valid string (word) in the trie.</li> </ol>"},{"location":"data-structure/trees/trie/#how-tries-work","title":"How Tries Work","text":"<ol> <li>Insertion</li> <li>To insert a word, start at the root node.</li> <li>For each character in the word, check if there is an edge to a child node corresponding to that character.</li> <li>If the edge does not exist, create a new node and edge.</li> <li>Move to the child node and repeat the process for the next character.</li> <li> <p>After the last character, mark the node as the end of a word.</p> </li> <li> <p>Search</p> </li> <li>To search for a word, start at the root node.</li> <li>For each character in the word, follow the edge to the corresponding child node.</li> <li>If at any point the edge does not exist, the word is not in the trie.</li> <li> <p>If all characters are found and the final node is marked as the end of a word, the word is in the trie.</p> </li> <li> <p>Deletion</p> </li> <li>To delete a word, first search for the word to ensure it exists.</li> <li>If found, unmark the end of the word node.</li> <li>Optionally, remove nodes that are no longer needed to save space.</li> </ol>"},{"location":"data-structure/trees/trie/#advantages-of-tries","title":"Advantages of Tries","text":"<ul> <li>Efficient Search: Tries provide efficient search operations, typically O(m) where m is the length of the word, because each character is processed once.</li> <li>Prefix Matching: Tries are excellent for prefix-based searches, such as autocomplete, because all words with a given prefix are located in the same subtree.</li> <li>Space Efficiency: Although tries can use more space compared to other data structures for small datasets, they can be more space-efficient for large datasets with common prefixes.</li> </ul>"},{"location":"data-structure/trees/trie/#disadvantages-of-tries","title":"Disadvantages of Tries","text":"<ul> <li>Memory Usage: Tries can consume a lot of memory, especially if there are not many common prefixes among the strings.</li> <li>Complex Implementation: Implementing tries can be more complex compared to other data structures like hash tables.</li> </ul>"},{"location":"data-structure/trees/trie/#applications-of-tries","title":"Applications of Tries","text":"<ol> <li>Autocomplete: Quickly suggest completions for a given prefix.</li> <li>Spell Checkers: Verify if a word exists and suggest corrections.</li> <li>IP Routing: Store routing tables in networking.</li> <li>DNA Sequencing: Efficient storage and retrieval of DNA sequences.</li> <li>Word Games: Support for games like Boggle or Scrabble.</li> </ol>"},{"location":"data-structure/trees/trie/#example","title":"Example","text":"<p>Here is a simple implementation of a trie in Python:</p> <pre><code>class TrieNode:\n    def __init__(self):\n        self.children = {}\n        self.is_end_of_word = False\n\nclass Trie:\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word):\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                node.children[char] = TrieNode()\n            node = node.children[char]\n        node.is_end_of_word = True\n\n    def search(self, word):\n        node = self.root\n        for char in word:\n            if char not in node.children:\n                return False\n            node = node.children[char]\n        return node.is_end_of_word\n\n    def starts_with(self, prefix):\n        node = self.root\n        for char in prefix:\n            if char not in node.children:\n                return False\n            node = node.children[char]\n        return True\n\n# Example usage:\ntrie = Trie()\ntrie.insert(\"apple\")\nprint(trie.search(\"apple\"))    # Output: True\nprint(trie.search(\"app\"))      # Output: False\nprint(trie.starts_with(\"app\")) # Output: True\ntrie.insert(\"app\")\nprint(trie.search(\"app\"))      # Output: True\n</code></pre> <p>In this example, the <code>Trie</code> class supports inserting words, searching for exact words, and checking if any words in the trie start with a given prefix. The <code>TrieNode</code> class represents each node in the trie, holding a dictionary of children and a boolean indicating if the node marks the end of a word.</p>"},{"location":"databases/","title":"Databases","text":"<p>A lot has been achieved in the world of databases, and a lot still goes on. New databases are designed based on new use cases and learnings from the past. Let's explore some of these concepts.</p>"},{"location":"databases/glossary/","title":"Glossary","text":""},{"location":"databases/glossary/#tps-qps","title":"TPS / QPS","text":"<p><code>Transactions per second</code> or <code>Query-rate per second</code>. Time to complete a transaction varies based on the operations being performed by the transaction. Hence, TPS is the average count of transactions, a server is capable of processing in a second.</p> <p>The overall processing capability of the system depends on the minimum TPS value of a system.</p> <pre><code>TPS = (num_of_connections * num_of_concurrent_users) / response_time\n\nIf transactions happened sequentially, and in only one thread (on one TCP connection), num_of_connections would be 1, however it is not true in the real world.\n</code></pre>"},{"location":"databases/glossary/#latency","title":"Latency","text":"<p>Latency is the time difference between the cause and the effect of a request to a system being observed. It is also known as <code>lag</code> in the gaming world. Latency is a consequence of the limited velocity (transmission delay) at which system interactions can propagate. It generally includes one-way delay and round trip delay, measured generally in milliseconds.</p> <p>Think of a line of people waiting to buy a movie ticket. The line can only move as fast as the person on the counter can process payments and issue tickets to the buyers. Transmission delays happens for the similar reasons. The CPU may be busy handling so many requests that it makes the new requests wait before taking any action on it, for example. Another reason could be that the request is traveling on a high-latency network, and may have nothing to do with the CPU wait.</p> <p>Measuring <code>throughput</code> and <code>latency</code> can help to identify performance issues on the network. The intent is to generally to have as much high throughput as possible, and as low latency (quick response) as possible.</p>"},{"location":"databases/glossary/#references","title":"References","text":"<ul> <li>Latency | Wiki</li> <li>Escaping the DRAM price trap: Storage Class Memory | Blocks &amp; Files</li> </ul>"},{"location":"databases/glossary/#throughput","title":"Throughput","text":"<p>The throughput (pressure capacity) of a system is closely related to the CPU consumption of requests, external interfaces, IO, etc. The higher the CPU consumption of a single request, the slower the external system interface and IO impact speed, the lower the system throughput, and vice versa.</p> <p>Two factors determine the upper limit of throughput:</p> <ol> <li>The number of hardware resources available</li> <li>The resource allocation and effective utilization in a system</li> </ol> <p>Important parameters of system throughput: <code>QPS (TPS)</code>, <code>concurrent users</code>, <code>response time</code>.</p> <p>If the system is overloaded, other consumptions such as context switching, memory, etc cause the system performance to decline.</p>"},{"location":"databases/glossary/#availability","title":"Availability","text":"<p>TBD</p>"},{"location":"databases/glossary/#consistency","title":"Consistency","text":"<p>TBD</p>"},{"location":"databases/glossary/#durability","title":"Durability","text":"<p>TBD</p>"},{"location":"databases/glossary/#fault-tolerant","title":"Fault-Tolerant","text":"<p>TBD</p>"},{"location":"databases/glossary/#rdbms","title":"RDBMS","text":"<p>Relational Database Management System</p>"},{"location":"databases/replication/","title":"Replication","text":"<p>To explain the concept, we're going to be building on a growth story</p>"},{"location":"databases/replication/#scenario","title":"Scenario","text":"<p>Let's say you are working on a proof-of-concept application with a need for some form of persistent storage, which has a potential to generate revenue if there are enough users. You anticipate a few hundred users to get on the platform every month. Because there is not enough revenue potential in first year, so you spin up a relational database instance and roll out the application in production.</p> <p>Based on real-life traffic pattern, you see a growth rate of 5x users per month. As your application starts getting this gradual increase in traffic, you realize that your database is capable of handling 5k queries per second, and this demand will only go up.</p>"},{"location":"databases/replication/#what-if-i-dont-do-anything","title":"What if I don't do anything?","text":"<p>At soe point, there will be more users than the database can handle. You'll start seeing query latencies go up because the database instance is occupied in receiving queries and putting them in queue to respond back. After some time, some of the queries will start timing out, because they weren't able to finish in time. Increasing timeouts will not help much here, if increasing was even possible. Eventually, the database will crash, some of the transactions in progress will be lost.</p>"},{"location":"databases/replication/#database-restarts","title":"Database restarts","text":"<p>Sure. The database will boot up again, start taking connection requests, and then eventually end up in the same state. Terrible user experience and potentially creating bad reputation with your increased user base.</p>"},{"location":"databases/replication/#increasing-instance-size","title":"Increasing instance size","text":"<p>This is the simplest solution (also called <code>Horizontal Scaling</code>), and recommended one without having to increase engineering complexity. You'll just need to be careful that the new instance is up to date, and find a way to avoid downtime when switching traffic between the instances.</p>"},{"location":"databases/replication/#challenges","title":"Challenges","text":"<ol> <li>Most likely you'll have some down time upgrading the database. Trying to upgrade without downtime can result in risks of data getting out of sync.</li> <li>This is not a highly scalable approach. There is only so much CPU and memory you can add on a single machine. Eventually you'll run out of it all. A good problem to have, but still a problem.</li> <li>Your single database instance is still a single-point-of-failure. A machine can be unavailable for a number of reasons, including hardware failure, network error, power outage, regional outage, etc. You wouldn't want a revenue generating app to have downtime, would you?</li> </ol>"},{"location":"databases/replication/#adding-more-db-machines","title":"Adding more DB machines","text":"<p>Another approach is to let more than one database machine handle incoming queries (also called <code>Scaling Vertically</code>).</p> <p>If you split the data between those machines, it's called <code>Sharding</code>. If you clone the data across machines, it's called <code>Replication</code>.</p> <p>Not one approach is better than other, just they have very specific use-cases.</p> <p>If the entire data is too large for a single machine to host, then regardless of how many replicas we add, we're not going to solve the problem of running out of memory. This is where <code>Sharding</code> comes in.</p> <p>Regardless of we shard the data or not, it is a good idea to add more replicas for the following reasons:</p> <ul> <li>Reduce downtime - be it for scheduled maintenance, server overload, or even node failure</li> <li>Improved scalability - a db cluster running with <code>n</code> nodes (n &gt; 1), will be capable of running <code>n + 1</code> nodes, which becomes a less-effort to no-effort strategy, when there is a need to scale the system.</li> </ul>"},{"location":"databases/replication/#but-where-to-send-the-request","title":"But where to send the request?","text":"<p>Now that you have multiple replicas, how would the application know where to send the request or is it the application's responsibility of managing load on database instances equally?</p> <p>This is achieved by using <code>DNS</code> Domain Name Service, which acts as a load balancer in front of the database cluster (example Amazon RDS).</p>"},{"location":"databases/replication/#does-master-use-the-same-load-balancer","title":"Does master use the same load balancer?","text":"<p>Shouldn't be, otherwise how would the application target writes to master?</p>"},{"location":"databases/replication/#what-happens-to-a-node-that-keeps-dying","title":"What happens to a node that keeps dying?","text":"<p>Will it keep erroring transactions?</p>"},{"location":"databases/replication/#topics","title":"Topics","text":"<ul> <li>Scenario</li> <li>Challenge</li> <li>What is replication?</li> <li>Types of replication</li> <li>Single-leader</li> <li>Examples</li> <li>Challenges</li> <li>Multi-leader</li> <li>Examples</li> <li>Challenges</li> <li>Leaderless</li> <li>Examples</li> <li>Challenges</li> <li>Advanced Topics</li> <li>Paxos, </li> <li>What if</li> <li>user growth was overnight instead of gradual?</li> </ul>"},{"location":"databases/fundamentals/02-acid-01-overview/","title":"ACID","text":"<p>The ACID properties (Atomicity, Consistency, Isolation, Durability) are fundamental principles that aim to ensure the <code>reliability of database transactions</code> in relational database systems. However, the strict adherence to these principles can vary based on the specific requirements of a database system and the <code>trade-offs</code> that are acceptable for a given application or context.</p>"},{"location":"databases/fundamentals/02-acid-01-overview/#properties","title":"Properties","text":""},{"location":"databases/fundamentals/02-acid-01-overview/#atomicity","title":"Atomicity","text":"<p>Atomicity ensures that all operations within a transaction are completed successfully; if not, the transaction is aborted.</p> <p>That is, all the queries in a transaction must succeed for a transaction to be successful, since all queries in the transaction are considered to be one unit of work. Like an <code>atom</code>, which cannot be broken apart further. This means, if one of the query fails in a transaction, then all previous queries must roll back.</p> <p>In the case when the database crashes prior to commit, all the successful queries in the transaction should roll back.</p> <p>Lack of atomicity leads to <code>inconsistency</code> since the money taken out from one account would never make it to another account during a database crash.</p>"},{"location":"databases/fundamentals/02-acid-01-overview/#isolation","title":"Isolation","text":"<p>Ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially.</p> <p>Each transaction opens a TCP connection, and there are multiple transactions happening at a time in a database. Hence, there is a chance that multiple transactions can compete to read/write the same piece of data.</p> <p>This brings up a question: can my inflight transaction see changes made by other transactions? That depends on the isolation level.</p>"},{"location":"databases/fundamentals/02-acid-01-overview/#consistency","title":"Consistency","text":"<p>Guarantees that a transaction can only bring the database from one valid state to another, maintaining database invariants.</p>"},{"location":"databases/fundamentals/02-acid-01-overview/#durability","title":"Durability","text":"<p>Ensures that once a transaction has been committed, it will remain so, even in the event of power loss, crashes, or errors.</p>"},{"location":"databases/fundamentals/02-acid-01-overview/#flexibility","title":"Flexibility","text":"<p>The ACID properties are not always strictly enforced in all database systems. Some systems may choose to relax these properties to achieve better performance, speed, or scalability.</p> <p>While these properties are ideal for ensuring <code>data integrity</code> and <code>consistency</code>, there are scenarios where strict adherence to all ACID properties may not be necessary or could lead to performance bottlenecks. For example, in highly distributed systems or in scenarios where performance and availability are prioritized over strict consistency, some flexibility is allowed.</p> <p>Flexibility with ACID: - Eventual Consistency: Some systems may opt for eventual consistency over immediate consistency to improve availability and partition tolerance, relaxing the Consistency requirement. - Isolation Levels: Different isolation levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable) offer flexibility in how transactions are isolated from each other, trading off between performance and the degree of isolation. - Performance Optimization: In some cases, systems might relax durability guarantees (e.g., using delayed writes) to enhance performance, although this is less common because it risks data loss.</p> <p>In summary, while the ACID properties are a cornerstone of relational database design, practical considerations and the specific needs of applications may lead to variations in how strictly these properties are enforced. The choice to relax certain ACID properties is a trade-off that needs to be carefully considered based on the application's requirements for consistency, availability, and performance.</p>"},{"location":"databases/fundamentals/02-acid-02-atomicity/","title":"Atomicity","text":"<p>Atomicity states that all the queries in a transaction must succeed for a transaction to be successful, since all queries in the transaction are considered to be one unit of work. Like an <code>atom</code>, which cannot be broken apart further.</p> <p>This means, if one of the query fails in a transaction, then all previous queries must roll back.</p> <p>In the case when the database crashes prior to commit, all the successful queries in the transaction should roll back.</p> <p>Lack of atomicity leads to <code>inconsistency</code> since the money taken out from one account would never make it to another account during a database crash.</p> <p>Q: Could we have separated credit and debit in two separate transactions? A: No, because the order of execution matters here. There may be another way to ensure order here though.......</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/","title":"Isolation","text":"<p>Each transaction opens a TCP connection, and there are multiple transactions happening at a time in a database. Hence, there is a chance that multiple transactions can compete to read/write the same piece of data.</p> <p>This brings up a question: can my inflight transaction see changes made by other transactions? That depends on the isolation level.</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/#read-phenomena","title":"Read phenomena","text":"<p>In the context of <code>transaction isolation</code>, <code>Read phenomena</code> refer to specific types of <code>inconsistencies</code> that can occur when multiple transactions operate on the same data concurrently. These phenomena are critical to understanding because they directly impact the <code>integrity</code> and <code>consistency</code> of data within a database.</p> <p><code>Dirty reads</code> occurs when a transaction is allowed to read data from a row that has been modified by another ongoing transaction but not yet committed. This means the data being read could potentially be rolled back if the modifying transaction fails, leading to the reading transaction obtaining data that might never be committed or is inconsistent with the database's final state. To prevent dirty reads, databases use <code>isolation levels</code> that define the degree to which the transactions must be isolated from each other, with higher levels of isolation reducing the risk of such phenomena but potentially <code>impacting performance</code> due to <code>increased locking</code> and <code>reduced concurrency</code>.</p> <p><code>Non-repeatable reads</code> occur in a database when, during the course of a transaction, a row is retrieved twice and different values are obtained each time. This phenomenon typically happens in a <code>read-committed isolation level</code>, where a transaction may see changes made by other transactions that were committed after its start. This inconsistency can lead to issues in applications that <code>assume</code> data will not change during the transaction.  To prevent non-repeatable reads, a higher isolation level, such as <code>repeatable read</code> or <code>serializable</code>, can be used. These levels ensure that once a row is read by a transaction, no other transaction can modify that row until the first transaction is completed. However, increasing the isolation level can <code>impact</code> database performance due to the <code>increased locking</code> and <code>reduced concurrency</code>. For example, to solve for this, Postgres updates the row version number every time a row is updated. It never updates the row in place. While MySQL uses a <code>read-view</code> to keep track of the changes made by other transactions.</p> <p><code>Phantom reads</code> occur in a database when a transaction reads a set of rows that match a certain search condition and then, in a subsequent read within the same transaction, finds additional rows (or fewer rows) that match the condition, due to another transaction's insert or delete operations that are committed in the interim. This phenomenon is called <code>phantom</code> because the new rows seem to appear or disappear unexpectedly, like a phantom. To prevent phantom reads, a higher isolation level, such as <code>serializable</code>, can be used, which ensures that the result set of a query remains consistent throughout the transaction. However, using higher isolation levels can <code>impact</code> database performance due to <code>increased locking</code> and <code>reduced concurrency</code>. In case of Phantom reads, we cannot even acquire a lock on the new row because it doesn't exist yet. Hence, we need to lock the range of rows that match the search condition. Postgres uses <code>MVCC</code> to prevent phantom reads.</p> <p><code>Lost updates</code> occur in a database scenario when two or more transactions read the same data and then update it based on the read value. If these transactions are executed concurrently without proper isolation, they may overwrite each other's updates without knowing the other transaction's modifications. This leads to <code>lost updates</code>, where the final value in the database reflects only the last write, disregarding the other transactions' updates. To prevent lost updates, databases implement <code>locking mechanisms</code> and <code>isolation levels</code>. <code>Optimistic locking checks</code> if the data has changed before committing the transaction, while higher isolation levels, like Repeatable Read or Serializable, ensure that a transaction can detect changes made by others and act accordingly, either by rolling back or retrying the operation. This can be solved by using <code>pessimistic locking</code> where we lock the row before reading it.</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/#isolation-levels","title":"Isolation levels","text":"<p>Isolation levels are a property of a database that defines how transactions interact with each other. These were introduced to prevent <code>read phenomena</code> and to ensure that transactions are executed in a consistent manner.</p> <p>There are four isolation levels in SQL: 1. Read Uncommitted - No isolation, any change from the outside is visible to the transaction, committed or not. This means you can see dirty reads. It also means that the reads are not repeatable, and fast here. 2. Read Committed - Each query in a transaction sees only the committed data by other transactions. This means you can see non-repeatable reads. This is the default and most popular isolation level in most databases. 3. Repeatable Read - This is the highest isolation level that allows you to see the same data throughout the transaction. This means you can see non-repeatable reads. This is the default isolation level in MySQL. 4. Snapshot - This is a special isolation level that allows you to see the data as it was when the transaction started. It's like a snapshot version fo the database at the moment. This is the default isolation level in Oracle. 5. Serializable - This is the highest isolation level that allows you to see the same data throughout the transaction. This means you can see non-repeatable reads. Transactions run as if they are serialized one after the other. This is the slowest. This is the default isolation level in PostgreSQL.</p> <p>Each database implements isolation levels differently, and the choice of level depends on the application's requirements for <code>consistency</code>, <code>performance</code>, and <code>concurrency</code>.</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/#isolation-level-implementation","title":"Isolation level implementation","text":"<p>Each DBMS implements isolation levels differently. For example, MySQL uses <code>read-view</code> to keep track of changes made by other transactions. While Postgres updates the row version number every time a row is updated. It never updates the row in place.</p> <ul> <li><code>Pessimistic locking</code> is when we lock (row, table, or page) before reading it, to avoid lost updates. Page-level lock is useful when data is clustered together. Table-level lock is useful when we are doing a full table scan. Row-level lock is useful when we are updating a single row.</li> <li><code>Optimistic locking</code> checks if the data has changed before committing the transaction. If the data has changed, then the transaction is rolled back.</li> <li><code>Repeatable read</code> locks the row before reading it. This is expensive but useful when we want to prevent non-repeatable reads. Postgres implements RR as snapshot, hence the reason you don't get phantom reads with postgres in RR.</li> <li><code>Serializable</code> is usually implemented with <code>two-phase locking</code>  or <code>optimistic concurrency control</code> depending on the dbms. This is the slowest but the most consistent isolation level. You can implement it pessimistically with <code>SELECT FOR UPDATE</code> or optimistically with <code>SERIALIZABLE</code> isolation level.</li> <li><code>Read-view</code> is a mechanism used by MySQL to keep track of changes made by other transactions. It keeps track of the changes made by other transactions and uses this information to prevent non-repeatable reads.</li> <li><code>MVCC</code> (Multi-Version Concurrency Control) is a technique used by databases to allow multiple transactions to read and write data without blocking each other. It creates a new version of the row every time it is updated. This is how Postgres implements isolation levels.</li> </ul> <p>To know if a data is locked, databases have a lock management system. We can use the <code>pg_locks</code> table in Postgres.</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/#serializable","title":"Serializable","text":"<p>The <code>Serializable</code> isolation level can be implemented using either two-phase locking (2PL) or optimistic concurrency control (OCC), depending on the database management system (DBMS).</p> <ul> <li> <p>Two-phase locking (2PL): This is a locking mechanism where transactions must first acquire all the locks they need before releasing any. Once a transaction starts to release locks, it cannot acquire any new ones. This strict locking protocol can effectively serialize transactions, ensuring that they execute in a manner equivalent to some serial order. 2PL can be used to implement the <code>Serializable</code> isolation level by ensuring that transactions hold read and write locks on all accessed data until the transaction completes, preventing other transactions from making conflicting changes.</p> </li> <li> <p>Optimistic Concurrency Control (OCC): This approach assumes that conflicts are rare and does not use locks. Instead, it checks at the end of each transaction to see if any data read or written by the transaction has been changed by another transaction. If a conflict is detected, the transaction is rolled back. OCC can implement the <code>Serializable</code> isolation level by ensuring that a transaction can only commit if it does not conflict with any other concurrent transactions, effectively serializing them.</p> </li> </ul> <p>The choice between 2PL and OCC for implementing the <code>Serializable</code> isolation level depends on the specific DBMS and its design goals, particularly regarding performance and the expected workload's contention level.</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/#serializable-vs-repeatable-read","title":"Serializable vs Repeatable Read","text":"<ul> <li>Serializable: This is the highest isolation level that ensures that transactions are executed as if they were serialized, one after the other. This level provides the strongest guarantees for consistency and prevents all read phenomena, including dirty reads, non-repeatable reads, and phantom reads. However, it can be the slowest isolation level due to the increased locking and reduced concurrency.</li> <li>Repeatable Read: This isolation level ensures that once a row is read by a transaction, no other transaction can modify that row until the first transaction is completed. This prevents non-repeatable reads but may still allow phantom reads. Repeatable Read is less strict than Serializable but provides a good balance between consistency and performance. It is the default isolation level in MySQL.</li> </ul> <p>The choice between <code>Serializable</code> and <code>Repeatable Read</code> depends on the application's requirements for consistency, performance, and concurrency. <code>Serializable</code> is suitable for applications that require the highest level of consistency and can tolerate lower performance, while <code>Repeatable Read</code> is a good choice for applications that need a balance between consistency and performance.</p> <p>Example: Two transactions t1 and t2 are running concurrently. t1 changes all 'a' to 'b', while t2 changes all 'b' to 'a'. If the isolation level is <code>Serializable</code>, then only one of the transactions will succeed, while the other will fail. If the isolation level is <code>Repeatable Read</code>, then both transactions will succeed, leading to a lost update.</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/#example-isolation-levels-in-postgresql","title":"Example: Isolation levels in PostgreSQL","text":"<p>Let's say we have two transactions, <code>T1</code> and <code>T2</code>, and they are running concurrently in separate terminals.</p> <pre><code>-- Terminal 1\nBEGIN TRANSACTION;\nSELECT * FROM users WHERE id = 1;\n\n-- Terminal 2\nBEGIN TRANSACTION;\nUPDATE users SET name = 'Alice' WHERE id = 1;\nCOMMIT;\n\n-- Terminal 1\nSELECT * FROM users WHERE id = 1;\nCOMMIT;\n</code></pre> <p>In this example, <code>T1</code> starts by reading the user with <code>id = 1</code>, while <code>T2</code> updates the same user's name to 'Alice' and commits the change. When <code>T1</code> reads the user again, it may see the updated name 'Alice' if the isolation level allows it, leading to a non-repeatable read.</p> <p>Now, to change the isolation level in PostgreSQL, you can use the <code>SET TRANSACTION ISOLATION LEVEL</code> command:</p> <pre><code>SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n</code></pre> <p>Or</p> <pre><code>BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n</code></pre> <p>Repeatable Read is expensive because it locks the row before reading it. The default in postgres is <code>READ COMMITTED</code>, which means you can't see the changes made by other transactions until they are committed.</p>"},{"location":"databases/fundamentals/02-acid-03-isolation/#summary","title":"Summary","text":"<ul> <li>Isolation levels define how transactions interact with each other in a database.</li> <li>Read phenomena, such as dirty reads, non-repeatable reads, phantom reads, and lost updates, can occur when transactions are not properly isolated.</li> <li>Isolation levels, such as Read Uncommitted, Read Committed, Repeatable Read, Snapshot, and Serializable, provide different levels of consistency, performance, and concurrency control.</li> <li>Databases implement isolation levels using mechanisms like pessimistic locking, optimistic locking, two-phase locking, and optimistic concurrency control.</li> <li>The choice of isolation level depends on the application's requirements for consistency, performance, and concurrency.</li> <li>Implementing the Serializable isolation level can be done using two-phase locking or optimistic concurrency control, depending on the DBMS.</li> <li>Understanding isolation levels is essential for designing database systems that balance consistency, performance, and concurrency.</li> <li>Isolation levels are a critical aspect of database transactions and play a key role in ensuring data integrity and consistency.</li> <li>The choice of isolation level depends on the specific requirements of the application, balancing consistency, performance, and concurrency.</li> <li>Implementing isolation levels involves trade-offs between consistency, performance, and concurrency control, and different databases may use different mechanisms to achieve the desired level of isolation.</li> </ul>"},{"location":"databases/fundamentals/02-acid-03-isolation/#references","title":"References","text":"<ul> <li>Isolation (database systems)</li> </ul>"},{"location":"databases/fundamentals/02-acid-04-consistency/","title":"Consistency","text":"<p>Consistency is the property that ensures that the database remains in a consistent state before and after the transaction. It is the responsibility of the database to ensure that the data is consistent after the transaction is completed. Consistency ensures that the data is valid and follows all the rules and constraints defined in the database schema.</p> <ul> <li><code>Consistency in data</code> means the data is valid and follows all the rules and constraints defined in the database schema. This mostly comes down to the enforcing <code>referential integrity</code>, uniqueness, and other constraints.</li> <li><code>Consistency in reads</code> means that the data read is the same as the data written by the transaction. This can happen because there are <code>multiple instances</code> of the database, and they may be out of sync, and the transaction might read from a different copy than it wrote to.</li> </ul> <p><code>Atomicity</code> and <code>Isolation</code> are the properties that <code>ensure</code> consistency in the database. This is because if the transaction is atomic, then all the queries in the transaction must succeed for a transaction to be successful. If one of the queries fails, then all previous queries must roll back. This ensures that the data is consistent.</p> <p>It is one of the property that was traded off in different database systems to achieve better performance, speed and scalability. This means that the database might not be consistent at all times, but it will <code>eventually</code> become consistent.</p>"},{"location":"databases/fundamentals/02-acid-04-consistency/#replication","title":"Replication","text":"<p><code>Synchronous replication</code> is a technique used to ensure consistency in distributed databases. In this technique, the data is written to multiple nodes before the transaction is considered complete. This ensures that the data is consistent across all nodes.</p> <p><code>Asynchronous replication</code> is another technique used to ensure consistency in distributed databases.</p>"},{"location":"databases/fundamentals/02-acid-04-consistency/#eventual-consistency","title":"Eventual Consistency","text":"<p><code>Eventual Consistency</code> is a consistency model used in distributed systems. In this model, the system guarantees that if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value. This model is used to achieve high availability and partition tolerance.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/","title":"Durability","text":"<p>Durability ensures that once a transaction is committed, it will remain committed even in the case of a <code>system failure</code>. Durability guarantees that the changes made by a transaction are permanent and will not be lost, however it introduces <code>latency</code>.</p> <p>If the crash happens during the commit the database cannot guarantee durability. The system is durable only when the commit is successful( the data is fully written to disk). That is why commit speeds are critical, the faster you can commit the lower the chances of such corruption.</p> <p>The process of persisting the writes that the clients make to the database to a <code>non-volatile system storage</code> is called <code>write-ahead logging</code>. Even if the database crashes, the changes made by the transaction are still available in the log files. When the database comes back online, it can replay the log files to recover the changes made by the transaction.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/#durability-techniques","title":"Durability Techniques","text":""},{"location":"databases/fundamentals/02-acid-05-durability/#write-ahead-logging-wal","title":"Write-Ahead Logging (WAL)","text":"<p><code>Write-Ahead Logging</code> is a technique used by databases to ensure durability. In WAL, the database writes the changes to the log file before writing them to the actual data files.</p> <p>When a transaction is committed, the database writes the changes to the log file and then to the data files. If the database crashes before the changes are written to the data files, it can replay the log files to recover the changes.</p> <p>Writing a lot of data to disk is expensive, so databases use a technique called <code>group commit</code> to write multiple changes to the disk in a single operation. The database persists a compressed version of the changes to the log file, known as <code>WAL Segments</code>, and then writes them to the disk in a single operation.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/#asynchronous-snapshot","title":"Asynchronous Snapshot","text":"<p>In <code>Asynchronous Snapshot</code>, the database writes everything to memory and in the background takes a snapshot of the data and writes it to the disk at once. This technique is faster than <code>Write-Ahead Logging</code> because it writes to the disk in a single operation.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/#append-only-file-aof","title":"Append-Only File (AOF)","text":"<p>In <code>Append-Only File</code>, the database writes all the changes to a log file (similar to WAL). When the database starts, it reads the log file and applies the changes to the data files.</p> <p>This technique is used by databases like Redis.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/#database-implementations","title":"Database Implementations","text":"<p>Different databases implement durability differently: 1. MySQL: Uses <code>InnoDB</code> storage engine that supports <code>Write-Ahead Logging</code>. 2. PostgreSQL: Uses <code>Write-Ahead Logging</code> and <code>MVCC</code> (Multi-Version Concurrency Control) to ensure durability. 3. Redis: Uses <code>Append-Only File</code> to ensure durability. 4. MongoDB: Uses <code>journaling</code> to ensure durability. 5. Cassandra: Uses <code>commit logs</code> to ensure durability. 6. SQLite: Uses <code>Write-Ahead Logging</code> to ensure durability. 7. Oracle: Uses <code>Redo Logs</code> to ensure durability.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/#os-cache","title":"OS Cache","text":"<p>The <code>Operating System Cache</code> is a cache that the operating system uses to store data in memory. When the database writes data to the disk, it writes to the OS Cache first and then the OS Cache <code>batch-writes</code> to the disk. This is done to improve performance, as writing to the disk is slower than writing to memory.</p> <p>The problem with that is that the OS will tell the database that the data has been written to the disk, even though it is still in the OS Cache. The database believes that because the OS has acknowledged the writes, the data is safe. If the system crashes before the OS Cache writes to the disk, the data will be lost. And it is assumed that the database is not durable.</p> <p>Most of the applications use OS cache, which may not be fine for databases to use. <code>Fsync</code> OS command forces writes to always go to disk and bypass the cache. This can be expensive, but it ensures durability.</p> <p>Databases that use Fsync:</p> <ol> <li>PostgreSQL: Uses <code>fsync</code> to ensure durability.</li> <li>MySQL: Uses <code>InnoDB</code> storage engine that supports <code>fsync</code>.</li> <li>SQLite: Uses <code>fsync</code> to ensure durability.</li> <li>Oracle: Uses <code>Redo Logs</code> to ensure durability.</li> <li>Cassandra: Uses <code>commit logs</code> to ensure durability.</li> <li>Redis: Uses <code>Append-Only File</code> to ensure durability.</li> <li>MongoDB: Uses <code>journaling</code> to ensure durability.</li> <li>SQL Server: Uses <code>Write-Ahead Logging</code> to ensure durability.</li> <li>MariaDB: Uses <code>InnoDB</code> storage engine that supports <code>fsync</code>.</li> </ol> <p>Redis gives you the option to choose between <code>fsync</code> and <code>no fsync</code> to trade-off between durability and performance. Fsync doesn't have to be used for every write, but it should be used for critical writes or when the system is shutting down.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/#example","title":"Example","text":"<p>Consider a banking application where a user transfers money from one account to another. The transaction is successful, and the money is deducted from the sender's account and credited to the receiver's account. If the database crashes before the changes are written to the disk, the changes will be lost. This is where durability comes into play. The database ensures that the changes made by the transaction are permanent and will not be lost, even in the case of a system failure.</p>"},{"location":"databases/fundamentals/02-acid-05-durability/#summary","title":"Summary","text":"<ul> <li>Durability ensures that once a transaction is committed, it will remain committed even in the case of a system failure.</li> <li>Durability guarantees that the changes made by a transaction are permanent and will not be lost.</li> <li>The process of persisting the writes that the clients make to the database to a non-volatile system storage is called write-ahead logging.</li> <li>Different databases implement durability differently, using techniques like Write-Ahead Logging, Asynchronous Snapshot, and Append-Only File.</li> <li>The Operating System Cache is a cache that the operating system uses to store data in memory.</li> <li>Fsync OS command forces writes to always go to disk and bypass the cache, ensuring durability.</li> <li>Databases like PostgreSQL, MySQL, SQLite, Oracle, Cassandra, Redis, MongoDB, SQL Server, and MariaDB use Fsync to ensure durability.</li> <li>Redis gives you the option to choose between fsync and no fsync to trade-off between durability and performance.</li> <li>Fsync doesn't have to be used for every write, but it should be used for critical writes or when the system is shutting down.</li> </ul>"},{"location":"databases/scaling/","title":"Database Scaling strategies","text":""},{"location":"databases/scaling/#indexing","title":"Indexing","text":"<p>Analyze the query patterns of your application and create the right indexes.</p>"},{"location":"databases/scaling/#denormalization","title":"Denormalization","text":"<p>Reduce complex joins to improve query performance</p>"},{"location":"databases/scaling/#database-caching","title":"Database Caching","text":"<p>Store frequently accessed data in a faster storage layer</p>"},{"location":"databases/scaling/#materialized-view","title":"Materialized View","text":"<p>Pre-compute complex query results and store them for faster access.</p>"},{"location":"databases/scaling/#replication","title":"Replication","text":"<p>Create replicas of your primary database on different servers for scaling the reads.</p>"},{"location":"databases/scaling/#sharding","title":"Sharding","text":"<p>Split your database tables into smaller pieces and spread them across servers. Used for scaling the writes as well as the reads.</p>"},{"location":"databases/scaling/#vertical-scaling","title":"Vertical scaling","text":"<p>Boost your database server by adding more CPU, RAM, or storage.</p>"},{"location":"databases/scaling/caching/","title":"Database Caching","text":"<p>Database caching is a process that involves storing data from a database in a cache, which is a temporary storage area, to improve the speed of data retrieval. When a request is made to retrieve data, the system first checks the cache. If the requested data is found in the cache (a cache hit), it is returned immediately. If the data is not found in the cache (a cache miss), the system retrieves the data from the database, stores a copy in the cache for future requests, and then returns the data.</p> <p>Caching can significantly improve the performance of a system by reducing the load on the database and decreasing the latency of data retrieval. However, it also introduces the challenge of cache invalidation, which is ensuring that the data in the cache is kept up-to-date with the data in the database.</p>"},{"location":"databases/scaling/caching/#example","title":"Example","text":"<p>Here's a simple example of database caching in Python using a dictionary as a cache:</p> <pre><code>class Database:\n    def __init__(self):\n        self.data = {\"key1\": \"value1\", \"key2\": \"value2\"}\n        self.cache = {}\n\n    def get_data(self, key):\n        # Check if the data is in the cache\n        if key in self.cache:\n            print(\"Cache hit\")\n            return self.cache[key]\n\n        # If not in the cache, retrieve from the database\n        print(\"Cache miss\")\n        value = self.data.get(key)\n\n        # Store the data in the cache for future requests\n        if value is not None:\n            self.cache[key] = value\n\n        return value\n\ndb = Database()\nprint(db.get_data(\"key1\"))  # Cache miss\nprint(db.get_data(\"key1\"))  # Cache hit\n</code></pre>"},{"location":"databases/scaling/caching/#best-practices","title":"Best practices","text":"<p>It's important to use database caching correctly to avoid potential issues. Here are some best practices for using database caching:</p> <ol> <li> <p>Choose the Right Data to Cache: Not all data benefits from being cached. Data that is rarely accessed or frequently updated may not be suitable for caching. On the other hand, data that is frequently accessed and rarely changed is a good candidate for caching.</p> </li> <li> <p>Implement Cache Invalidation: Cache invalidation is the process of updating or removing data in the cache when it is updated in the database. This is crucial to ensure that the data in the cache is kept up-to-date with the data in the database. There are several strategies for cache invalidation, such as time-based expiration (TTL), write-through, write-around, and write-back.</p> </li> <li> <p>Use Appropriate Cache Size: The size of the cache should be large enough to store the most frequently accessed data, but not so large that it consumes excessive system resources. Monitor your cache's hit rate (the percentage of requests that are served from the cache) to help determine the appropriate cache size.</p> </li> <li> <p>Distribute Your Cache If Necessary: If your application is distributed across multiple servers, consider using a distributed caching system. This allows all servers to share the same cache, ensuring that they all have access to the same data.</p> </li> <li> <p>Secure Your Cache: If your cache stores sensitive data, make sure it is secure. This might involve encrypting the data in the cache, restricting access to the cache, or both.</p> </li> <li> <p>Monitor Your Cache: Regularly monitor your cache's performance and adjust your caching strategy as necessary. This might involve changing what data is cached, how long it is cached for, or how much memory is allocated to the cache.</p> </li> </ol> <p>Remember, the goal of database caching is to improve the performance of your application. If your caching strategy is not achieving this goal, it may need to be adjusted.</p>"},{"location":"databases/scaling/denormalization/","title":"Database Denormalization","text":"<p>Database denormalization is the process of combining multiple database tables into one to improve read performance. While normalization is about breaking down a database into smaller tables to avoid data redundancy and improve data integrity, denormalization is the opposite. It's about boosting read performance of a relational database by adding redundant data or by grouping data.</p> <p>In some cases, denormalization can help because it can eliminate expensive SQL joins, reduce the amount of data that needs to be read from disk, and simplify SQL queries. However, it also has drawbacks such as increased storage space, potential for data inconsistency due to redundant data, and increased complexity in maintaining data consistency.</p> <p>Remember, denormalization is a trade-off. It can improve read performance, but it comes at the cost of increased storage space and potential data inconsistency. It should be used judiciously and only when necessary.</p>"},{"location":"databases/scaling/indexing/","title":"Database Indexing","text":"<p><code>Database indexing</code> is a data structure technique which allows you to quickly retrieve records from a database. Indexes are used to find data rows with specific column values quickly. Without an index, the database must begin with the first row and then read through the entire table to find the relevant rows: the larger the table, the more costly the operation.</p> <p>Indexes are special lookup tables that the database search engine can use to speed up data retrieval. Simply put, an index is a pointer to data in a table. An index in a database is very similar to an index in the back of a book.</p> <p>It's important to note that while indexes can speed up data retrieval, they also have some drawbacks. Indexes take up <code>disk space</code>, and they can slow down the <code>performance</code> of write operations (INSERT, UPDATE, DELETE) because the index also needs to be updated when these operations are performed. Therefore, it's important to find a good balance and only use indexes on columns that are frequently used in WHERE clauses, JOIN conditions, or ORDER BY clauses.</p>"},{"location":"databases/scaling/indexing/#implementations","title":"Implementations","text":"<p>Database indexes are implemented in various ways depending on the database system. Here are a few examples:</p> <ol> <li> <p>B-Tree Indexes: This is the most common type of index. B-Tree indexes are used in many databases like MySQL, PostgreSQL, and SQLite. In a B-Tree index, data is stored in a tree-like structure where each node contains a certain number of keys and pointers. The keys act as separation values which divide its subtrees. The left subtree of a key contains values less than the key, and the right subtree of a key contains values greater than the key.</p> </li> <li> <p>Hash Indexes: Hash indexes use a hash function to map keys to specific locations in an array. They are extremely fast for exact match lookups but do not support range queries or sorting. Hash indexes are used in databases like MySQL (with MEMORY/HEAP tables) and MongoDB.</p> </li> <li> <p>Bitmap Indexes: Bitmap indexes use bit arrays (commonly known as bitmaps) and answer queries by performing bitwise logical operations on these bitmaps. Bitmap indexes are typically used in databases that have low cardinality, i.e., the number of unique values is small compared to the number of records. Examples include Oracle and some versions of MySQL.</p> </li> <li> <p>Clustered Indexes: In a clustered index, records themselves are stored physically on the disk in the same sequence as the index. Therefore, there can be only one clustered index on a table. The advantage of clustered indexes is that they are extremely fast for range queries. SQL Server, MySQL, and PostgreSQL use clustered indexes.</p> </li> <li> <p>Non-Clustered Indexes: Non-clustered indexes have a structure separate from the data rows. A non-clustered index contains the non-clustered index key values and each key value entry has a pointer to the data row that contains the key value. The pointer from an index row in a non-clustered index to a data row is called a row locator. SQL Server, MySQL, and PostgreSQL use non-clustered indexes.</p> </li> <li> <p>Multicolumn Indexes (Composite Indexes): These are indexes that are created on more than one column of a table. The columns are ordered in the index definition as per their priority. PostgreSQL, MySQL, and SQL Server support multicolumn indexes.</p> </li> <li> <p>Spatial Indexes: Spatial indexes are used to index spatial data types. They are used to optimize queries that perform operations on geometric shapes in space. MySQL and PostgreSQL (with the PostGIS extension) support spatial indexes.</p> </li> <li> <p>Full-Text Indexes: Full-text indexes in MySQL are an index of type FULLTEXT. Full-text indexes can be used only with InnoDB or MyISAM tables, and can be created only for CHAR, VARCHAR, or TEXT columns. A FULLTEXT index definition can be given in the CREATE TABLE statement when a table is created, or added later using ALTER TABLE or CREATE INDEX.</p> </li> </ol> <p>Remember, the type of index used depends on the specific requirements of the database system and the data it is handling.</p>"},{"location":"databases/scaling/materialized-view/","title":"Materialized View","text":"<p>A Materialized View is a database object that contains the results of a query. It is a physical copy, snapshot or a representation of the base table(s) that are stored on the disk like a regular table with a set of precomputed rows. </p> <p>The query can be a join or aggregated data, such as the sum of sales. Materialized views are used as a performance-enhancing technique. When you create a materialized view, Oracle Database creates one internal table and at least one index, and may create one view, all in the schema of the materialized view.</p> <p>Materialized views can be refreshed periodically using the original base tables to get updated data, thereby making the Materialized View synchronous with the base tables. </p> <p>Here's an example of creating a materialized view in SQL:</p> <pre><code>CREATE MATERIALIZED VIEW sales_summary AS\nSELECT product_id, SUM(sales) as total_sales\nFROM sales\nGROUP BY product_id;\n</code></pre> <p>In this example, <code>sales_summary</code> is a materialized view that stores the total sales for each product. The database would keep this data stored so that it can quickly return the total sales for a product without needing to perform the aggregation every time.</p> <p>Materialized views are particularly useful when you have heavy aggregations or complex joins that are performed frequently, as they can speed up query performance by pre-calculating the expensive parts of the query. However, they do take up additional storage space, and there is a cost associated with keeping them up-to-date.</p>"},{"location":"databases/scaling/replication/","title":"Database Replication","text":"<p>Database replication is a technique in which data from a database is copied (replicated) from one database (master) to another database (replica) to keep the replica database synchronized with the master database. This process can be done in real-time or at scheduled intervals.</p> <p>The main purpose of database replication is to ensure data availability and consistency across different geographical and network locations. It also helps in load balancing by allowing read operations to be distributed across multiple nodes.</p> <p>There are several types of database replication, including:</p> <ol> <li> <p>Master-Slave Replication: In this type, one database server maintains the master database and the rest of the database servers maintain the slave databases. All data modification operations (INSERT, UPDATE, DELETE) are performed on the master database and then replicated to the slave databases.</p> </li> <li> <p>Multi-Master Replication: In this type, two or more database servers maintain the same database. Any changes made to any server are replicated to all other servers. This type of replication is more complex but provides a higher level of availability.</p> </li> <li> <p>Snapshot Replication: In this type, the entire database is copied from the master to the slave at scheduled intervals. This is a simple form of replication but can be resource-intensive for large databases.</p> </li> <li> <p>Transactional Replication: In this type, only the changes (transactions) made at the master database are sent to the slave databases. This is more efficient than snapshot replication for databases where changes are relatively infrequent.</p> </li> <li> <p>Merge Replication: In this type, changes can be made to both the master and slave databases and the changes are merged together. This is useful for distributed databases where network connectivity cannot be guaranteed.</p> </li> </ol> <p>Remember, the choice of replication strategy depends on the specific requirements of your application, including the need for data availability, consistency, network bandwidth, and system performance.</p>"},{"location":"databases/scaling/sharding/","title":"Database Shrading","text":"<p>Database sharding is a type of database partitioning that separates very large databases into smaller, faster, more easily managed parts called data shards. The word shard means a small part of a whole. </p> <p>Sharding is a method of splitting and storing a single logical dataset in multiple databases. By distributing the data among multiple machines, a shard can improve performance in retrieving the data.</p> <p>Here are some key points about sharding:</p> <ol> <li> <p>Horizontal Partitioning: Sharding is a type of horizontal partitioning. While vertical partitioning involves creating tables with fewer columns and using additional tables to store the remaining columns, horizontal partitioning is about creating tables with fewer rows.</p> </li> <li> <p>Data Distribution: In sharding, data is distributed across multiple databases such that each database only manages a subset of the data.</p> </li> <li> <p>Scalability: Sharding can be a great way to achieve scalability. It can allow you to add additional machines to support more transactions and store more data.</p> </li> <li> <p>Performance: Sharding can improve application performance through query load balancing and reducing the database index size.</p> </li> <li> <p>Complexity: Implementing sharding architecture can be complex. Data distribution, shard performance, handling shard failures, and maintaining data consistency are some of the challenges with sharding.</p> </li> <li> <p>Shard Key: The shard key is a data item that is used to determine the allocation of the rows to the shards. The choice of the shard key can impact the performance of the sharded database.</p> </li> </ol> <p>Sharding is used in data-intensive applications like social networks, IoT applications, and in high-traffic website backends. It's important to note that sharding comes with its own complexities and potential drawbacks, so it should be implemented judiciously.</p>"},{"location":"databases/scaling/vertical-scaling/","title":"Vertical Scaling","text":"<p>Database vertical scaling, also known as \"scaling up\", is a method of increasing database capacity by adding more power to an existing machine. This can involve increasing the CPU power, RAM, SSD, or other hardware resources on a single machine.</p> <p>In the context of databases, vertical scaling can help to handle a larger load by allowing more transactions to be processed simultaneously and more data to be stored in memory, which can significantly speed up query processing times.</p> <p>Here are some key points about vertical scaling:</p> <ol> <li> <p>Performance Improvement: Vertical scaling can lead to significant performance improvements, as it allows for more computational resources per transaction or query.</p> </li> <li> <p>Simplicity: Vertical scaling is generally simpler to implement than horizontal scaling (scaling out) because it doesn't require changes to the database schema or application logic.</p> </li> <li> <p>Cost: While vertical scaling can be more cost-effective in the short term, it can become expensive as you reach the limits of available technology. There's a limit to how much you can scale up a single machine.</p> </li> <li> <p>Single Point of Failure: With vertical scaling, your entire database system resides on a single machine. If that machine fails, your entire database system goes down.</p> </li> <li> <p>Downtime: Vertical scaling often requires downtime while you shut down your database system, upgrade it, and then restart it.</p> </li> <li> <p>Limited by Hardware: The extent to which you can scale vertically is limited by the maximum capacity of individual hardware components. </p> </li> </ol> <p>In contrast, horizontal scaling, also known as \"scaling out\", involves adding more machines to a system and distributing the load across multiple machines. While it can be more complex to implement, it allows for virtually unlimited scalability. </p> <p>The choice between vertical and horizontal scaling depends on the specific requirements of your application, including the need for performance, cost-effectiveness, and high availability.</p>"},{"location":"digital-content/","title":"A Guide to Protection and Consuming Online Content","text":"<p>In our fast-paced digital world, accessing a myriad of resources online has become second nature. From software and media assets to various forms of content, the internet has made it all readily available. However, the ease of access often leads to confusion between content that is freely accessible and content that is available for free. It's crucial to recognize the <code>stringent rules</code> governing the usage of online materials, serving to uphold copyright laws and honor the effort invested by creators.</p>"},{"location":"digital-content/#understanding-your-rights-as-a-content-creator","title":"Understanding Your Rights as a Content Creator","text":"<ol> <li> <p>Protecting Your Content</p> <p>As a content creator, safeguarding your creations \u2013 be it text, images, or media \u2013 is paramount. Explore strategies to fortify your content against unauthorized use or reproduction.</p> </li> <li> <p>Contesting Unlawful Copies</p> <p>Discover ways to challenge and address instances of unauthorized copying or distribution of your content. Being proactive in contesting such actions helps maintain the integrity of your work.</p> </li> <li> <p>Handling Mistaken Notices</p> <p>In the digital realm, misunderstandings can occur. Learn how to effectively counter and rectify mistaken copyright notices, ensuring that your content is rightfully protected.</p> </li> <li> <p>Sharing Content Internationally</p> <p>For those creators with a global audience, consider the implications of sharing your content internationally. Familiarize yourself with international copyright laws and best practices to navigate this aspect successfully.</p> </li> </ol>"},{"location":"digital-content/#navigating-content-consumption","title":"Navigating Content Consumption","text":"<ol> <li> <p>Understanding Usage Limitations</p> <p>As a content consumer, it's vital to recognize the limitations surrounding the usage of publicly accessible content. Ensure you are aware of the rights and restrictions associated with the materials you engage with.</p> </li> <li> <p>Correcting Honest/Accidental Mistakes</p> <p>Mistakes can happen, even in the digital space. Learn how to rectify errors in the use of content promptly and responsibly, preserving a respectful relationship with creators.</p> </li> <li> <p>Handling International Content</p> <p>In an interconnected world, accessing content from various regions is common. Be mindful of cultural nuances and legal disparities when engaging with international content, ensuring a respectful and lawful experience.</p> </li> </ol> <p>In conclusion, whether you're a content creator or consumer, understanding the nuances of content protection is crucial in today's digital age. By navigating the intricate web of rights and responsibilities, you contribute to the creation of a digital environment that respects the creativity and effort invested by individuals worldwide.</p>"},{"location":"digital-content/content-creation/","title":"Content Creation","text":"<p>Creating content for platforms like YouTube and TikTok requires a tailored approach to engage audiences effectively. While various strategies can work, the effectiveness often depends on your content, target audience, and platform dynamics. Here are some key strategies for both platforms:</p>"},{"location":"digital-content/content-creation/#youtube","title":"YouTube:","text":"<ol> <li> <p>Consistent Schedule:    Establish a consistent posting schedule. Regular uploads help build anticipation and keep your audience engaged.</p> </li> <li> <p>Quality Production:    Invest in good video and audio quality. Viewers on YouTube often appreciate well-produced content.</p> </li> <li> <p>Keyword Optimization:    Optimize your video titles, descriptions, and tags with relevant keywords. This improves discoverability on YouTube's search and recommendation algorithms.</p> </li> <li> <p>Engaging Thumbnails:    Create eye-catching thumbnails that accurately represent your video content. Thumbnails play a significant role in attracting clicks.</p> </li> <li> <p>Compelling Thumbnails:    Capture viewers' attention within the first few seconds. Hook them with an intriguing introduction to encourage them to watch the entire video.</p> </li> <li> <p>Community Interaction:    Respond to comments and engage with your audience. Building a community around your channel fosters loyalty and encourages more people to subscribe.</p> </li> <li> <p>Collaborations:    Collaborate with other YouTubers in your niche. This can introduce your content to a broader audience and provide cross-promotional opportunities.</p> </li> <li> <p>Playlists:    Organize your videos into playlists. This encourages viewers to watch more of your content, increasing your overall watch time.</p> </li> </ol>"},{"location":"digital-content/content-creation/#tiktok","title":"TikTok:","text":"<ol> <li> <p>Short and Snappy Content:    Create short, attention-grabbing videos. TikTok's format thrives on brief, engaging content that quickly captures viewers' interest.</p> </li> <li> <p>Trend Participation:    Stay current with TikTok trends. Participate in popular challenges or use trending sounds to leverage the platform's algorithm.</p> </li> <li> <p>Authenticity:    Showcase authenticity. TikTok users appreciate genuine, relatable content. Be yourself and connect with your audience on a personal level.</p> </li> <li> <p>Creative Editing:    Explore TikTok's creative tools for video editing. Experiment with effects, transitions, and text overlays to make your content visually appealing.</p> </li> <li> <p>Consistent Branding:    Maintain a consistent style and branding for your TikTok profile. This helps users recognize your content and builds a sense of familiarity.</p> </li> <li> <p>Hashtags:    Use relevant and popular hashtags to increase the discoverability of your TikTok videos. Research trending hashtags in your niche.</p> </li> <li> <p>Duets and Stitches:    Take advantage of TikTok's duet and stitch features. Collaborate with other creators or interact with popular videos to expand your reach.</p> </li> <li> <p>Cross-Promotion:    Share your TikTok videos on other social media platforms. Cross-promotion can drive traffic to your TikTok account and increase your follower count.</p> </li> </ol> <p>Ultimately, the key is to understand your audience, stay true to your style, and adapt your strategies based on platform trends and user feedback. Experimentation and staying informed about platform updates can also contribute to your success on YouTube and TikTok.</p>"},{"location":"digital-content/dmca/","title":"Demystifying the DMCA","text":"<p>Subtitle: Navigating Safe Harbors and Copyright Compliance</p> <p>In the dynamic digital landscape, the protection of intellectual property has become a paramount concern, especially for service providers hosting user-generated content. The Digital Millennium Copyright Act (DMCA) emerges as a crucial piece of legislation, providing a safe harbor for these service providers while addressing the complexities of copyright infringement.</p>"},{"location":"digital-content/dmca/#understanding-the-dmcas-safe-harbor","title":"Understanding the DMCA's Safe Harbor","text":"<p>The DMCA's Safe Harbor provision is a lifeline for internet service providers grappling with the potential repercussions of hosting user-generated content. Even a single claim of copyright infringement can result in statutory damages of up to $150,000. This looming threat of liability underscores the critical need for a protective mechanism.</p> <p>To mitigate these risks, the DMCA establishes a copyright liability safe harbor for internet service providers. In essence, this provision shields service providers from being held liable for copyright infringement stemming from user-generated content, given they adhere to the DMCA's notice-and-takedown rules.</p>"},{"location":"digital-content/dmca/#the-notice-and-takedown-mechanism","title":"The Notice-and-Takedown Mechanism","text":"<p>The cornerstone of the DMCA's safe harbor is the notice-and-takedown mechanism. When a copyright owner identifies potentially infringing content on a service provider's platform, they can submit a formal notice. The service provider, in turn, is obligated to promptly remove the contested material.</p> <p>This system not only provides a swift resolution for copyright owners but also ensures that service providers can continue offering platforms for user-generated content without constant fear of legal repercussions.</p>"},{"location":"digital-content/dmca/#prohibiting-circumvention-of-technical-measures","title":"Prohibiting Circumvention of Technical Measures","text":"<p>Beyond the safe harbor provision, the DMCA also tackles another facet of digital copyright protection\u2014 the circumvention of technical measures. The act prohibits the circumvention of technical measures that control access to works protected by copyright.</p> <p>This provision is crucial in safeguarding the integrity of digital content. It prevents individuals from bypassing technological safeguards put in place to control access to copyrighted works, ensuring that creators' rights are upheld in the digital realm.</p>"},{"location":"digital-content/dmca/#striking-a-balance","title":"Striking a Balance","text":"<p>While the DMCA offers essential protections for service providers and copyright owners alike, it also highlights the delicate balance required in the digital age. Facilitating the free flow of information and creativity while respecting intellectual property rights remains a complex challenge.</p> <p>In conclusion, the DMCA stands as a vital legal framework, providing a safe harbor for service providers navigating the intricate landscape of user-generated content. By understanding and adhering to its provisions, both content creators and service providers can contribute to a digital environment that fosters innovation while respecting copyright protections.</p>"},{"location":"digital-content/dmca/#resources","title":"Resources","text":"<ul> <li>DMCA : Protection</li> <li>DMCA : About us</li> <li>ReadTheDocs : DMCA Takedown Policy</li> <li>GitHub : DMCA Takedown Policy</li> </ul>"},{"location":"digital-content/protecting-content/","title":"Protecting your Content","text":"<p>Protecting your content on platforms like YouTube and TikTok involves different strategies due to the nature and format of the content. Here are tailored content protection strategies for each platform:</p>"},{"location":"digital-content/protecting-content/#youtube","title":"YouTube:","text":"<ol> <li> <p>Watermarking:    Consider adding a subtle watermark to your videos. This can be your channel logo or a unique identifier. Watermarks can discourage unauthorized use and serve as a branding element.</p> </li> <li> <p>License and Usage Terms:    Clearly state the terms of use and licensing for your content in your video descriptions or on your channel page. This informs viewers about how they can and cannot use your content.</p> </li> <li> <p>Enable Content ID:    If eligible, use YouTube's Content ID system. This tool automatically scans uploaded videos for matches with your content, allowing you to manage and monetize or block unauthorized use.</p> </li> <li> <p>Regularly Monitor Comments and Shares:    Keep an eye on the comments section and track how your videos are shared. Promptly address any instances of unauthorized use reported by your audience.</p> </li> <li> <p>Collaborate with Brands and Verified Users:    Collaborate with established brands and verified users. This not only expands your reach but also adds an additional layer of protection as these entities are less likely to engage in unauthorized use.</p> </li> </ol>"},{"location":"digital-content/protecting-content/#tiktok","title":"TikTok:","text":"<ol> <li> <p>Watermarking and Branding:    Incorporate a subtle watermark or your TikTok username into your videos. This not only serves as a form of branding but also makes it more difficult for others to claim your content as their own.</p> </li> <li> <p>Private Accounts:    Consider making your TikTok account private, limiting access to your videos to approved followers. While this may reduce visibility, it adds a layer of protection against unauthorized downloads.</p> </li> <li> <p>Educate Your Audience:    Use your videos or captions to educate your audience about the importance of respecting intellectual property rights. Encourage proper attribution and discourage unauthorized use.</p> </li> <li> <p>Report and Block Features:    Utilize TikTok's reporting and blocking features. If you come across instances of content theft or unauthorized use, report the user and consider blocking them to prevent further issues.</p> </li> <li> <p>Limit Duets and Stitches:    While duets and stitches can be great for engagement, be cautious about how you use these features. Limiting duets and stitches may reduce the chances of someone using your content without permission.</p> </li> <li> <p>Cross-Platform Promotion:    If you share your TikTok content on other platforms, be vigilant about how it is used. Monitor its usage outside of TikTok and take action if you notice any unauthorized use.</p> </li> </ol> <p>Remember that these strategies work best when implemented together. Consistency in applying protective measures and staying vigilant in monitoring your content across platforms are key elements of a comprehensive content protection strategy.</p>"},{"location":"digital-content/securing-creativity/","title":"Securing Your Creativity: Effective Strategies to Protect Your Content Online","text":"<p>Protecting your content from unauthorized use or reproduction involves implementing effective strategies to safeguard your intellectual property. Here are some practical measures you can take:</p> <ol> <li> <p>Copyright Registration:    Register your content with the relevant copyright authorities. This provides legal evidence of your ownership and makes it easier to take legal action against infringement.</p> </li> <li> <p>Watermarking:    Apply visible or invisible watermarks to your images or media. This not only serves as a deterrent but also helps in identifying the original source in case of unauthorized use.</p> </li> <li> <p>Use of Digital Signatures:    Consider applying digital signatures to your electronic files. <code>Digital signatures</code> provide a unique identifier that verifies the authenticity and integrity of your content.</p> </li> <li> <p>Leverage Content Protection Technologies:    Explore <code>digital rights management</code> (DRM) tools and technologies that control access to your content. These systems can restrict copying, printing, and sharing without proper authorization.</p> </li> <li> <p>Terms of Use and Licensing:    Clearly outline the terms of use and licensing agreements for your content. Make sure users understand how they can and cannot use your work. This adds a layer of legal protection and helps set expectations.</p> </li> <li> <p>Regular Monitoring and Enforcement:    Implement a system for regularly monitoring the usage of your content online. Utilize tools and services that can identify potential instances of unauthorized use. Take prompt action to enforce your rights when infringement is detected.</p> </li> <li> <p>Educate Your Audience:    Educate your audience about the importance of respecting intellectual property rights. Clearly communicate the terms of use and encourage proper attribution when sharing your content.</p> </li> <li> <p>Collaborate with Platforms:    Work with online platforms and social media networks to report and address unauthorized use of your content. Many platforms have mechanisms in place to handle copyright infringement claims.</p> </li> <li> <p>Secure Your Website:    If you share your content on a website, implement security measures to prevent unauthorized downloads or copying. This may include disabling right-click options or using secure streaming technologies.</p> </li> <li> <p>Legal Action When Necessary:     If all else fails, be prepared to take legal action against individuals or entities that persistently infringe on your intellectual property rights. Consult with legal professionals to explore your options.</p> </li> </ol> <p>Remember that a combination of these strategies may be the most effective way to protect your content. It's also essential to stay vigilant and adapt your approach as technology and online behaviors evolve.</p>"},{"location":"digital-content/software-licensing/","title":"Demystifying Software Licensing","text":"<p>Subtitle: A Developer's Guide to Code Protection</p> <p>In the realm of software development, the significance of software licensing cannot be overstated. Whether you're crafting proprietary code or contributing to open-source projects, licensing serves as a crucial aspect that benefits both the developers and end-users. Let's delve into the depths of software licensing, exploring its importance, common practices, and a variety of popular licenses including MIT, Apache 2.0, GNU GPL, BSD, Mozilla MPL, Creative Commons, ISC, and LGPL.</p>"},{"location":"digital-content/software-licensing/#understanding-the-essence-of-software-licensing","title":"Understanding the Essence of Software Licensing","text":"<p>When you invest time and effort in developing software, licensing your code becomes more than a suggestion; it becomes a prudent practice. This is particularly true for those venturing into the world of open-source software. Licensing isn't just about protecting your intellectual property; it's about fostering a transparent and mutually beneficial relationship between developers and users.</p>"},{"location":"digital-content/software-licensing/#the-role-of-licensing","title":"The Role of Licensing","text":"<p>Consider software licensing as a language that speaks to users, outlining the terms and conditions under which they can interact with your creation. It's a set of rules that demystify the \"how\" of using your software. Whether you wish to grant users the freedom to modify, build applications over, or use your software in specific ways, a well-defined license is the key to clear communication.</p>"},{"location":"digital-content/software-licensing/#popular-licenses-unveiled","title":"Popular Licenses Unveiled","text":"<p>Now, let's shine a spotlight on a variety of licenses that developers commonly encounter in the software development landscape.</p>"},{"location":"digital-content/software-licensing/#mit-license","title":"MIT License","text":"<p>The MIT License is a permissive open-source license that allows developers to use, modify, and distribute the software for almost any purpose. It grants users the freedom to integrate the code into their projects without the fear of legal encumbrances. This simplicity and permissiveness make the MIT License a favorite among developers keen on fostering collaboration and innovation.</p> <p>Example: The Node.js JavaScript runtime is licensed under the MIT License.</p>"},{"location":"digital-content/software-licensing/#apache-20-license","title":"Apache 2.0 License","text":"<p>The Apache 2.0 License, while also permissive, comes with additional protections and obligations compared to the MIT License. It includes a patent license, providing a degree of protection against intellectual property disputes. This license encourages collaboration but requires contributors to grant patent rights for any contributions. The Apache 2.0 License strikes a balance between openness and legal safeguards.</p> <p>Example: The Apache HTTP Server, one of the most widely used web servers, is licensed under Apache 2.0.</p>"},{"location":"digital-content/software-licensing/#gnu-general-public-license-gpl","title":"GNU General Public License (GPL)","text":"<p>The GPL is one of the most well-known open-source licenses. It ensures that any derivative work using GPL-licensed code must also be open-source. There are different versions of the GPL, such as GPL-2.0 and GPL-3.0.</p> <p>Example: The Linux operating system kernel is released under the GNU General Public License.</p>"},{"location":"digital-content/software-licensing/#bsd-licenses-berkeley-software-distribution","title":"BSD Licenses (Berkeley Software Distribution)","text":"<p>BSD licenses come in various forms (e.g., 2-clause, 3-clause). They are permissive licenses allowing the use, modification, and distribution of the software with minimal restrictions. BSD licenses are often chosen for their simplicity.</p> <p>Example: The FreeBSD operating system uses the 2-clause BSD License.</p>"},{"location":"digital-content/software-licensing/#mozilla-public-license-mpl","title":"Mozilla Public License (MPL)","text":"<p>The MPL is a copyleft license that allows for the creation of proprietary derivative works while requiring any changes to the MPL-licensed code to be open-source. It aims to find a balance between open-source principles and proprietary software.</p> <p>Example: The Mozilla Firefox web browser is released under the Mozilla Public License.</p>"},{"location":"digital-content/software-licensing/#creative-commons-licenses","title":"Creative Commons Licenses","text":"<p>While not specifically designed for software, Creative Commons licenses are sometimes used for open-source projects, especially for non-code assets like documentation and media. They provide a range of permissions and restrictions.</p> <p>Example: The Blender 3D creation suite uses Creative Commons licenses for some of its documentation and media.</p>"},{"location":"digital-content/software-licensing/#isc-license","title":"ISC License","text":"<p>The ISC License is a permissive open-source license similar to the MIT License. It is concise and easy to understand, making it a choice for projects seeking minimal licensing complexity.</p> <p>Example: The OpenBSD operating system uses the ISC License.</p>"},{"location":"digital-content/software-licensing/#gnu-lesser-general-public-license-lgpl","title":"GNU Lesser General Public License (LGPL)","text":"<p>The LGPL is similar to the GPL but has more permissive terms regarding linking and usage in proprietary software. It is often chosen for libraries and allows for a broader range of applications.</p> <p>Example: The GTK toolkit, used in graphical user interface development, is released under the LGPL.</p>"},{"location":"digital-content/software-licensing/#navigating-the-software-licensing-landscape","title":"Navigating the Software Licensing Landscape","text":"<p>As a developer, adopting a software license is not just a legal formality; it's a conscientious decision that sets the tone for your software's journey in the digital world. By embracing licensing, you empower users with clarity, foster collaboration within the development community, and mitigate legal risks.</p> <p>In conclusion, while the process of licensing may seem like a formality, it is, in fact, a powerful tool for creating a harmonious ecosystem within the software development realm. Choose your license wisely, communicate your terms transparently, and contribute to a community that thrives on clarity, collaboration, and innovation.</p>"},{"location":"digital-content/software-licensing/#references","title":"References","text":"<ul> <li>https://www.mend.io/blog/top-10-apache-license-questions-answered/</li> </ul>"},{"location":"digital-content/user-privacy/","title":"A Deep Dive into User Privacy","text":"<p>In our interconnected world, where digital footprints are omnipresent, the topic of user privacy has never been more critical. As we seamlessly move through the online realm, from social media interactions to e-commerce transactions, the protection of personal information becomes a paramount concern. This blog explores the nuances of user privacy, delving into its significance, challenges, and the evolving landscape of data protection.</p>"},{"location":"digital-content/user-privacy/#the-essence-of-user-privacy","title":"The Essence of User Privacy","text":""},{"location":"digital-content/user-privacy/#what-is-user-privacy","title":"What is User Privacy?","text":"<p>User privacy encompasses the right of individuals to control their personal information and determine how it is collected, used, and shared. In the digital age, where data is a commodity, safeguarding user privacy becomes a fundamental aspect of ethical and responsible data practices.</p>"},{"location":"digital-content/user-privacy/#the-evolving-landscape","title":"The Evolving Landscape","text":""},{"location":"digital-content/user-privacy/#digital-footprints-and-big-data","title":"Digital Footprints and Big Data","text":"<p>Our online activities generate a continuous stream of data, creating intricate digital footprints. From search queries to location tracking, the sheer volume of information collected is staggering. Big Data analytics leverage this wealth of data for insights, but it also raises concerns about user anonymity and the potential for misuse.</p>"},{"location":"digital-content/user-privacy/#challenges-in-the-digital-sphere","title":"Challenges in the Digital Sphere","text":""},{"location":"digital-content/user-privacy/#data-breaches-and-cybersecurity-threats","title":"Data Breaches and Cybersecurity Threats","text":"<p>The rise in cyber threats has led to an alarming increase in data breaches. Major corporations and platforms have fallen victim to cyber-attacks, exposing sensitive user information. The fallout from such breaches not only jeopardizes user privacy but also erodes trust in digital ecosystems.</p>"},{"location":"digital-content/user-privacy/#surveillance-and-privacy-erosion","title":"Surveillance and Privacy Erosion","text":"<p>Surveillance practices, both by governments and private entities, pose a threat to user privacy. Issues such as mass surveillance and the use of facial recognition technologies raise ethical questions about the balance between security measures and individual privacy rights.</p>"},{"location":"digital-content/user-privacy/#the-regulatory-landscape","title":"The Regulatory Landscape","text":""},{"location":"digital-content/user-privacy/#gdpr-ccpa-and-beyond","title":"GDPR, CCPA, and Beyond","text":"<p>In response to growing privacy concerns, regulations such as the General Data Protection Regulation (GDPR) in Europe and the California Consumer Privacy Act (CCPA) in the United States have been enacted. These regulations empower users with greater control over their data and impose stringent obligations on businesses to ensure compliance.</p>"},{"location":"digital-content/user-privacy/#safeguarding-user-privacy","title":"Safeguarding User Privacy","text":""},{"location":"digital-content/user-privacy/#encryption-and-anonymization","title":"Encryption and Anonymization","text":"<p>Implementing robust encryption measures and anonymizing user data are crucial steps in safeguarding privacy. These techniques protect sensitive information from unauthorized access and mitigate the risk of personally identifiable data exposure.</p>"},{"location":"digital-content/user-privacy/#transparent-data-practices","title":"Transparent Data Practices","text":"<p>Transparency in data collection, usage, and sharing is fundamental to maintaining user trust. Organizations should adopt clear and concise privacy policies, providing users with comprehensive information about how their data will be handled.</p>"},{"location":"digital-content/user-privacy/#user-education-and-empowerment","title":"User Education and Empowerment","text":"<p>Educating users about privacy risks and empowering them with tools to control their online footprint are essential components of a privacy-centric approach. From privacy settings on social media platforms to awareness campaigns, user empowerment is a key factor in fostering a privacy-conscious society.</p>"},{"location":"digital-content/user-privacy/#the-future-of-user-privacy","title":"The Future of User Privacy","text":""},{"location":"digital-content/user-privacy/#emerging-technologies-and-privacy-by-design","title":"Emerging Technologies and Privacy by Design","text":"<p>As technologies like artificial intelligence and the Internet of Things become pervasive, the concept of \"privacy by design\" gains prominence. Embedding privacy considerations into the development of technologies ensures that privacy is not an afterthought but an integral part of innovation.</p>"},{"location":"digital-content/user-privacy/#ethical-data-practices","title":"Ethical Data Practices","text":"<p>The future of user privacy lies in ethical data practices. Organizations must prioritize ethical considerations, ensuring that data collection and usage align with user expectations and societal values.</p>"},{"location":"digital-content/user-privacy/#conclusion","title":"Conclusion","text":"<p>User privacy stands at the intersection of technology, ethics, and regulation. As custodians of digital spaces, it is incumbent upon individuals, businesses, and policymakers to collaborate in creating a digital landscape that respects and protects user privacy. By embracing transparent practices, robust regulations, and ethical principles, we can navigate the digital realm while preserving the fundamental right to privacy. After all, in this interconnected world, the safeguarding of user privacy is not just a responsibility; it's a shared commitment to a more trustworthy and secure digital future.</p> <ul> <li>GDPR</li> <li>CCPA</li> </ul>"},{"location":"kafka/","title":"Kafka","text":"<p>Kafka is going to be the main focus of this section, while also explaining general concepts.</p>"},{"location":"kafka/#todo-kafka-outline","title":"TODO Kafka Outline","text":"<ul> <li>[ ] Observer</li> <li>[ ] Event Bus</li> <li>[ ] Pub-Sub</li> <li>[ ] Message Broker</li> </ul>"},{"location":"kafka/#index","title":"Index","text":"<ul> <li>Publish/Subscribe Messaging</li> <li>Why Kafka?</li> <li>Kafka use-cases</li> <li>Terms Explained</li> <li>Installation</li> </ul>"},{"location":"kafka/#history","title":"History","text":"<p>Kafka was originally developed by Jay Kreps, LinkedIn, later released as an open source project on GitHub in late 2010. As it started to gain attention in the open source community, it was proposed and accepted as an Apache Software Foundation incubator project in July of 2011. Apache Kafka graduated from the incubator in October of 2012.</p> <p>LinkedIn continues to maintain several, including Cruise Control, Kafka Tools, Kafka Monitor, and Burrow. In addition to its commercial offerings, Confluent has released projects including ksqlDB, a schema registry, and a REST proxy under a community license (which is not strictly open source, as it includes use restrictions).</p> <p>In the fall of 2014, Jay Kreps, Neha Narkhede, and Jun Rao left LinkedIn to found Confluent, a company centered around providing development, enterprise support, and training for Apache Kafka. Confluent, through a partnership with Google, provides managed Kafka clusters on Google Cloud Platform, as well as similar services on Amazon Web Services and Azure. One of the other major initiatives of Confluent is to organize the Kafka Summit conference series.</p>"},{"location":"kafka/#message-queue-vs-message-bus","title":"Message Queue vs Message Bus","text":"<p>There is a lot of overlap in the terms that we use today, in the modern world. They\u2019re all similar: they share similar interfaces (sending and receiving events); they share many features; and they\u2019re both used in complex products or at scale. While similar, they\u2019re (typically) used for different purposes.</p> <p>A typical queue receives events, buffers them (typically persistently), and allows a worker to read from the queue to process the events. It gives you:</p> <ol> <li>Ordering</li> <li>Coupling</li> <li>Pull-based</li> <li>Retries</li> </ol> <p>A typical message bus (or, message broker, event bus, or event broker) also accepts events to be received by other services, though they\u2019re different than queues. Within a message broker, you typically send events to a \u2018topic\u2019 (instead of a queue) which is then received by one or more services \u2014 unlike a single service within a queue. It gives you:</p> <ol> <li>Fan out</li> <li>Delivery guarantees</li> <li>Real-time distribution</li> <li>Scale</li> </ol>"},{"location":"kafka/#references","title":"References","text":"<ul> <li>Kafka: The Definitive Guide (book)</li> <li>Kafka in Action (book)</li> </ul>"},{"location":"kafka/00-PREFACE/","title":"Preface","text":"<p>With this book, I've made an attempt to explain the world of <code>asynchronous communication</code> over internet, in my own words (along with some borrowed words).</p> <p>Note: The contents of this book do not have to be ready cover-to-cover. Instead feel free to jump around sections that appeal most to you.</p>"},{"location":"kafka/00-PREFACE/#forms-of-communication","title":"Forms of communication","text":"<ul> <li>Synchrounous</li> <li>Asynchrounous</li> <li>Messaging</li> </ul>"},{"location":"kafka/00-PREFACE/#how-computers-talk","title":"How computers \"talk\"?","text":"<p>Computer \"A\" sends a message to computer \"B\". Until \"B\" responds back to \"A\", \"A\" has no way of knowing whether \"B\" received the message or not. Something along the lines of <code>delivery confirmation</code> (acknowledging) or even <code>an answer</code> (responding) to a question asked.</p> <p><code>Synchrounous communication (sync)</code> happens when \"A\" decides to <code>wait</code> for \"B\" to respond, and not do anything until it recieves a response.</p> <p><code>Asynchrounous communication (async)</code> happens when \"A\" continues performing other tasks, while \"B\" sends the response back. To do this \"A\" would send the message, somehow remember it is yet to get the response back, continune performing other tasks, and when receives the response from \"B\", goes back to reading it. Similar to how time-sharing systems work, formally known as multi-tasking.</p> <p><code>Message-based communication (event)</code> happens when \"A\" is only interested in sending a message to \"B\", and not care about what \"B\" does with it.</p>"},{"location":"kafka/00-PREFACE/#protocols","title":"Protocols","text":"<p>How does \"B\" know that it has to even respond back to \"A\"? How does \"B\" know how to read the message that \"A\" sent? - all binary</p>"},{"location":"kafka/00-PREFACE/#sync-vs-async","title":"Sync vs Async","text":"<p>So long we've relied on exchange mechanisms for communication, and we still do. It is very effective, and the right use case for <code>a variety of needs</code>. There is nothing wrong in it. And it is going nowhere.</p> <p>What has changed, is our needs. Our needs have evolved into doing everything faster and doing a lot more at the same time. This is where at times it becomes impractical to committing on real-time communication, a.k.a. synchrounous communication.</p>"},{"location":"kafka/01-pub-sub-messaging/","title":"Pub/Sub Messaging","text":"<p>Also known as <code>Publish-Subscribe messaging pattern</code>.</p> <p>The <code>sender</code> (publisher) classifies data (message), and sends it without directing it to a receiver.</p> <p>The <code>receiver</code> (subscriber) subscribes to a certain class of messages.</p> <p>The <code>broker</code> is a central place where messages are published (stored), and that facilitates this pattern.</p>"},{"location":"kafka/01-pub-sub-messaging/#use-case-simple-message-queue-or-interprocess-communication","title":"Use Case: simple message queue or interprocess communication","text":"<p>You need to send information \u201csomewhere\u201d without adding latency to the current task. So you open a direct connection from your application, implement a simple queue to be accessed on a port, for other applications to read. You send messages directly to the interested application.</p> <p></p> <p>This doesn\u2019t work when you decide you want to analyze your metrics over a long term. Instead your receiving app will need to store the messages for them to be analyzed. Every single application needing this feature will have to implement a connection to the receiving application.</p> <p>Instead you may choose to do polling. However after a while you have other servers interested in the same data. Hence the architecture may evolve into something like:</p> <p></p> <p>The technical debt being built up here is obvious. To simplify this you may end up setting a single service as a main place receiving those metrics:</p> <p></p> <p>At the same time you or other team may want to do something similar with logging and tracing as well, other than metrics:</p> <p></p> <p>This is certainly a lot better than point to point connection, but still there is a lot of duplication, having multiple systems for queuing the data.</p>"},{"location":"kafka/01-pub-sub-messaging/#solution","title":"Solution","text":"<p>Kafka was built to solve this problem, often called \u201cdistributed commit log\u201d or \u201cdistributed streaming platform\u201d.  This system is built to provide a durable record of all transactions so they can be replayed to consistently build the state of a system.</p> <p>Data within Kafka is stored durably, in order, and can be read deterministically. Additionally, data can be distributed within the system to provide additional protections against failures, as well as support higher performance via scaling.</p>"},{"location":"kafka/02-why-kafka/","title":"Why Kafka?","text":"<p>Among the many choices for a pub/sub messaging system out there, Kafka can be a good choice because:</p>"},{"location":"kafka/02-why-kafka/#multiple-producers","title":"Multiple Producers","text":"<p>Kafka is able to seamlessly handle multiple producers, whether those clients are using many topics or the same topic. This makes the system ideal for aggregating data from many clients and making it consistent. Consumer applications can then receive a single stream of events without having to coordinate consuming from multiple topics.</p>"},{"location":"kafka/02-why-kafka/#multiple-consumers","title":"Multiple Consumers","text":"<p>Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other client. This is in contrast to many queuing systems where once a message is consumed by one client only. Multiple Kafka consumers can choose to operate as part of a group and share a stream, assuring that the entire group processes a given message only once.</p>"},{"location":"kafka/02-why-kafka/#disk-based-retention","title":"Disk-based Retention","text":"<p>Durable message retention means that consumers do not always need to work in real time. Messages are written to disk and will be stored with configurable retention rules. This configuration can be applied on a per-topic basis, allowing for different streams of messages to have different amounts of retention depending on the consumer needs. Durable retention means that if a consumer falls behind, either due to slow processing or a burst in traffic, there is no danger of losing data. It also means that maintenance can be performed on consumers, taking applications offline for a short period of time, with no concern about messages backing up on the producer or getting lost. This allows the consumers to restart and pick up processing messages where they left off with no data loss.</p>"},{"location":"kafka/02-why-kafka/#scalable","title":"Scalable","text":"<p>Users can start with a single broker, and add tens or even hundreds of brokers to the cluster that grows over time as the data scales up. Expansions can be performed while the cluster is online, with no impact on the availability of the system as a whole. This also means that a cluster of multiple brokers can handle the failure of an individual broker and continue servicing clients. Clusters that need to tolerate more simultaneous failures can be configured with higher replication factors.</p>"},{"location":"kafka/02-why-kafka/#high-performance","title":"High Performance","text":"<p>Producers, consumers, and brokers can all be scaled out to handle very large message streams with ease. This can be done while still providing subsecond message latency from producing a message to availability to consumers.</p>"},{"location":"kafka/02-why-kafka/#platform-features","title":"Platform Features","text":"<p>The core Apache Kafka project has also added some streaming platform features that can make it much easier for developers to perform common types of work.</p> <p><code>Kafka Connect</code> assists with the task of pulling data from a source data system and pushing it into Kafka, or pulling data from Kafka and pushing it into a sink data system.</p> <p><code>Kafka Streams</code> provides a library for easily developing stream processing applications that are scalable and fault tolerant.</p>"},{"location":"kafka/02-why-kafka/#references","title":"References","text":"<ul> <li>YT - System Design: Why is Kafka fast?<ul> <li>Why is Kafka fast?</li> </ul> </li> <li>How Kafka Is so Performant If It Writes to Disk?</li> <li>Maximizing Efficiency Official doc</li> </ul>"},{"location":"kafka/03-kafka-use-cases/","title":"Kafka Use Cases","text":""},{"location":"kafka/03-kafka-use-cases/#activity-tracking","title":"Activity Tracking","text":"<p>Originally Kafka was designed at LinkedIn for user activity tracking. The messages are published to one or more topics, which are then consumed by applications on the backend. These applications may be generating reports, feeding machine learning systems, updating search results, or performing other operations that are necessary to provide a rich user experience.</p>"},{"location":"kafka/03-kafka-use-cases/#messaging","title":"Messaging","text":"<p>Case where applications need to send notifications (such as emails) to users. Those applications can produce messages without needing to be concerned about formatting or how the messages will actually be sent. A single application can then read all the messages to be sent and handle them consistently, which avoids the need to duplicate functionality in multiple applications, as well as allows operations like aggregation that would not otherwise be possible.</p>"},{"location":"kafka/03-kafka-use-cases/#metrics-and-logging","title":"Metrics and Logging","text":"<p>This is a use case in which the ability to have multiple applications producing the same type of message shines. Applications publish metrics on a regular basis to a Kafka topic, and those metrics can be consumed by systems for monitoring and alerting. Log messages can be published in the same way and can be routed to dedicated log search systems like Elasticsearch or security analysis applications.</p>"},{"location":"kafka/03-kafka-use-cases/#commit-log","title":"Commit Log","text":"<p>Database changes can be published to Kafka, and applications can easily monitor this stream to receive live updates as they happen. This changelog stream can also be used for replicating database updates to a remote system, or for consolidating changes from multiple applications into a single database view. Durable retention is useful here for providing a buffer for the changelog, meaning it can be replayed in the event of a failure of the consuming applications. Alternately, log-compacted topics can be used to provide longer retention by only retaining a single change per key.</p>"},{"location":"kafka/03-kafka-use-cases/#stream-processing","title":"Stream Processing","text":"<p>While almost all usage of Kafka can be thought of as stream processing, the term is typically used to refer to applications that provide similar functionality to map/reduce processing in Hadoop.</p> <p>Hadoop usually relies on aggregation of data over a long time frame, either hours or days. Stream processing operates on data in real time, as quickly as messages are produced. Stream frameworks allow users to write small applications to operate on Kafka messages, performing tasks such as counting metrics, partitioning messages for efficient processing by other applications, or transforming messages using data from multiple sources.</p>"},{"location":"kafka/05-need-for-zookeeper/","title":"Need for ZooKeeper","text":"<p>Writes to ZooKeeper are only performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. This amount of traffic is generally minimal, and it does not justify the use of a dedicated ZooKeeper ensemble for a single Kafka cluster.</p> <p>Dependency on ZooKeeper is shrinking over time. Administration tools now connect directly to the cluster and have deprecated the need to connect to ZooKeeper directly for operations.</p> <p>ZooKeeper-less Kafka is not production-ready yet.</p>"},{"location":"kafka/05-need-for-zookeeper/#other-references","title":"Other References","text":"<ul> <li>Confluent Kafka without ZooKeeper</li> <li>Hevodata Kafka without ZooKeeper</li> </ul>"},{"location":"kafka/06-producers/","title":"Kafka Producers","text":"<p>Kafka ships with <code>built-in client</code> APIs that applications interact with. In addition to the client, Kafka has a <code>binary wire protocol</code>, which enables applications to read/write messages from/to Kafka by sending the correct byte sequences to Kafka's network port. Third-party clients also implement the wire protocol in language of choice.</p> <p>Regardless of the business use-case, producer is the one that sends message to Kafka, as desired. And each use-case may have diverse set of requirements:</p> <ul> <li>Is every message critical, or can we tolerate loss of messages?</li> <li>Are we OK with accidentally duplicating messages?</li> <li>Are there any strict latency or throughput requirements we need to support?</li> </ul> <p>Example, it would never be ok to accidentally duplicate a credit card transaction or tolerate its loss. On the other side, link click information for a website may tolerate some percentage of losses or duplicates, without worrying about latency.</p>"},{"location":"kafka/06-producers/#high-level-overview","title":"High-level Overview","text":"<ol> <li>The producer accepts a ProducerRecord object. Once we send the <code>ProducerRecord</code>, producer will serialize key-value to byte arrays so they can be sent over the network.</li> <li>If no partition is specified, the data is sent to the <code>Partitioner</code>, usually based on the key.</li> <li>Now that the producer knows which topic and partition the message will go to. It adds the message to a <code>batch of records</code> for that specific partition.</li> <li>A <code>separate thread</code> is responsible for sending these batches of records to the appropriate Kafka broker.</li> <li>On receiving a message, the broker sends back a response.</li> <li>On <code>success</code> - returns <code>RecordMetadata</code> object with the topic, partition, and the offset of the record within the partition</li> <li>On <code>failure</code> - returns an error to the producer. The producer may retry a few times, depending on the configruation, before giving up and returning error to the application.</li> </ol>"},{"location":"kafka/06-producers/#configuration","title":"Configuration","text":"<ul> <li><code>bootstrap.server</code> - a comma-separated list of brokers (<code>host:pair</code>) to establish the <code>initial connection</code> to the Kafka cluster. The list doesn't need to include all the brokers, since the producer gets all the information from the connection. But it is recommended to have at least 2 to avoid a single point of failure.</li> <li><code>key.serializer</code> - name of the class (one that implements <code>org.apache.kafka.common.serialization.Serializer</code> interface) to convert the key to byte array. This value is required even if key is optional (use <code>VoidSerializer</code> in that case).</li> <li><code>value.serializer</code> - Same as for key, but applying on value</li> </ul> <p>For more configurations, refer here</p>"},{"location":"kafka/06-producers/#primary-methods-of-sending-message","title":"Primary methods of sending message","text":"<ol> <li><code>Fire and Forget</code> - send a message to the server and not care if it arrived successfully or not. Producer still retries in case of failure, however the message can get lost in case of nonretriable errors or timeouts, and there is no way for the application to log error/exception</li> <li><code>Synchronous send</code> - technically, the producer is always async, however when we <code>send()</code> the message, we use <code>get()</code> to wait for the promise to be resolved before sending the next record.</li> <li><code>Asynchronous send</code> - we call the <code>send()</code> method with a <code>callback</code> function to capture the response asynchronously</li> </ol> <p>Note: a producer object can be used by single thread as well as multiple threads to send messages.</p> <p>When the producer object sends <code>ProducerRecord</code> object using <code>send()</code> message, the message will be placed in a buffer and will be sent to the broker in a separate thread. The send() method returns a Java <code>Future</code> object with <code>RecordMetadata</code>, but since we simply ignore the returned value, we have no way of knowing whether the message was sent successfully or not.</p> <p>Producer can also encounter errors before sending the message to Kafka, like <code>SerializationException</code>, <code>BufferExhaustedException</code> or <code>TimeoutException</code> when buffer is full, <code>InterruptException</code> if the sending thread was interrupted. </p>"},{"location":"kafka/06-producers/#synchronous-way","title":"Synchronous way","text":"<p>The main trade-off involved in sending message synchronously is performance. Brokers can take anywhere from 2ms to a few seconds to respond to producer requests. When sending messages synchronously, the sending thread will spend this time waiting and doing nothing else, not even sending additional messages. This leads to very poor performance (not preferred).</p>"},{"location":"kafka/06-producers/#asynchronous-way","title":"Asynchronous way","text":"<p>Suppose the network round-trip time between our application and the Kafka cluster is 10 ms. If we wait for a reply after sending each message, sending 100 messages will take around 1 second. On the other hand, if we just send all our messages and not wait for any replies, then sending 100 messages will barely take any time at all.</p>"},{"location":"kafka/06-producers/#errors","title":"Errors","text":"<p>Kafka producer has two types of errors.</p>"},{"location":"kafka/06-producers/#retriable-errors","title":"Retriable Errors","text":"<p>Retriable errors are those that can be resolved by sending the message again. The producer can be configured to retry for those erros automatically.</p> <p>For example, a connection error can be resolved because the connection may get reestablished. A \u201cnot leader for partition\u201d error can be resolved when a new leader is elected for the partition and the client metadata is refreshed.</p>"},{"location":"kafka/06-producers/#non-retriable","title":"Non-Retriable","text":"<p>Some errors cannot be resolved by retrying. In such cases, the producer will not attempt a retry and return the exception immediately.</p> <p>For example, \"Message size too large.\"</p>"},{"location":"kafka/06-producers/#configuring-producers","title":"Configuring Producers","text":"<p>Many configurations have reasonable defaults. Some of the parameters have significant impact on memory usage, performance, and reliability of the producers. Hence, setting them manually based on application needs can be extremely helpful.</p>"},{"location":"kafka/06-producers/#required-parameters","title":"Required Parameters","text":"<ul> <li>bootstrap servers</li> <li>serializers</li> </ul>"},{"location":"kafka/06-producers/#performance-parameters","title":"Performance Parameters","text":"<p><code>client.id</code> is a unique logical identifier for the client, used by the brokers to identify messages sent from the client. It is used in logging, metrics and quotas.</p> <p><code>acks</code> controls how many partition replicas must receive the record before the producer can consider the write successful (default is async). This option has significant impact on the durability of written messages. Choosing acks really means you trade off between reliability and producer latency.</p> <ul> <li>acks=0    - No acknowledgement from the broker. Fast, but messages can get lost if something goes wrong with the broker. Producer can send messages as fast as the network can support.</li> <li>acks=1    - Waits for acknowledgement from the leader broker. Producer retries <code>X times</code> on failure, <code>avoiding potential</code> data loss. Message can still get lost if leader crashes and last messages were not replicated before that.</li> <li>acks=all  - Safest mode. Waits for acknowledgement from the leader broker and all replicas. Latency keeps increasing as the number of acks keep increasing.</li> </ul> <p>Note: <code>end-to-end latency</code> is measured from the time a record was produced until it is available for consumers to read and is identical for all three options. The reason is that, in order to maintain consistency, Kafka will not allow consumers to read records until they are written to all in sync replicas. Therefore, if you care about end-to-end latency, rather than just the producer latency, <code>there is no trade-off to make</code>.</p>"},{"location":"kafka/06-producers/#instantiating-producer","title":"Instantiating Producer","text":"<p>Once we instantiate a producer instance: 1. can we create more than one instance in an application? 2. what are the downsides if any, or things to watch for? 3. Can I have multiple producer instances with different serializers? 4. Retriable and Nonretriable errors and timeouts 5. Why do we need to wait for <code>get()</code>, and not use the future object RecordMetadata.</p>"},{"location":"kafka/06-producers/#tbd-index","title":"TBD Index","text":"<ul> <li>Create KafkaProducer</li> <li>Create ProducerRecord</li> <li>send records to Kafka</li> <li>Handle errors that Kafka may return</li> <li>Configuration options to control producer behavior</li> <li>Partitioning methods</li> <li>Serializers</li> <li>Custom serializers and partitioners</li> </ul>"},{"location":"kafka/06-producers/#wire-protocol","title":"Wire Protocol","text":"<ul> <li>https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol</li> <li>https://kafka.apache.org/protocol.html</li> <li>https://en.wikipedia.org/wiki/Wire_protocol</li> </ul>"},{"location":"kafka/06-producers/#tooling","title":"Tooling","text":"<ul> <li>https://kafka.js.org/docs/getting-started</li> <li>https://github.com/SOHU-Co/kafka-node</li> <li>https://docs.confluent.io/platform/6.0.4/tutorials/examples/clients/docs/nodejs.html#consume-records</li> <li>https://www.npmjs.com/package/kafka-console</li> <li>https://github.com/strimzi/strimzi-kafka-bridge</li> </ul>"},{"location":"kafka/99-terms/","title":"Terms","text":""},{"location":"kafka/99-terms/#message","title":"Message","text":"<p>A <code>message</code>/<code>event</code> is a unit of data, basically an array of bytes with no meaning to the broker.</p> <p>A message can have an optional piece of metadata, called a <code>key</code> (also a byte array). Keys are useful when messages are to be written to topic-partitions in a more controlled manner, using consistent hashing or any other technique. This ensures that messages with the same key are always written into the same partition.</p>"},{"location":"kafka/99-terms/#schema","title":"Schema","text":"<p>It is recommended to impose structure/schema on message content for a consistent data format. JSON and XML are readable but lack type handling and compatibility between schema versions.</p> <p>Avro and Protobuf are top choices. <code>Avro</code> provides a compact serialization format, schema that are separate from the message payloads and do not require code to be generated when they change.</p>"},{"location":"kafka/99-terms/#topics-and-partitions","title":"Topics and Partitions","text":"<p>Messages are categorized into <code>topics</code>, like a db table. Topics are additionally broken down into a number of <code>partitions</code>.</p> <p>A partition is a single log. Messages are written into it in append-only mode, and are read in order from beginning to end. Hence there is no guarantee of ordering across the entire topic.</p> <p>Each partition may be stored on a different server, to help scale the topic\u2019s capacity beyond the ability of a single machine. Additionally partitions can be replicated, to handle cases when a server fails.</p> <p></p>"},{"location":"kafka/99-terms/#producers-and-consumers","title":"Producers and Consumers","text":"<p>These are the core clients of the broker.</p> <p><code>Producers</code> create new messages, to a specific <code>topic</code>. By default, producers will balance messages over all partitions of a topic evenly. To direct messages to specific partitions, producers use a message key and partitioner that will generate a hash of the key and map it to a specific partition. </p> <p><code>Consumers</code> read messages, in the order in which they were produced to each partition, by subscribing to one or more topics. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages.</p> <p><code>Offset</code> is another metadata that gets added to each message by the broker. It is an integer value that continually increases. Each message in a given partition has a unique offset, and in an increasing order. Consumers store the next possible offset for each partition, typically in the broker itself, so that the consumer can stop reading messages and restart without losing its place.</p> <p>Multiple instances of consumers can work together to consume a topic. This is important because a topic can have many partitions, which makes the message in-flow rate much much higher, and a single consumer instance may not be able to consume all these messages at the same rate.</p> <p>This is why every consumer is part of a <code>Consumer Group</code>. The group ensures that each partition is consumed by <code>only one consumer</code> instance.</p> <p></p> <p>This way consumers can horizontally scale to consume topics with a large number of messages, across several partitions.</p> <p>If a single consumer fails, the remaining members of the group will reassign the partitions to cover for the missing consumer instance.</p> <p>Note: This does not change the existing partition assignment of a healthy consumer.</p>"},{"location":"kafka/99-terms/#advanced-clients","title":"Advanced Clients","text":"<p>There are other advanced clients that use producers and consumers as a building block:</p> <ul> <li>Kafka connect API for data integration</li> <li>Kafka streams for stream processing</li> </ul>"},{"location":"kafka/99-terms/#stream","title":"Stream","text":"<p>Most often a stream is considered to be a single topic of data, regardless of the number of partitions. Look up Kafka Streams, Apache Samza, and Apache Storm for stream processing frameworks.</p>"},{"location":"kafka/99-terms/#brokers-and-clusters","title":"Brokers and Clusters","text":"<p>Each kafka server is called a <code>broker</code>, designed to operate as part of a cluster. The broker mainly:</p> <ol> <li>Receives messages from producers, assigns offsets to them, and writes the messages to storage on disk.</li> <li>Services consumers, responding to fetch requests for partitions, with the published messages.</li> </ol> <p>Depending on the hardware, a single broker can be capable of handling thousands of partitions and millions of messages per second.</p>"},{"location":"kafka/99-terms/#cluster-leader","title":"Cluster Leader","text":"<p>Within a cluster of brokers, one broker will also function as the leader, elected automatically from the live members of the cluster. The leader is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures.</p>"},{"location":"kafka/99-terms/#replication","title":"Replication","text":"<p>A partition is owned by a single broker in the cluster, while replicated partitions are assigned to additional brokers. Replication provides redundancy of messages in the partition, such that one of the followers can take over leadership if there is a broker failure. All producers must connect to the leader in order to publish messages, but consumers may fetch from either the leader or one of the followers.</p> <p></p>"},{"location":"kafka/99-terms/#data-retention","title":"Data Retention","text":"<p>Retention is durable storage of messages for some period of time. When not using default retention settings, the broker can be configured for either retaining messages for a time period (like a TTL) or until the partition reaches a certain size in bytes. Any data outside this window gets deleted automatically.</p> <p>The retention setting can be applied as fine as individual topics.</p> <p>Topics can also be configured as <code>log compacted</code>, which retains only the last message produced with a specific key. This can be helpful where only the latest update is valuable.</p>"},{"location":"kafka/99-terms/#multiple-clusters","title":"Multiple Clusters","text":"<p>Reasons why it can be useful to have multiple clusters:</p> <ul> <li>Segregation of types of data</li> <li>Isolation for security requirements</li> <li>Multiple datacenters (disaster recovery)</li> </ul>"},{"location":"kafka/99-terms/#mirrormaker","title":"MirrorMaker","text":"<p>When working with multiple datacenters, it is often required that messages between them stays in sync. The replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters.</p> <p>A tool called <code>MirrorMaker</code> is used for replicating data to other clusters. It is simply a Kafka consumer and producer, linked together with a queue.</p> <p></p>"},{"location":"kafka/configuration-management-kafka/","title":"Configuration Management - Kafka","text":"<p>Apache Kafka, a distributed streaming platform, manages configuration through several mechanisms. Configurations in Kafka are crucial for fine-tuning various aspects of the system to meet specific requirements. Here are the key components of Kafka's configuration management:</p> <ol> <li>Server Properties:</li> <li>Kafka maintains server properties in a configuration file usually named <code>server.properties</code>. This file contains settings related to the Kafka broker, such as port numbers, data directories, replication factors, and more.</li> <li> <p>Administrators can modify these properties to tailor the behavior of the Kafka broker to their specific needs.</p> </li> <li> <p>Dynamic Broker Configurations:</p> </li> <li>Kafka supports dynamic configuration updates for certain properties without requiring a broker restart. This is achieved through the Kafka Broker API.</li> <li> <p>Admins can use tools like <code>kafka-configs.sh</code> to update configurations dynamically. For example:      <code>bash      bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type brokers --entity-name 1 --add-config max.connections=1000</code></p> </li> <li> <p>Topic-Level Configurations:</p> </li> <li>Kafka allows configuring individual topics with specific properties. These configurations include parameters like retention policies, replication factors, and segment size.</li> <li> <p>Topic-level configurations can be set at the time of topic creation or modified later using the Kafka topic command or the AdminClient API.</p> </li> <li> <p>Consumer and Producer Configurations:</p> </li> <li>Producers and consumers in Kafka have their configurations, specifying various parameters related to message production and consumption.</li> <li> <p>Producers and consumers can set their configurations programmatically or through properties files. For example, consumers can specify properties like <code>group.id</code>, <code>auto.offset.reset</code>, etc.</p> </li> <li> <p>ZooKeeper for Metadata Storage:</p> </li> <li>Kafka relies on Apache ZooKeeper to manage metadata, including configuration information. ZooKeeper maintains information about brokers, topics, and partitions.</li> <li> <p>Changes to the Kafka cluster, such as the addition or removal of brokers, trigger updates in ZooKeeper, which are then reflected in Kafka's configuration.</p> </li> <li> <p>KRaft Mode:</p> </li> <li>With the introduction of the KRaft mode in Kafka, there are improvements in terms of configuration and consensus.</li> <li> <p>KRaft uses an ensemble of Raft consensus groups, and configuration changes can be made using the AdminClient API, making it more straightforward and consistent.</p> </li> <li> <p>Security Configurations:</p> </li> <li>Kafka provides extensive security configurations to control access to the system and secure data transmission. Security-related settings, such as SSL/TLS configurations and authentication mechanisms, are an essential part of Kafka's configuration management.</li> </ol> <p>Overall, Kafka's configuration management system is flexible and allows operators to customize the behavior of the Kafka cluster based on their specific requirements. It combines static configurations in property files with dynamic updates and leverages ZooKeeper for distributed coordination and metadata storage.</p>"},{"location":"kafka/configuration-management-kafka/#learn-more","title":"Learn more","text":"<p>To learn more about Kafka's configuration management and gain a deeper understanding of Apache Kafka in general, you can explore various resources. Here are some recommended materials:</p> <ol> <li>Official Documentation:</li> <li>The official documentation for Apache Kafka is an excellent starting point. It provides comprehensive information about Kafka's configuration options, usage, and best practices.</li> <li> <p>Apache Kafka Documentation</p> </li> <li> <p>Books:</p> </li> <li>\"Kafka: The Definitive Guide\" by Neha Narkhede, Gwen Shapira, and Todd Palino is a comprehensive resource that covers Kafka architecture, configuration, and usage.</li> <li> <p>Kafka: The Definitive Guide</p> </li> <li> <p>Online Courses:</p> </li> <li>Platforms like Coursera and LinkedIn Learning offer courses on Apache Kafka that cover various aspects, including configuration management.</li> <li> <p>Search for courses such as \"Apache Kafka essentials\" or \"Kafka administration\" on these platforms.</p> </li> <li> <p>Blogs and Articles:</p> </li> <li>Reading blogs and articles from experts in the field can provide practical insights and tips for Kafka configuration. Platforms like Medium, Towards Data Science, and the Confluent Blog often feature Kafka-related content.</li> <li> <p>Search for Kafka configuration tips, best practices, or specific use cases.</p> </li> <li> <p>Community Forums:</p> </li> <li>Engaging with the Kafka community can be valuable. Platforms like the Apache Kafka mailing list and Stack Overflow are places where you can ask questions and learn from experienced Kafka users.</li> <li> <p>Apache Kafka Mailing List</p> </li> <li> <p>Webinars and Conferences:</p> </li> <li>Keep an eye out for webinars and conference talks related to Kafka. Confluent, the company founded by the creators of Kafka, often hosts webinars covering various Kafka-related topics.</li> <li> <p>Check the event sections on the official Kafka and Confluent websites.</p> </li> <li> <p>GitHub Repositories:</p> </li> <li>Exploring Kafka-related repositories on GitHub, especially those related to configuration management tools or utilities, can provide practical examples.</li> <li>Search for repositories related to Kafka configuration, Kafka tools, or Kafka management.</li> </ol> <p>Remember to complement your theoretical knowledge with hands-on experience. Setting up a Kafka cluster, experimenting with different configurations, and observing the behavior will deepen your understanding of Kafka's configuration management.</p>"},{"location":"kafka/monitoring/","title":"Kafka Monitoring","text":"<p>It all comes down to:</p> <ol> <li>Do I know which metrics to monitor?</li> <li>Do I know knobs to turn if I need to tune things relative to each of these performance metrics?</li> </ol> <p>To get these answers, here is what you do:</p> <ol> <li>Monitor and observe Kafka performance, throughput and latency</li> <li>Automatically detect and alert on Kafka issues maintaining data integrity</li> <li>Automatically detect threshold, component, hardware degradation and failures</li> <li>Forecast Kafka performance and capacity trends and needs over time.</li> </ol>"},{"location":"kafka/monitoring/#key-metrics","title":"Key Metrics","text":""},{"location":"kafka/monitoring/#the-big-4","title":"The Big 4","text":"<ol> <li>Number of active controllers should always be 1</li> <li>Number of under replicated partitions should always be 0</li> <li>Number of offline partitions should always be 0</li> <li>Consumer lag should be under control (varies by use-case)</li> </ol>"},{"location":"kafka/monitoring/#producer","title":"Producer","text":"<p><code>Production rate</code> - when a message leaves a producer, it is typically not on its own. It's been batched with other messages. Production rate is about:</p> <ul> <li>how big is that batch size?</li> <li>how long is it buffered on the producer before being sent?</li> <li>what's the network latency between the producer and the broker?</li> <li>what's the throughput from producer to the broker?</li> <li>were there any failures?</li> <li>how often are you acknowledging those packets that were sent?</li> </ul> <p>All these are a potential gating factor in getting the message(s) from the producer over to the broker.</p>"},{"location":"kafka/monitoring/#broker","title":"Broker","text":"<ul> <li>Component health (topics and hardware)</li> <li>Load skew</li> <li>Capacity</li> <li>how many leaders per broker am I actually running?</li> </ul>"},{"location":"kafka/monitoring/#topic","title":"Topic","text":"<ul> <li>is partition healthy?</li> <li>are we fully replicated?</li> <li>are we evenly distributed among the hardware we have? (load skew)</li> <li>are topic priorities set based on most important topic (if applicable)?</li> </ul>"},{"location":"kafka/monitoring/#consumer","title":"Consumer","text":"<ul> <li>are the consumers online?</li> <li>is there consumer lag?</li> <li>consumption rate &amp; trends</li> </ul>"},{"location":"kafka/monitoring/#beyond-the-obvious","title":"Beyond the obvious","text":"<ul> <li>Log flush latency</li> <li>Messages per second / bytes per second thresholds</li> <li>Available network processor / request handler bandwidth</li> <li>Topic status metwork throughput</li> <li>Open file handles</li> <li>Memory, Load</li> <li>Disk usage</li> <li>GC pauses</li> <li>Heap usage</li> <li>Swapping</li> <li>Dropped packets</li> </ul>"},{"location":"kafka/monitoring/#trends-to-watch","title":"Trends to watch","text":"<ul> <li>Rate of topic growth</li> <li>Is TTL on data (retention) long enough for data safety margins, but not too long?</li> <li>Is the hardware keeping up with Kafka? - (CPU, Memory, Network and total I/O capcity)</li> </ul>"},{"location":"kafka/monitoring/#references","title":"References","text":"<ul> <li>Kafka uses <code>Yammer Metrics</code> for metrics reporting in the server. For more on monitoring, refer kafka official documentation.</li> <li>AWS MSK metrics details</li> </ul>"},{"location":"kafka/todo/","title":"Review First","text":""},{"location":"kafka/todo/#message-bus","title":"Message Bus","text":"<ul> <li>https://www.inngest.com/blog/message-bus-vs-queues</li> <li>https://stackoverflow.com/questions/7793927/message-queue-vs-message-bus-what-are-the-differences</li> <li>https://ardalis.com/bus-or-queue/</li> <li>https://dreamix.eu/blog/dreamix/message-queue-vs-message-broker-whats-the-difference</li> </ul>"},{"location":"kafka/todo/#kafka","title":"Kafka","text":"<ul> <li>https://www.linkedin.com/pulse/kafka-action-part-5-consumers-advanced-config-saahas-kulkarni/</li> </ul>"},{"location":"kafka/todo/#http","title":"Http","text":""},{"location":"kafka/todo/#http-headers","title":"Http Headers","text":"<ul> <li>Http Headers</li> <li>Request Headers</li> <li>Response Headers</li> <li>Representational Headers</li> <li>MIME type</li> <li>Payload Headers</li> </ul>"},{"location":"kafka/todo/#http-response-codes","title":"Http Response codes","text":"<ul> <li>Http Response Status codes</li> </ul>"},{"location":"kafka/todo/#tools","title":"Tools","text":"<ul> <li>Postman</li> <li>Charles Proxy</li> <li>Flipper</li> </ul>"},{"location":"kafka/todo/#from-work","title":"from WORK","text":"<ul> <li>Android Learning Session #1 presentation</li> </ul>"},{"location":"kafka/tuning-kafka/","title":"Kafka Performance Tuning","text":"<p>You don't tune Kafka. You tune the Kafka components.</p> <p>Your first decision point is - am tuning for <code>Throughput</code> or <code>Latency</code>?</p> <p>TODO: This question needs to be explained since at first it isn't making sense.</p>"},{"location":"kafka/tuning-kafka/#common-tuning-knobs","title":"Common Tuning Knobs","text":"<p>Each knob you turn will give and take something.</p>"},{"location":"kafka/tuning-kafka/#producers","title":"Producers","text":"<ul> <li>Acknowledgements, Buffers, Batches<ul> <li>Speed vs Data Integrity</li> </ul> </li> </ul>"},{"location":"kafka/tuning-kafka/#brokers","title":"Brokers","text":"<ul> <li>How many nodes and node architecture<ul> <li>Speed and Data Integrity vs Cost and Management burden</li> <li>Budgets and SLAs</li> </ul> </li> <li>Replication factor</li> <li>Block sizes (of storage)</li> <li>Network thread counts (per CPU)</li> </ul>"},{"location":"kafka/tuning-kafka/#topics","title":"Topics","text":"<ul> <li>Number of partitions per node (sweet spot)<ul> <li>Speed and Data Integrity vs Cost and Management burden</li> <li>Business priority</li> <li>Architectural decisions</li> </ul> </li> </ul>"},{"location":"kafka/tuning-kafka/#consumers","title":"Consumers","text":"<ul> <li>Commit batching</li> <li>Parallelism<ul> <li>Speed and Data Integrity vs Cost and Management burden</li> <li>Complexity</li> <li>Monitoring</li> </ul> </li> <li>Pausing (producers) - when the consuming app is down</li> </ul>"},{"location":"kafka/tuning-kafka/#references","title":"References","text":"<ul> <li>YT | Best Practices for Monitoring and Improving Kafka Performance</li> </ul>"},{"location":"kafka/Ch%2001%20Background/","title":"Background","text":"<p>Kafka as a queue, message bus, vs data storage platform.</p>"},{"location":"kafka/Ch%2001%20Background/#topics","title":"Topics","text":"<ul> <li>Buffer</li> <li>Message Bus</li> <li>Message Queue</li> <li>Pub/Sub</li> <li>Stream</li> </ul>"},{"location":"kafka/Ch%2001%20Background/#references","title":"References","text":"<ul> <li>SQS - Getting started</li> <li>SNS - Getting started</li> <li>YT AWS SQS Overview For Beginners - Be a Better Dev</li> <li>YT AWS SQS vs SNS vs EventBridge - When to Use What? - Be a Better Dev</li> <li>YT SNS And SQS Deep Dive | SNS Vs SQS | Standard Vs FIFO | Use Cases - Cloud with Raj</li> <li>YT What is a Message Queue and When should you use - Hussein Nasser</li> <li>YT Publish-Subscribe Pattern vs Message Queues vs Request Response - Hussein Nasser</li> <li>YT Doordash moves their Backend to Apache Kafka from RabbitMQ - Hussein Nasser</li> <li>YT Event Driven Architectures vs Workflows (with AWS Services!) - Be a Better Dev</li> <li>YT What is a Message Queue and Where is it used? - Gaurav Sen</li> <li>YT Cross Account AWS SNS to SQS Subscription Tutorial - Be a Better Dev</li> <li>YT  - Be a Better Dev</li> </ul>"},{"location":"kafka/Ch%2001%20Background/async-operation/","title":"Async Operation","text":"<p>Fact: true asynchrony must be provided at the OS (Operating System) level.</p> <ul> <li>CPU bound operations are inhenrently synchronous, hence they generally use Threading to support parallel operations. A thread can also pretend to be async by offloading it's taks to another thread.</li> <li>I/O bound operations generally use Asynchronous I/O</li> </ul>"},{"location":"kafka/Ch%2001%20Background/async-operation/#state-machine","title":"State Machine","text":"<p>Refer State Machine Design in C++</p>"},{"location":"kafka/Ch%2001%20Background/async-operation/#os-level-asynchrony","title":"OS level Asynchrony","text":"<p>TBD</p>"},{"location":"kafka/Ch%2001%20Background/async-operation/#application-level-aysnchrony","title":"Application level Aysnchrony","text":"<p>Let's take an example of JavaScript's <code>async/await</code>, which is a compiler-generated <code>state machine</code>, that runs async functions, but relies on OS primitives (<code>interrupts</code>, <code>threads</code>) to acutally perform a task in async manner.</p> <p>Often there won't be a new thread created to handle async operations. On Windows, the primary mechanism for performing async work is I/O Completion Ports - a Windows API, used under the hood even by the <code>HttpClient</code>.</p> <p>For non-I/O operations, we can always use Thread and ThreadPool API, to perform background work that will complete asynchronously.</p>"},{"location":"kafka/Ch%2001%20Background/async-operation/#how-async-works","title":"How Async works?","text":"<p>TBD</p> <pre><code>Making a method `async` doesn't meant it will spawn a thread. I/O will not use a thread, unless intentionally created. This is managed at OS level.\nGeneral use of async is to help with I/O bound processes, not CPU bound. I/O has callback operated interfaces at the lowest level.\n\nInternally the compiler makes a state machine, which it uses to pause/await/resume executing code.\nThe method basically turns into a state engine relinquishing control to the task scheduler waiting for a completed task being signalled.\n</code></pre> <p>There are no free lunches. To save on waiting on this network round-trip, the producer application has a <code>operation-stack or something</code> waiting to listen on the response, whenever it comes back. This means that part resources on the consumers will be occupied till that stack clears up, or for always serving some.</p>"},{"location":"kafka/Ch%2001%20Background/async-operation/#references","title":"References","text":"<ul> <li>There is no thread blog by Stephen Cleary</li> <li>Task.Run Etiquette and Proper Usage</li> <li>Task.Run Etiquette Examples</li> <li>Regarding <code>deadlocks</code> blog by Stephen Cleary</li> <li>Parallel Computing - SynchronizationContext</li> <li>Async and Await</li> <li>Don't Block on Async Code</li> <li>Don't Block in Asynchronous Code</li> <li>Regarding <code>Task</code> blogs by Stephen Cleary</li> <li>A Tour of Task, Part 0: Overview series</li> <li>Task.Run vs BackgroundWorker: Intro series</li> <li>Cancellation, Part 1: Overview series</li> <li>Asynchronous Messaging, Part 1: Basic Distributed Architecture series</li> <li>BackgroundService Gotcha: Startup series</li> <li>Talks by Stephen Cleary Search</li> <li>asyncfx</li> <li>Introduction to Async Streams</li> <li>Testing async code</li> <li>Stephen Toub</li> <li>An Introduction to System.Threading.Channels</li> <li>Asynchronous programming with async and await</li> <li>Multithreaded Asynchronous I/O &amp; I/O Completion Ports blog by Dr Dobbs</li> <li>https://stackoverflow.com/questions/40916697/must-async-methods-be-supported-by-os-or-is-async-program-level-feature</li> <li>.NET Parallel Programming blogs</li> <li>Stephen Toub blogs</li> </ul>"},{"location":"kafka/Ch%2001%20Background/event-bus/","title":"Event Bus","text":"<p>TBD</p>"},{"location":"kafka/Ch%2002%20Fundamentals/","title":"Fundamentals","text":"<p>TBD</p>"},{"location":"kafka/configuration/","title":"Kafka Configurations","text":"<ul> <li>Configuring Broker</li> <li>Configuring Topic</li> <li>Common configurations</li> </ul>"},{"location":"kafka/configuration/#references","title":"References","text":"<ul> <li>Official Kafka configurations</li> </ul>"},{"location":"kafka/configuration/00-configuring-the-cluster/","title":"Configuraing the Cluster","text":"<p>Currently, in a well-configured environment, it is recommended to not have more than 14,000 partition replicas per broker and 1 million replicas per cluster.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#number-of-brokers","title":"Number of Brokers","text":"<p>The size of the cluster will be bound on the following key areas:</p> <ul> <li>Disk capacity</li> <li>Replica capacity per broker</li> <li>CPU capacity</li> <li>Network capacity</li> </ul>"},{"location":"kafka/configuration/00-configuring-the-cluster/#disk-capacity","title":"Disk capacity","text":"<p>Consider how much disk capacity is required for retaining messages and how much storage is available on a single broker. If the cluster is required to retain 10 TB of data and a single broker can store 2 TB, then the minimum cluster size is 5 brokers.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#replica-capacity-per-broker","title":"Replica capacity per broker","text":"<p>Increasing the replication factor will increase the storage requirements by x factor.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#cpu-capacity","title":"CPU capacity","text":"<p>Consider the capacity of the cluster to handle requests. CPU usually is not a major bottleneck for most use cases, but it can be if there is an excessive amount of <code>client connections</code> and <code>requests</code> on a broker.</p> <p>Keep an eye on overall CPU based on total <code>unique clients</code> and <code>consumer groups</code>.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#network-capacity","title":"Network capacity","text":"<p>It is important to keep in mind the capacity of the network interfaces and whether they can handle the client traffic if there are multiple consumers of the data or if the traffic is not consistent over the retention period of the data.</p> <p>If the network interface on a single broker is used to 80% capacity at peak, and there are two consumers of that data, the consumers will not be able to keep up with peak traffic unless there are two brokers.</p> <p>If replication is being used in the cluster, this is an additional consumer of the data that must be taken into account.</p> <p>You may also want to scale out to more brokers in a cluster in order to handle performance concerns caused by lesser disk throughput or system memory available.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#broker-configuration","title":"Broker Configuration","text":"<p>Requirements in the broker configuration to allow multiple Kafka brokers to join a single cluster:</p> <ol> <li>all brokers must have the same configuration for the <code>zookeeper.connect</code> parameter</li> <li>all brokers in the cluster must have a unique value for the <code>broker.id</code> parameter</li> </ol>"},{"location":"kafka/configuration/00-configuring-the-cluster/#os-tuning","title":"OS Tuning","text":"<p>Configurations for <code>virtual memory</code> and <code>networking subsystems</code> are typically configured in the <code>/etc/sysctl.conf</code> file (refer to your Linux distribution regarding how to adjust the kernel configuration).</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#virtual-memory","title":"Virtual memory","text":"<p>In general, the Linux virtual memory system will automatically adjust itself for the system workload. We can make some adjustments to how swap space is handled, as well as to dirty memory pages, to tune these for Kafka\u2019s workload.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#swap-space","title":"Swap Space","text":"<p>Generally, it is best to avoid swapping at (almost) all costs. However Kafka makes heavy use of the system page cache, and if the VM system is swapping to disk, there is not enough memory being allocated to page cache.</p> <p>One way to avoid swapping is simply not to configure any swap space at all. Having swap is not a requirement, but it does provide a safety net if something catastrophic happens on the system. Having swap can prevent the OS from abruptly killing a process due to an out-of-memory condition. For this reason, the recommendation is to set the <code>vm.swappiness</code> parameter to a very low value, such as <code>1</code>. The parameter is a percentage of how likely the VM subsystem is to use swap space rather than dropping pages from the page cache. It is preferable to reduce the amount of memory available for the page cache.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#dirty-memory-pages","title":"Dirty Memory Pages","text":"<p>Kafka relies on disk I/O performance to provide good response times to producers. This is also the reason that the log segments are usually put on a fast disk. Set <code>vm.dirty_background_ratio</code> value lower than the default of 10. The value is a percentage of the total amount of system memory, and setting this value to 5 is appropriate in many situations.</p> <p>The total number of dirty pages allowed before the kernel forces synchronous operations to flush them to disk can also be increased by changing the value of <code>vm.dirty_ratio</code> to <code>above</code> the default of 20 (also a percentage of total system memory). Between 60 and 80 is a reasonable number. This setting does introduce a small amount of risk, both in regard to the amount of unflushed disk activity as well as the potential for long I/O pauses if synchronous flushes are forced.</p> <p>It is wise to review the number of dirty pages over time while the Kafka cluster is running under load.</p> <p>It is also recommended to set <code>vm.overcommit_memory</code> to <code>0</code>. Setting the default value of 0 indicates that the kernel determines the amount of free memory from an application. If the property is set to a value other than zero, it could lead the operating system to grab too much memory, depriving memory for Kafka to operate optimally. This is common for applications with high ingestion rates.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#disk","title":"Disk","text":"<p>Other than the configuration of RAID if it is used, the choice of filesystem for this disk can have the next largest impact on performance. Most common choices for local filesystems are either <code>Ext4</code> (fourth extended filesystem) or <code>XFS</code> (Extents File System). <code>XFS</code> also has better performance for Kafka\u2019s workload without requiring tuning beyond the automatic tuning performed by the filesystem. It is also more efficient when batching disk writes, all of which combine to give better overall I/O throughput.</p>"},{"location":"kafka/configuration/00-configuring-the-cluster/#networking","title":"Networking","text":"<p>The kernel is not tuned by default for large, high-speed data transfers. In fact, the recommended changes for Kafka are the same as those suggested for most web servers and other networking applications.</p> <p>The first adjustment is to change the default and maximum amount of memory allocated for the send and receive buffers for each socket. This will significantly increase performance for large transfers. A a reasonable setting is 128KiB. Keep in mind that the maximum size does not indicate that every socket will have this much buffer space allocated; it only allows up to that much if needed.</p> <p>In addition to the socket settings, the send and receive buffer sizes for TCP sockets must be set separately. A reasonable setting is 4 KiB minimum, 64 KiB default, and 2 MiB maximum buffer. Based on the actual workload of your Kafka brokers, you may want to increase the maximum sizes to allow for greater buffering of the network connections.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/","title":"Configure the Broker","text":"<p>The default configuration provided with the Kafka distribution is sufficient to run a standalone server as a proof of concept, but most likely will not be sufficient for large installations. There are numerous configuration options for Kafka that control all aspects of setup and tuning. Most of the options can be left at the default settings, though, as they deal with tuning aspects of the Kafka broker that will not be applicable until you have a specific use case that requires adjusting these settings.</p> <p>Configuration can be set via a properties file or even command line arguments. Configuration can be found in <code>/usr/local/kafka/config/server.properties</code>.</p> <p>There are several broker configuration parameters that should be reviewed when deploying Kafka for any environment other than a standalone broker on a single server.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#brokerid","title":"broker.id","text":"<p>Every Kafka broker must have an integer identifier, which is set using the configuration. By default, this integer is set to, but it can be any value. The selection of this number is technically arbitrary, however, it is highly recommended to set this value to something intrinsic to the host so that when performing maintenance it is not onerous to map broker ID numbers to hosts.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#listeners","title":"listeners","text":"<p>Older versions of Kafka used a simple <code>port</code> configuration, which started Kafka with a listener on TCP port 9092.</p> <p>The new <code>listeners</code> config is a comma-separated list of URIs that we listen on with the listener names. If the listener name is not a common security protocol, then another config <code>listener.security.protocol.map</code> must also be configured. A listener is defined as <code>&lt;protocol&gt;://&lt;hostname&gt;:&lt;port&gt;</code>.</p> <p>Specifying the hostname as <code>0.0.0.0</code> will bind to all interfaces. Leaving the hostname empty will bind it to the default interface. Keep in mind that if a port lower than 1024 is chosen, Kafka must be started as root. Running Kafka as root is not a recommended configuration.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#zookeeperconnect","title":"zookeeper.connect","text":"<p>The location of the ZooKeeper used for storing the broker metadata is set using the <code>zookeeper.connect</code> configuration parameter. The format for this parameter is a semicolon-separated list of <code>hostname:port/path</code> strings. Example value is <code>localhost:2181</code></p> <p><code>/path</code> - an optional path to use as a chroot environment for the Kafka cluster. If it is omitted, the root path is used.</p> <p>If a <code>chroot</code> path (a path designated to act as the root directory for a given application) is specified and does not exist, it will be created by the broker when it starts up.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#logdirs","title":"log.dirs","text":"<p>Kafka persists all messages to disk, and these log segments are stored in the directory specified in the <code>log.dir</code> configuration. For multiple directories, the config <code>log.dirs</code> is preferable. If this value is not set, it will default back to <code>log.dir</code>. <code>log.dirs</code> is a comma-separated list of paths on the local system. If more than one path is specified, the broker will store partitions on them in a \u201cleast-used\u201d fashion, with one partition\u2019s log segments stored within the same path. Note that the broker will place a new partition in the path that has the least number of partitions currently stored in it, not the least amount of disk space used, so an even distribution of data across multiple directories is not guaranteed.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#numrecoverythreadsperdatadir","title":"num.recovery.threads.per.data.dir","text":"<p>Kafka uses a configurable pool of threads for handling log segments. Currently, this thread pool is used:</p> <ul> <li>When starting normally, to open each partition\u2019s log segments</li> <li>When starting after a failure, to check and truncate each partition\u2019s log segments</li> <li>When shutting down, to cleanly close log segments</li> </ul> <p>By default, only one thread per log directory is used. As these threads are only used during startup and shutdown, it is reasonable to set a larger number of threads in order to parallelize operations. Specifically, when recovering from an unclean shutdown, this can mean the difference of several hours when restarting a broker with a large number of partitions!</p> <p>When setting this parameter, remember that the number configured is per log directory specified with <code>log.dirs</code>. This means that if <code>num.recovery.threads.per.data.dir</code> is set to 8, and there are 3 paths specified in <code>log.dirs</code>, this is a total of 24 threads.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#autocreatetopicsenable","title":"auto.create.topics.enable","text":"<p>The default Kafka configuration specifies that the broker should automatically create a topic under the following circumstances:</p> <ul> <li>When a producer starts writing messages to the topic</li> <li>When a consumer starts reading messages from the topic</li> <li>When any client requests metadata for the topic</li> </ul> <p>In many situations, this can be undesirable behavior, especially as there is no way to validate the existence of a topic through the Kafka protocol without causing it to be created. If you are managing topic creation explicitly, whether manually or through a provisioning system, you can set the <code>auto.create.topics.enable</code> configuration to <code>false</code>.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#autoleaderrebalanceenable","title":"auto.leader.rebalance.enable","text":"<p>In order to ensure a Kafka cluster doesn\u2019t become unbalanced by having all topic leadership on one broker, this config can be specified to ensure leadership is balanced as much as possible. It enables a background thread that checks the distribution of partitions at regular intervals (this interval is configurable via <code>leader.imbalance.check.interval.seconds</code>). If leadership imbalance exceeds another config, <code>leader.imbalance.per.broker.percentage</code>, then a rebalance of preferred leaders for partitions is started.</p>"},{"location":"kafka/configuration/01-configuring-the-broker/#deletetopicenable","title":"delete.topic.enable","text":"<p>Depending on your environment and data retention guidelines, you may wish to lock down a cluster to prevent arbitrary deletions of topics. Disabling topic deletion can be set by setting this flag to <code>false</code>.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/","title":"Configuring Topics","text":"<p>The Kafka server configuration specifies many default configurations for topics that are created, including partition counts and message retention, etc.</p> <p>The defaults in the server configuration should be set to baseline values that are appropriate for the majority of the topics in the cluster.</p> <p>Automatic topic creation is enabled by default</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#numpartitions","title":"num.partitions","text":"<p>Determines how many partitions a new topic is created with, defaults to <code>1</code> partition.</p> <p>The number of partitions for a topic can only be increased, never decreased.</p> <p>Partitions are the way a topic is scaled within a Kafka cluster, which makes it important to use partition counts that will balance the message load across the entire cluster as brokers are added. It is common to have the partition count for a topic be equal to, or a multiple of, the number of brokers in the cluster. This allows the partitions to be evenly distributed to the brokers, which will evenly distribute the message load.</p> <p>Refer Choosing Partitions for more details.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#defaultreplicationfactor","title":"default.replication.factor","text":"<p>Replication strategy can vary depending on the desired durability or availability of a cluster.</p> <p>Below is a recommendation that will prevent outages due to factors outside of Kafka\u2019s internal capabilities, such as hardware failures.</p> <p>Set the replication factor to at least 1 above the <code>min.insync.replicas</code> setting (abbreviated as RF++). RF++ will allow easier maintenance and prevent outages.</p> <p>For more fault-resistant settings, if you have large enough clusters and enough hardware, setting your replication factor to 2 above. This is to allow for one planned outage within the replica set and one unplanned outage to occur simultaneously. This would mean you\u2019d have a minimum of three replicas of every partition.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#logretentionms","title":"log.retention.ms","text":"<p>Configuration for how long Kafka will retain messages is by time.</p> <p>The default is specified in the configuration file using the <code>log.retention.hours</code> parameter, and it is set to 168 hours, or one week. However, there are two other parameters allowed, <code>log.retention.minutes</code> and <code>log.retention.ms</code>. All three of these control the same goal (the amount of time after which messages may be deleted), but the recommended parameter to use is <code>log.retention.ms</code>, as the smaller unit size will take precedence if more than one is specified.</p> <p>Retention by time is performed by examining the last modified time (mtime) on each log segment file on disk. Under normal cluster operations, this is the time that the log segment was closed, and represents the timestamp of the last message in the file.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#logretentionbytes","title":"log.retention.bytes","text":"<p>Another way to expire messages based on the total number of bytes of messages retained. Note that all retention is performed for individual <code>partitions</code>, not the topic. Setting the value to <code>\u20131</code> will allow for infinite retention.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#logsegmentbytes","title":"log.segment.bytes","text":"<p>The <code>log.retention.*</code> settings operate on log segments, not individual messages. As messages are produced to the Kafka broker, they are appended to the current <code>log segment for the partition</code>. Once the log segment has reached the size specified by the <code>log.segment.bytes</code> parameter, which defaults to <code>1 GB</code>, the log segment is closed and a new one is opened. Once a log segment has been closed, it can be considered for expiration. A smaller log segment size means that files must be closed and allocated more often, which reduces the overall efficiency of disk writes.</p> <p>Adjusting the size of the log segments can be important if topics have a low produce rate. For example, if a topic receives only 100 megabytes per day of messages, and <code>log.segment.bytes</code> is set to the default, it will take 10 days to fill one segment. As messages cannot be expired until the log segment is closed, if <code>log.retention.ms</code> is set to 604800000 (1 week), there will actually be up to 17 days of messages retained until the closed log segment expires. This is because once the log segment is closed with the current 10 days of messages, that log segment must be retained for 7 days before it expires based on the time policy (as the segment cannot be removed until the last message in the segment can be expired).</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#retrieving-offsets-by-timestamp","title":"Retrieving Offsets by Timestamp","text":"<p>The size of the log segment also affects the behavior of fetching offsets by timestamp. When requesting offsets for a partition at a specific timestamp, Kafka finds the log segment file that was being written at that time. It does this by using the creation and last modified time of the file, and looking for a file that was created before the timestamp specified and last modified after the timestamp. The offset at the beginning of that log segment (which is also the filename) is returned in the response.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#logrollms","title":"log.roll.ms","text":"<p>Another way to control when log segments are closed. This config specifies the amount of time after which a log segment should be closed.</p> <p><code>log.segment.bytes</code> and <code>log.roll.ms</code> are not mutually exclusive properties. Kafka will close a log segment either when the size limit is reached or when the time limit is reached, whichever comes first. By default, there is no setting for <code>log.roll.ms</code>, which results in only closing log segments by size.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#disk-performance-when-using-time-based-segments","title":"Disk performance when using time-based segments","text":"<p>When using a time-based log segment limit, it is important to consider the impact on disk performance when multiple log segments are closed simultaneously. This can happen when there are many partitions that never reach the size limit for log segments, as the clock for the time limit will start when the broker starts and will always execute at the same time for these low-volume partitions.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#mininsyncreplicas","title":"min.insync.replicas","text":"<p>When configuring your cluster for data durability, setting <code>min.insync.replicas</code> to 2 ensures that at least two replicas are caught up and <code>in sync</code> with the producer. This is used in tandem with setting the producer config to ack \u201call\u201d requests. This will ensure that at least two replicas (leader and one other) acknowledge a write for it to be successful. This can prevent data loss in scenarios where the leader acks a write, then suffers a failure and leadership is transferred to a replica that does not have a successful write. Without these durable settings, the producer would think it successfully produced, and the message(s) would be dropped on the floor and lost.</p> <p>However, configuring for higher durability has the side effect of being less efficient due to the extra overhead involved, so clusters with high-throughput that can tolerate occasional message loss <code>aren\u2019t recommended</code> to change this setting from the default of 1.</p>"},{"location":"kafka/configuration/02-configuring-the-topic/#messagemaxbytes","title":"message.max.bytes","text":"<p>This config limits the maximum size of a message that can be produced, which defaults to 1000000, or 1 MB. A producer that tries to send a message larger than this will receive an error back from the broker, and the message will not be accepted.</p> <p>This configuration deals with <code>compressed message size</code>, which means that producers can send messages that are much larger than this value uncompressed, provided they compress to under the configured size.</p> <p>There are noticeable performance impacts from increasing the allowable message size. Larger messages will mean that the broker threads that deal with processing network connections and requests will be working longer on each request. Larger messages also increase the size of disk writes, which will impact I/O throughput. Other storage solutions, such as blob stores and/or tiered storage, may be another method of addressing large disk write issues.</p>"},{"location":"kafka/configuration/03-common-configuration-settings/","title":"Common Configurations","text":""},{"location":"kafka/configuration/03-common-configuration-settings/#topic","title":"Topic","text":"Property Value default.replication.factor 3 log.retention.ms 604800000  // 1 week"},{"location":"kafka/configuration/03-common-configuration-settings/#broker","title":"Broker","text":"Property Value TBD TBD TBD TBD"},{"location":"kafka/configuration/03-common-configuration-settings/#configuring-retention-by-size-and-time","title":"Configuring retention by size and time","text":"<p>If both values <code>log.retention.ms</code> and <code>log.retention.bytes</code> are specified, messages may be removed when either criteria is met. For example, if <code>log.retention.ms</code> is set to 86400000 (1 day) and <code>log.retention.bytes</code> is set to 1000000000 (1 GB), it is possible for messages that are less than 1 day old to get deleted if the total volume of messages over the course of the day is greater than 1 GB. Conversely, if the volume is less than 1 GB, messages can be deleted after 1 day even if the total size of the partition is less than 1 GB. It is recommended, for simplicity, to choose either size-based or time-based retention and not both \u2014 to prevent surprises and unwanted data loss, but both can be used for more advanced configurations.</p>"},{"location":"kafka/configuration/03-common-configuration-settings/#coordinating-message-size-configurations","title":"Coordinating message size configurations","text":"<p>The message size configured on the Kafka broker must be coordinated with the <code>fetch.message.max.bytes</code> configuration on consumer clients. If this value is smaller than <code>message.max.bytes</code>, then consumers that encounter larger messages will fail to fetch those messages, resulting in a situation where the consumer gets stuck and cannot proceed. The same rule applies to the <code>replica.fetch.max.bytes</code> configuration on the brokers when configured in a cluster.</p>"},{"location":"kafka/how-to/","title":"How to?","text":"<ul> <li>Choose partitions</li> <li>Selecting hardware</li> <li>Performance</li> </ul>"},{"location":"kafka/how-to/choosing-partitions/","title":"How to choose # of partitions","text":"<p>You want many partitions, but not too many.</p>"},{"location":"kafka/how-to/choosing-partitions/#considerations","title":"Considerations","text":"<p>Several factors to consider:</p> <ul> <li>What is the throughput (in bytes) you expect to achieve for the <code>topic</code>?</li> <li>What is the maximum throughput you expect to achieve when consuming from a <code>single partition</code>? A partition will always be consumed completely by a single consumer (even when not using consumer groups, the consumer must read all messages in the partition). If you know that your slower consumer writes the data to a database and this database never handles more than 50 MBps from each thread writing to it, then you know you are limited to 50 MBps throughput when consuming from a partition.</li> <li>If you are sending messages to partitions based on keys, adding partitions later can be very challenging, so calculate throughput based on your expected future usage, not the current usage.</li> <li>Consider the number of partitions you will place on each broker and available diskspace and network bandwidth per broker.</li> <li>Avoid overestimating, as each partition uses memory and other resources on the broker and will increase the time for metadata updates and leadership transfers.</li> <li>Will you be mirroring data? You may need to consider the throughput of your mirroring configuration as well. Large partitions can become a bottleneck in many mirroring configurations.</li> <li>If you are using cloud services, do you have IOPS (input/output operations per second) limitations on your VMs or disks? There may be hard caps on the number of IOPS allowed depending on your cloud service and VM configuration that will cause you to hit quotas. Having too many partitions can have the side effect of increasing the amount of IOPS due to the parallelism involved.</li> </ul>"},{"location":"kafka/how-to/choosing-partitions/#example","title":"Example","text":"<p>If you have some estimate regarding the target throughput of the topic and the expected throughput of the consumers, you can divide the target throughput by the expected consumer throughput and derive the number of partitions this way. So if we want to be able to write and read 1 GBps from a topic, and we know each consumer can only process 50 MBps, then we know we need at least 20 partitions. This way, we can have 20 consumers reading from the topic and achieve 1 GBps.</p>"},{"location":"kafka/how-to/choosing-partitions/#what-if-i-have-no-metrics","title":"What if I have no metrics?","text":"<p>If you don\u2019t have this detailed information, limiting the size of the partition on the disk to less than 6 GB per day of retention often gives satisfactory results. Starting small and expanding as needed is easier than starting too large.</p>"},{"location":"kafka/how-to/performance/","title":"Performance Considerations","text":""},{"location":"kafka/how-to/performance/#write-efficiency","title":"Write Efficiency","text":"<p>For efficiency, messages are written to Kafka in batches, all of which are <code>produced to the same topic and partition</code>, saving network round trips.</p> <p>This is really a trade-off between latency and throughput: the larger the batch, the more messages can be handled per unit of time.</p> <p>Batches are typically compressed to provide more efficient data transfer and storage at the cost of some processing power.</p>"},{"location":"kafka/how-to/selecting-hardware/","title":"Selecting Hardware","text":"<p>Once performance becomes a concern, however, there are several factors that can contribute to the overall performance bottlenecks: disk throughput and capacity, memory, networking, and CPU. When scaling Kafka very large, there can also be constraints on the number of partitions that a single broker can handle due to the amount of metadata that needs to be updated. Once you have determined which performance types are the most critical for your environment, you can select an optimized hardware configuration appropriate for your budget.</p>"},{"location":"kafka/how-to/selecting-hardware/#disk-throughput","title":"Disk Throughput","text":"<p>The performance of producer clients will be most directly influenced by the throughput of the broker disk that is used for storing log segments. Kafka messages must be committed to local storage when they are produced, and most clients will wait until at least one broker has confirmed that messages have been committed before considering the send successful. This means that faster disk writes will equal lower produce latency.</p> <p>SSDs have drastically lower seek and access times and will provide the best performance.</p> <p>HDDs, on the other hand, are more economical and provide more capacity per unit. You can also improve the performance of HDDs by using more of them in a broker, whether by having multiple data directories or by setting up the drives in a redundant array of independent disks (RAID) configuration.</p> <p>Other factors, such as the specific drive technology (e.g., serial attached storage or serial ATA), as well as the quality of the drive controller, will affect throughput.</p> <p>Generally, observations show that HDD drives are typically more useful for clusters with very high storage needs but aren\u2019t accessed as often, while SSDs are better options if there is a very large number of client connections.</p>"},{"location":"kafka/how-to/selecting-hardware/#disk-capacity","title":"Disk Capacity","text":"<p>The amount of disk capacity that is needed is determined by how many messages need to be retained at any time.</p> <p>If the broker is expected to receive 1 TB of traffic each day, with 7 days of retention, then the broker will need a minimum of 7 TB of usable storage for log segments. You should also factor in at least 10% overhead for other files, in addition to any buffer that you wish to maintain for fluctuations in traffic or growth over time.</p> <p>Storage capacity is one of the factors to consider when sizing a Kafka cluster and determining when to expand it. The total traffic for a cluster can be balanced across the cluster by having multiple partitions per topic, which will allow additional brokers to augment the available capacity if the density on a single broker will not suffice. The decision on how much disk capacity is needed will also be informed by the replication strategy chosen for the cluster.</p>"},{"location":"kafka/how-to/selecting-hardware/#memory","title":"Memory","text":"<p>The messages the consumer is reading from a topic are optimally stored in the system\u2019s page cache, resulting in faster reads than if the broker has to reread the messages from disk. Therefore, having more memory available to the system for page cache will improve the performance of consumer clients.</p> <p>Kafka itself does not need much heap memory configured for the Java Virtual Machine (JVM). Even a broker that is handling 150,000 messages per second and a data rate of 200 megabits per second can run with a 5 GB heap. The rest of the system memory will be used by the page cache and will benefit Kafka by allowing the system to cache log segments in use.</p> <p>This is the main reason it is not recommended to have Kafka colocated on a system with any other significant application, as it will have to share the use of the page cache. This will decrease the consumer performance for Kafka.</p>"},{"location":"kafka/how-to/selecting-hardware/#networking","title":"Networking","text":"<p>The available network throughput will specify the maximum amount of traffic that Kafka can handle. This can be a governing factor, combined with disk storage, for cluster sizing. This is complicated by the inherent imbalance between inbound and outbound network usage that is created by Kafka\u2019s support for multiple consumers. A producer may write 1 MB per second for a given topic, but there could be any number of consumers that create a multiplier on the outbound network usage.</p> <p>Other operations, such as <code>cluster replication</code> and <code>mirroring</code>, will also increase requirements. Should the network interface become saturated, it is not uncommon for cluster replication to fall behind, which can leave the cluster in a vulnerable state. To prevent the network from being a major governing factor, it is recommended to run with at least <code>10 Gb NICs</code> (Network Interface Cards).</p>"},{"location":"kafka/how-to/selecting-hardware/#cpu","title":"CPU","text":"<p>Processing power is not as important as disk and memory until you begin to scale Kafka very large, but it will affect overall performance of the broker to some extent. Ideally, clients should compress messages to optimize network and disk usage. The Kafka broker must decompress all message batches, however, in order to validate the of the individual messages and assign offsets. It then needs to recompress the message batch in order to store it on disk. This is where most of Kafka\u2019s requirement for processing power comes from. This should not be the primary factor in selecting hardware, however, unless clusters become very large with hundreds of nodes and millions of partitions in a single cluster. At that point, selecting more performant CPU can help reduce cluster sizes.</p>"},{"location":"kafka/how-to/selecting-hardware/#kafka-in-aws-cloud","title":"Kafka in AWS Cloud","text":"<p>A good place to start on decisions is with the <code>amount of data retention</code> required, followed by the <code>performance needed from the producers</code>.</p> <p>If very low latency is necessary, I/O optimized instances utilizing local SSD storage might be required. Otherwise, ephemeral storage (such as the Amazon Elastic Block Store) might be sufficient.</p> <p>A common choice in AWS is either the <code>m4</code> or <code>r3</code> instance types. The <code>m4</code> will allow for greater retention periods, but the throughput to the disk will be less because it is on elastic block storage. The <code>r3</code> instance will have much better throughput with local SSD drives, but those drives will limit the amount of data that can be retained. For the best of both worlds, it may be necessary to move up to either the <code>i2</code> or <code>d2</code> instance types, but they are significantly more expensive.</p>"},{"location":"kafka/installation/","title":"Installation","text":""},{"location":"kafka/installation/#prerequisite","title":"Prerequisite","text":"<ul> <li>Java</li> <li>ZooKeeper</li> <li>Kafka</li> </ul>"},{"location":"kafka/installation/#java","title":"Java","text":"<p>Set up a functioning Java environment. The latest versions of Kafka support both Java 8 and Java 11.</p> <p>Though ZooKeeper and Kafka will work with a runtime edition of Java, it is recommended when developing tools and applications to have the full Java Development Kit (JDK).</p>"},{"location":"kafka/installation/01-apache-zookeeper/","title":"Apache ZooKeeper","text":"<p>Apache Kafka uses <code>Apache ZooKeeper</code> for storing metadata for the <code>cluster</code>, <code>brokers</code>, <code>topics</code>, <code>partitions</code>, as well as <code>consumer client</code> details. ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.</p> <p>While it is possible to run a ZooKeeper server using scripts contained in the Kafka distribution, it is trivial to install a full version of ZooKeeper from the distribution.</p> <p></p> <p>ZooKeeper comes with a base example config file that will work well for most use cases in <code>/usr/local/zookeeper/config/zoo_sample.cfg</code>.</p>"},{"location":"kafka/installation/01-apache-zookeeper/#manual-standalone-server","title":"Manual Standalone Server","text":"<p>If wanting to manually create a standalone server:</p> <pre><code># tar -zxf apache-zookeeper-x.x.x-bin.tar.gz\n# mv apache-zookeeper-x.x.x-bin /usr/local/zookeeper\n# mkdir -p /var/lib/zookeeper\n# cp &gt; /usr/local/zookeeper/conf/zoo.conf &lt;&lt; EOF\n&gt; tickTime=2000\n&gt; dataDir=/var/lib/zookeeper\n&gt; clientPort=2181\n&gt; EOF\n# export JAVA_HOME=/usr/java/jdk-xx.x.xx\n\n# /usr/local/zookeeper/bin/zkServer.sh start\n\nJMX enabled by default\nUsing config: /usr/local/zookeeper/bin/../conf/zoo.conf\nStarting zookeeper ... STARTED\n</code></pre> <p>You can now validate that ZooKeeper is running correctly in standalone mode by connecting to the client port and sending the four-letter command <code>srvr</code>. This will return basic ZooKeeper information from the running server:</p> <pre><code># telnet localhost 2181\n</code></pre>"},{"location":"kafka/installation/01-apache-zookeeper/#zookeeper-ensemble","title":"ZooKeeper ensemble","text":"<p>ZooKeeper is designed to work as a cluster, called an ensemble, to ensure high availability. Due to the balancing algorithm used, it is recommended that ensembles contain an odd number of servers as a majority of ensemble members (a quorum) must be working in order for ZooKeeper to respond to requests. This means that in a three-node ensemble, you can run with one node missing. With a five-node ensemble, you can run with two nodes missing.</p> <p>Consider running ZooKeeper in a five-node ensemble. To make configuration changes to the ensemble, including swapping a node, you will need to reload nodes one at a time. If your ensemble cannot tolerate more than one node being down, doing maintenance work introduces additional risk. It is also not recommended to run more than seven nodes, as performance can start to degrade due to the nature of the consensus protocol.</p> <p>Additionally, if you feel that five or seven nodes aren\u2019t supporting the load due to too many client connections, consider adding additional observer nodes for help in balancing read-only traffic.</p> <p>To configure ZooKeeper servers in an ensemble, they must have a common configuration that lists all servers, and each server needs a myid file in the data directory that specifies the ID number of the server. The configuration file might look like this:</p> <pre><code>tickTime=2000\ndataDir=/var/lib/zookeeper\nclientPort=2181\ninitLimit=20\nsyncLimit=5\nserver.1=zoo1.example.com:2888:3888\nserver.2=zoo2.example.com:2888:3888\nserver.3=zoo3.example.com:2888:3888\n</code></pre> <p>The <code>initLimit</code> is the amount of time to allow followers to connect with a leader. The <code>syncLimit</code> value limits how long out-of-sync followers can be with the leader. Both values are a number of <code>tickTime</code> units, which makes the <code>initLimit</code> 20 x 2,000 ms, or 40 seconds. The configuration also lists each server in the ensemble. The servers are specified in the format:</p> <pre><code>server.x=hostname:peerPort:leaderPort\n</code></pre> <ul> <li><code>X</code> - The ID number of the server. This must be an integer, but it does not need to be zero-based or sequential.</li> <li><code>peerPort</code> - The TCP port over which servers in the ensemble communicate with one another.</li> <li><code>leaderPort</code> - The TCP port over which leader election is performed.</li> </ul> <p>Clients only need to be able to connect to the ensemble over the clientPort, but the members of the ensemble must be able to communicate with one another over all three ports.</p> <p>In addition to the shared configuration file, each server must have a file in the dataDir directory with the name myid. This file must contain the ID number of the server, which must match the configuration file. Once these steps are complete, the servers will start up and communicate with one another in an ensemble.</p> <p>It is possible to test and run a ZooKeeper ensemble on a single machine by specifying all hostnames in the config as and have unique ports specified for and for each instance. Additionally, a separate zoo.cfg would need to be created for each instance with a unique dataDir and defined for each instance. This can be useful for testing purposes only, but it is not recommended for production systems.</p>"},{"location":"kafka/installation/02-apache-kafka/","title":"Apache Kafka","text":"<p>Once Java and ZooKeeper are configured, you are ready to install Apache Kafka.</p>"},{"location":"kafka/installation/02-apache-kafka/#installation","title":"Installation","text":"<p>The following example installs Kafka in <code>/usr/local/kafka</code>, configured to use the ZooKeeper server started previously and to store the message log segments stored in <code>/tmp/kafka-logs</code>:</p> <pre><code># tar -zxf kafka_x.xx-x.x.x.tgz\n# mv kafka_x.xx-x.x.x /usr/local/kafka\n# mkdir /tmp/kafka-logs\n# export JAVA_HOME=/usr/java/jdk-xx.x.xx\n# /usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties\n</code></pre>"},{"location":"kafka/installation/02-apache-kafka/#verification","title":"Verification","text":"<p>Once the Kafka broker is started, we can verify that it is working by performing some simple operations against the cluster: creating a test topic, producing some messages, and consuming the same messages.</p> <p>Create and verify a topic:</p> <pre><code># /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --replication-factor 1 --partitions 1 --topic test\nCreated topic \"test\"\n\n# /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test\nTopic:test  PartitionCount:1    ReplicationFactor:1     Configs: ........\n</code></pre> <p>Produce messages to a test topic (use Ctrl-C to stop the producer at any time):</p> <pre><code># /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test\nTest Message 1\nTest Message 2\n^C\n</code></pre> <p>Consume messages from a test topic:</p> <pre><code># /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning\nTest Message 1\nTest Message 2\n^C\nProcessed a total of 2 messages\n</code></pre>"},{"location":"kafka/installation/02-apache-kafka/#deprecation-of-zookeeper-connections-on-kafka-cli-utilitites","title":"Deprecation of ZooKeeper connections on Kafka CLI utilitites","text":"<p>If you are familiar with older versions of the Kafka utilities, you may be used to using a <code>--zookeeper</code> connection string. This has been deprecated in almost all cases. The current best practice is to use the newer <code>--bootstrap-server</code> option and connect directly to the Kafka broker. If you are running in a cluster, you can provide the host:port of any broker in the cluster.</p>"},{"location":"machine-learning/","title":"Machine Learning (Simplified)","text":"<p>Machine learning is like teaching a robot to learn by itself. It's about teaching computers to learn from examples and experiences, just like how you learn new things!</p> <p>Imagine that you want a robot to learn to sort your toys into different boxes. At first, the robot doesn't know which toy goes into which box. But, if you show the robot many times that the teddy bear goes into the teddy bear box, and the car goes into the car box, the robot will start to learn. After a while, the robot can sort the toys by itself without your help.</p>"},{"location":"machine-learning/#ways-of-teaching-the-robot","title":"Ways of teaching the robot","text":""},{"location":"machine-learning/#supervised-learning","title":"Supervised Learning","text":"<p>These algorithms learn a function that maps inputs to an output from a set of labeled training data.</p> <p>This is like when parents help the kid with their homework. They already know the correct answers (because they're the \"teacher\") and they guide them to learn the right answers. In the same way, in supervised learning, we have a robot that is <code>learning from examples where we already know the right answer</code>. </p>"},{"location":"machine-learning/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>These algorithms learn fundamental patterns of the data from unlabeled data samples, without specific goal in mind. This is a NP-hard problem, and does not scale well with data. This issue has made it hard for the research to further much in this area, as compared to supervised learning.</p> <p>This is like when kids play with their Lego blocks. Nobody tells them what to build, but they start grouping blocks by color or size, and maybe they start building something on their own. In unsupervised learning, the robot doesn't have any right answers to learn from. Instead, it <code>tries to find patterns or groups in the data</code> all by itself.</p>"},{"location":"machine-learning/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>These algorithms try to mimic typically how human's learn, through taking an action and waiting for feedback or reward, for them to decide on the next step.</p> <p>Watch documentary of Google Deepmind Go challenge match. In game 2, step 32 or 37, the step that the algorithm made was ingenious. Find commentaries on that move or read literature around it. It's amazing what these algorithms can learn when they're given so much data.</p> <p>This is like when you play a video game. You try different things, and sometimes you win points, and sometimes you lose points. Over time, you learn what actions help you win more points. In reinforcement learning, a robot <code>learns by trying different actions and getting rewards or penalties</code>. It learns to do the things that get it the most rewards. </p>"},{"location":"machine-learning/#other-areas-in-research","title":"Other areas in research","text":"<ul> <li>Active learning - early in adoption. Also in a place where it allows to sparingly have a human in the loop.</li> <li>Self-supervised learning - Meta's chief scientist is a big proponent of learning from unlabeled data.</li> <li>Transfer learning</li> </ul>"},{"location":"machine-learning/#common-algorithms","title":"Common Algorithms","text":"<ul> <li>Regression - predicting a value</li> <li>Classification - predicting a class or category</li> <li>Clustering - grouping ungrouped data</li> <li>Association Mining Rule - finding associations between two or more classes</li> </ul>"},{"location":"machine-learning/#deep-learning","title":"Deep Learning","text":"<p>When you're applying machine learning techniques, one of the key aspect is <code>Feature engineering</code>, where you're trying to create some sort of features or variables that'll allow the system to learn the task better. Feature engineering is limited by your intuition and understanding of the problem itself. These are mostly driven by domain knowledge, the context of the problem space and your creativity in what you think is important to the problem.</p> <p><code>Deep learning</code> is a class of ML algorithms that uses multiple layers to progressively extract higher-level features/abstractions from raw inputs. </p> <p>Deep learning became a big deal in ML space, because it takes away the feature engineering aspect away from you. It starts to abstract the useful information from the data itself, and is not limited like human brain.</p> <p>On the contrary, deep learning has also made machine learning a black box, meaning it is harder to figure out what the model is trying to do.</p>"},{"location":"machine-learning/#trends-over-time","title":"Trends over time","text":"<ul> <li>&lt;1950s - Statistical methods</li> <li>1950s  - Simple ML algorithms</li> <li>1960s  - Bayesian methods in ML</li> <li>1970s  - AI winters</li> <li>1980s  - Back propagation</li> <li>1990s  - SVMs &amp; RNNs</li> <li>2000s  - Kernel methods</li> <li>2010s  - Deep learning</li> <li>2020s  - LLMs</li> </ul> <p>Note: When you're solving SVMs, you're actually solving quadratic optimization problem, that does not scale with the number of samples itself. In simple terms, if you had to train with a large amount of data, training of SVMs becomes a problem.</p> <p>Note: Yoshua Bengio is the one who brought breakthrough in Deep learning space.</p> <p>Note: Models in 1950s were trained on 2 parameters. GPT3 is trained on 175 billion parameters. ML space has evolved a lot in 70 years.</p>"},{"location":"machine-learning/#reason-for-progress","title":"Reason for progress","text":"<ol> <li>Computational power</li> <li>Big data + Map-Reduce</li> <li>Breakthrough in Deep learning</li> </ol> <p>All these things came in between 2006 to 2010. For example, you can have an EC2 instance with 12 TB of RAM, democratization of AI</p>"},{"location":"machine-learning/#references","title":"References","text":"<ul> <li>Applied Machine Learning, Columbia University</li> <li>Machine Learning Course - CS 156 | CatTech | YouTube playlist<ul> <li>Website</li> </ul> </li> <li>Book: Fundamentals of Data Visualization, Claus O. Wilke, 2019<ul> <li>Online book</li> <li>GitHub code</li> </ul> </li> </ul>"},{"location":"machine-learning/common-ml-techniques/","title":"Common ML Techniques","text":"<ul> <li> <p>Data Preprocessing - preparing the data for machine learning models. </p> <ul> <li>This includes cleaning the data, handling missing values, encoding categorical variables, scaling the data, etc.</li> <li>Data preprocessing is an essential step in the machine learning pipeline and is used with all types of machine learning algorithms. </li> <li>A crucial step before applying any machine learning algorithm, be it regression, classification, or clustering.</li> </ul> </li> <li> <p>Dimensionality Reduction - reducing the number of input variables in a dataset.</p> <ul> <li>Used with regression, classification, and clustering algorithms to improve their performance, especially when dealing with high-dimensional data</li> <li>Using feature extraction techniques like <code>Principal Component Analysis (PCA)</code>, <code>t-SNE</code>, <code>Hashing</code> (Locality sensitive hashing), etc.</li> </ul> </li> <li> <p>Ensemble Methods - combining predictions from multiple models to improve the performance and accuracy over any individual model.</p> <ul> <li>Techniques like <code>Bagging</code>, <code>Boosting</code>, <code>Stacking</code>, etc.</li> <li>Used with regression and classification algorithms.</li> </ul> </li> <li> <p>Evaluation Metrics - used to evaluate the performance of ML models. </p> <ul> <li>They are used with all types of machine learning tasks including regression (e.g., <code>Mean Squared Error</code>), classification (e.g., <code>Accuracy</code>, <code>Precision</code>, <code>Recall</code>), and clustering (e.g., <code>Silhouette Score</code>).</li> <li>Common metrics include accuracy, precision, recall, <code>F1 score</code>, <code>ROC-AUC</code>, etc.</li> </ul> </li> <li> <p>Overfitting and Underfitting - common problems in machine learning where the model performs well on the training data but poorly on the test data.</p> <ul> <li><code>Overfitting</code> occurs when the model is too complex and captures noise in the training data.</li> <li><code>Underfitting</code> occurs when the model is too simple and fails to capture the underlying patterns in the data.</li> <li>These concepts apply to all types of machine learning models including regression, classification, and clustering.</li> </ul> </li> <li> <p>Neural Networks and Deep Learning - types of ML model, particularly useful for complex tasks like image recognition, natural language processing, etc.</p> <ul> <li>These are a set of algorithms, modeled loosely after the human brain.</li> <li>Different types of neural networks like <code>Convolutional Neural Networks (CNN)</code>, <code>Recurrent Neural Networks (RNN)</code>, etc.</li> <li>Can be used for a wide range of tasks, including regression, classification, and clustering.</li> </ul> </li> </ul>"},{"location":"machine-learning/common-ml-techniques/#neural-networks-vs-deep-learning","title":"Neural Networks vs Deep Learning","text":"<p>NN consists of interconnected layers of nodes, or \"neurons\", and each connection between nodes has a weight that is adjusted during training. Neural networks can learn to perform tasks by considering examples, generally without task-specific programming.</p> <p>Deep learning is a subfield of machine learning that focuses on neural networks with many layers - hence the term \"deep\". These deep neural networks are capable of learning from large amounts of data and can automatically extract features from the data during training. This is a key advantage of deep learning over traditional machine learning methods, which often require manual feature extraction. Deep learning is particularly effective for complex tasks such as image recognition, speech recognition, and natural language processing.</p>"},{"location":"machine-learning/common-ml-techniques/#llm","title":"LLM","text":"<ul> <li> <p>Large Language Model (LLM): This is a type of AI model used in natural language processing (NLP). These models, such as GPT-3 or GPT-4, are designed to understand and generate human-like text. They are trained on a large corpus of text data and can generate coherent and contextually relevant sentences. They are used in a variety of applications including translation, question answering, and text generation.  </p> </li> <li> <p>Latent Linear Model (LLM): This is a type of statistical model used in machine learning for data analysis. In a latent linear model, the observed data is assumed to be a linear function of some latent (unobserved) variables, plus some noise. These models are often used in dimensionality reduction techniques, such as Principal Component Analysis (PCA), where the goal is to represent high-dimensional data in a lower-dimensional space.</p> </li> </ul>"},{"location":"machine-learning/getting-hands-on/","title":"Hands-on: Getting Started","text":"<p>It's easiest to jump in and get your hands dirty by building small projects in Python. Python already has supporting frameworks and libraries that makes it easier to enter the ML world.</p>"},{"location":"machine-learning/getting-hands-on/#python-frameworks","title":"Python Frameworks","text":"<p>Python Frameworks for building ML models</p> <ul> <li>Scikit-learn - very easy to use, implements many machine learning algorithms efficiently, so it makes for a great entry point to learning machine learning.</li> <li>Tensorflow - a more complex library for distributed numerical computation. It makes it possible to train and run very large neural networks efficiently by distributing the computations across potentially hundreds of multi-GPU (graphics processing unit) servers.</li> <li>Keras - a high-level deep learning API that makes it very simple to train and run neural networks. Keras comes bundled with TensorFlow, and it relies on TensorFlow for all the intensive computations.</li> </ul>"},{"location":"machine-learning/getting-hands-on/#data-manipulation-libraries","title":"Data Manipulation libraries","text":"<p>Python\u2019s main scientific libraries</p> <ul> <li>NumPy</li> <li>Pandas</li> <li>Matplotlib</li> </ul>"},{"location":"machine-learning/getting-hands-on/#resources","title":"Resources","text":"<ul> <li>Library tutorials - Google Colab, Aur\u00e9lien Geron</li> <li>Scikit-learn user guide</li> </ul>"},{"location":"machine-learning/learning-path/","title":"Learning Path","text":"<p>Machine learning is an extremely wide field. There is a lot to be learnt, and there won't ever be enough time. As software engineer, trying to get into the field of machine learning, you'll need to be picky about what you spend your time on.</p>"},{"location":"machine-learning/learning-path/#applied-machine-learning","title":"Applied Machine Learning","text":"<p>This topic dives into the application of machine learning in real world, rather than staying in the theoretical lane of general machine learning. The term <code>Applied</code> means you're not trying to prove anything, or research a topic or question an approach. The sole focus is on learning how to apply the knowledge in real world.</p> <p>This also means that you won't know everything, won't know all the trade-offs, and that's ok.</p>"},{"location":"machine-learning/learning-path/#next-steps","title":"Next steps","text":"<ul> <li>NLP</li> <li>Computer Vision</li> </ul>"},{"location":"machine-learning/resources/","title":"Resources","text":""},{"location":"machine-learning/resources/#books-applied-ml","title":"Books (Applied ML)","text":"<ul> <li>Introduction to Machine Learning with Python, Muller &amp; Guido | O'Reilly</li> <li>Learning from Data, Yaser Mostafa<ul> <li>About:</li> <li>CalTech Machine Learning Course - CS 156 | YouTube playlist</li> <li>Book website</li> </ul> </li> <li>Deep Learning, Yoshua Bengio<ul> <li>About: Contains everything you need to know about deep learning. So no need to refer anything else. First 5 chapters talk about foundational aspects of deep learning, not the core topic, like: statistics, probability, optimization, etc. From 6th chapter onwards, they'll go into deep learning, CNN, RNN, etc.</li> <li>Pdf on GitHub</li> <li>Online version</li> </ul> </li> <li>Applied Predictive Modeling</li> <li>Fundamentals of Data Visualization, Claus Wilke | O'Reilly<ul> <li>About: Classic book on data visualization. You'll read this book multiple times, esp when trying to present data.</li> </ul> </li> <li>Extras: Pattern Recognition, Christopher Bishop<ul> <li>Pdf on Microsoft website</li> </ul> </li> <li>Extras: Probabilistic Machine Learning, Kevin Murphy<ul> <li>Pdf on GitHub</li> <li>Book website</li> </ul> </li> </ul>"},{"location":"machine-learning/resources/#machine-learning-papers","title":"Machine Learning Papers","text":"<ul> <li>A Fast Learning Algorithm for Deep Belief Nets - Geoffrey Hinton, 2006<ul> <li>Shows how to train a deep neural network capable of recognizing handwritten digits with state-of-the-art precision (&gt;98%)</li> </ul> </li> <li>Gradient-Based Learning Applied to Document Recognition - Yann LeCun, 1998</li> <li>ImageNet Classification with Deep Convolutional Neural Networks - Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton, 2012</li> <li>Playing Atari with Deep Reinforcement Learning - Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, 2013</li> <li>Deep Residual Learning for Image Recognition - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2015</li> <li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018</li> <li>Attention is All You Need - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, 2017</li> </ul>"},{"location":"machine-learning/resources/#learning-resources","title":"Learning Resources","text":"<ul> <li>Google Machine Learning Crash Course</li> <li>The HundredPage Machine Learning Book | pdf</li> <li>Kaggle</li> <li>Machine Learning Mastery</li> <li>Pattern Recognition and Machine Learning - Christopher M. Bishop, 2006</li> <li>Machine Learning: A Probabilistic Perspective - Kevin P. Murphy, 2012</li> <li>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, third edition - Aur\u00e9lien G\u00e9ron, 2022</li> <li>Deep Learning - Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016</li> <li>Deep Learning Specialization</li> <li>Fast.ai</li> <li>edX</li> <li>DataQuest</li> <li>Machine Learning Course | Coursera</li> <li>MIT OpenCourseWare</li> <li>Towards Data Science</li> </ul>"},{"location":"machine-learning/resources/#blogs","title":"Blogs","text":"<ul> <li>Andrej Karpathy</li> <li>Christopher Olah</li> <li>Denny Britz</li> <li>Tim Dettmers</li> <li>Open AI</li> <li>Google Research</li> </ul>"},{"location":"machine-learning/resources/#newsletters","title":"Newsletters","text":"<ul> <li>AI Weekly</li> <li>Deep Learning Weekly</li> <li>Import AI</li> <li>Synced</li> <li>The Gradient</li> <li>Mostly behind paywall<ul> <li>Towards Data Science</li> </ul> </li> </ul>"},{"location":"machine-learning/resources/#tutorials","title":"Tutorials","text":"<ul> <li>Scikit-learn Documentation</li> <li>TensorFlow Documentation</li> <li>Keras Documentation</li> <li>PyTorch Documentation - an optimized tensor library for deep learning using GPUs and CPUs.</li> <li>Gradient Boosting<ul> <li>XGBoost Documentation</li> <li>LightGBM Documentation</li> <li>CatBoost Documentation</li> </ul> </li> <li>MLflow Documentation - a tool for managing Machine learning lifecycle</li> <li>DVC Documentation - helps machine learning teams manage large datasets, make projects reproducible, and collaborate better.</li> <li>Pandas Documentation</li> <li>NumPy Documentation</li> <li>Matplotlib Documentation</li> <li>RayTune - for hyperparameter tuning</li> </ul>"},{"location":"machine-learning/resources/#public-datasets","title":"Public Datasets","text":"<ul> <li>Dataset Search</li> <li>Kaggle</li> <li>Government datasets</li> </ul>"},{"location":"machine-learning/nuggets/01-exploratory-data-analysis/","title":"Exploratory Data Analysis (EDA)","text":"<p>EDA is an approach of analyzing datasets to <code>summarize</code> their main characteristics, often using statistical graphics and other data visualization methods.</p> <p>Think of this as telling story through data or presenting data in a way that actually conveys a point, which does not come to one easy. Some people have a knack of it, while most learn it over time.</p>"},{"location":"machine-learning/nuggets/01-exploratory-data-analysis/#why-eda","title":"Why EDA?","text":"<ul> <li>Explore - get a feel of the data, null values, missing values, data size, what can be plotted, etc</li> <li>Inform - to inform your findings downstream and establish some baselines. For example, if the data is categorical, trees work well with them</li> <li>Communicate - communicate results effectively based on audience, as business may not have the same background as you do about machine learning metrics like AUC or RMSC.</li> </ul>"},{"location":"machine-learning/nuggets/01-exploratory-data-analysis/#data-types","title":"Data Types","text":"<p>The kind of data types that you'll come across:</p> <ul> <li>Quantitative / numerical continuous - 1, 3.5, 10^10</li> <li>Quantitative / numerical  discrete - 1, 2, 3, 4</li> <li>Qualitative / categorical unordered - cat, dog, whale</li> <li>Qualitative / categorical ordered - good, better, best</li> <li>Date or time</li> <li>Text</li> </ul>"},{"location":"machine-learning/nuggets/02-data-visualization/","title":"Data Visualization","text":""},{"location":"machine-learning/nuggets/02-data-visualization/#building-blocks","title":"Building Blocks","text":""},{"location":"machine-learning/nuggets/02-data-visualization/#aesthetics","title":"Aesthetics","text":"<p>Aesthetics refer to a quantifiable set of features that are mapped to the data in a graphic. They describe every aspect of a given graphic element.</p> <p>Some aesthetics like position, size, color, and line width work for both <code>continuous</code> and <code>discrete</code> data, while others (shape &amp; line type) work for only discrete data.</p>"},{"location":"machine-learning/nuggets/02-data-visualization/#scales","title":"Scales","text":"<p>You have data, and you have aesthetics. You need some sort of mapping between the two, which is nothing but the <code>legend</code> that you plot in your plot.</p> <p><code>Scales</code> are the mapping between data values and aesthetics values.</p> <p>Taking an example of a 2D plot, it doesn't mean you can only use two scales. Sometimes you can expand your visualization to five scales (as an example) in the same plot. In this case, it is like providing 5 dimensions in a 2-dimensional figure.</p> <p>Of all the scales, the most common scales are:</p> <ol> <li>Position scale - most commonly uses Cartesian coordinate system. It is ok to use different <code>aspect ratios</code> (spacing between grids) across dimensions, unless you're using the same dimension on both the axes.</li> <li>Nonlinear scale (Logarithmic scale) - most commonly used for Nonlinear Axes. This is where you tend to use skewed data because of very different orders of magnitude. This scale is helpful when you're drawing proportions/representing ratios.</li> <li>Color scale - used to distinguish groups of data, or represent data values, or as a tool to highlight</li> </ol>"},{"location":"machine-learning/nuggets/02-data-visualization/#types-of-plots","title":"Types of plots","text":"<p>Most commonly, the kinds of things you're trying to plot are:</p> <ul> <li>Amounts</li> <li>Distributions</li> <li>Proportions</li> <li>X-Y relationships</li> <li>Uncertainty - esp important for A/B tests</li> </ul> <p>Sometimes ordering representation on an axis helps better represent the findings.</p>"},{"location":"machine-learning/nuggets/02-data-visualization/#visualizing-distributions","title":"Visualizing distributions","text":"<ul> <li>Histograms - when making histograms, always explore multiple bin widths</li> <li>Kernel density plots - if the values are too small, you instead try to fit a curve to the data through a function, solving the optimization problem (also called curve fitting). To visualize several distributions at once, kernel density plots work better than histograms.<ul> <li>Different kernels include Gaussian, Rectangular, etc.</li> <li>Each kernel is parameterized by bandwidth</li> </ul> </li> <li>Multiple distributions can be represented on <code>Box plot</code> (minimum 5 points), or <code>Violin plot</code> (also captures density of the points unlike box plot)</li> </ul> <p>For highly skewed distributions, you can still use log of values in visualization to get better representation and information about the data.</p>"},{"location":"machine-learning/nuggets/02-data-visualization/#visualizing-proportions","title":"Visualizing proportions","text":"<ul> <li>Pie charts - help visually emphasize simple fractions</li> <li>Stacked bars - preferred when there are only two quantities to compare over time.<ul> <li>Stacked density plots can be used to visualize how proportions change in response to a continuous variable.</li> </ul> </li> <li>Side-by-side bars - help visualize easily changing proportions over time</li> </ul>"},{"location":"machine-learning/nuggets/02-data-visualization/#visualizing-x-y-relationships","title":"Visualizing X-Y relationships","text":"<p>When you want to understand relationships between more than one column.</p> <ul> <li>Scatter plots</li> <li>Bubble plots</li> <li>Scatterplot matrix - you'll see this a lot in research papers</li> <li>Correlograms - uses correlation coefficient</li> </ul>"},{"location":"machine-learning/nuggets/02-data-visualization/#visualizing-uncertainty","title":"Visualizing uncertainty","text":"<p>Used a lot in A/B testing. Another example is election polls.</p> <ul> <li>Probability distribution</li> <li>Confidence intervals</li> <li>Comparing parameter estimates</li> </ul> <p>In machine learning, the gold standard for launching anything in production, is an A/B test.</p>"},{"location":"machine-learning/nuggets/02-data-visualization/#references","title":"References","text":"<ul> <li>Book: Fundamentals of Data Visualization, Claus O. Wilke, 2019<ul> <li>Online book</li> <li>GitHub code</li> </ul> </li> </ul>"},{"location":"machine-learning/nuggets/03-supervised-learning/","title":"Supervised Learning","text":"<p>These algorithms learn a function that maps inputs to an output from a set of labeled training data. The goal for supervised learning is to generalize to unseen data. </p>"},{"location":"machine-learning/nuggets/03-supervised-learning/#supervised-learning-framework","title":"Supervised Learning Framework","text":"<p>There is a lot of work that goes into creating this dataset, where this data has some <code>features</code> or <code>variables</code>, and a <code>label</code>. Label is termed as <code>Y</code>, and all the features are termed as <code>X</code>.</p> <ol> <li>You split the dataset into train/development data and test data.</li> <li>Go through a process called <code>hyperparameter tuning</code>, where these hyperparameters control the model complexity.<ol> <li>Split the development data into training data and validation data.</li> <li>Determine the <code>best set of hyperparameters</code> through hyperparameter tuning.</li> </ol> </li> <li>Take the development data, apply the best hyperparameters, and train the model (<code>optimal model training</code>).</li> <li>Evaluate the trained model on the test data (<code>model evaluation</code>), where you get some sort of model performance. This steps give you some sort of guarantee on how the model would do if deployed in production.</li> <li>Take the entire dataset, apply the best hyperparameters, and train the final model, that goes into deployment (<code>model deployment</code>).</li> </ol> <p>Note: The number of models you train here = the number of hyperparameters Hence, it is important to keep a check on the number of hyperparameters. For example, in a decision tree, one of the hyperparameter you're going to train on is the depth of the tree</p> <p>Note: For educational purposes, you can always use a publicly available dataset and skip this step.</p>"},{"location":"machine-learning/nuggets/03-supervised-learning/#development-test-data-split","title":"Development-test data split","text":"<ul> <li>Typically the dataset is split into development dataset and test dataset in the ratio of 4:1 (also called 80/20 split) or 3:1. scikit-learn has a default split of 3:1, which can be changed on will.</li> <li>Purpose of the test dataset is to evaluate the performance of the final optimal model</li> <li>Model evaluation is supposed to give a pulse on how the model would perform in the wild</li> <li>Be careful choosing splitting strategies. Overall you want these split datasets to be as similar as possible. Some of them are:<ul> <li>Random splitting</li> <li>Stratified splitting</li> <li>Structured splitting</li> </ul> </li> </ul>"},{"location":"machine-learning/nuggets/03-supervised-learning/#baseline","title":"Baseline","text":"<p>When you're training a model, establishing a baseline is very important, esp to know if you're doing good or bad in the next training.</p> <p>For example, in recommender systems, when we build very complex neural nets, we always try to compare to some simple models. One of the simple model could be making recommendations at random. Another, commonly used, simple model is to just recommend the most popular item to everyone. Sometimes it can be hard to beat these simple models.</p>"},{"location":"machine-learning/nuggets/03-supervised-learning/#k-nearest-neighbor","title":"k-nearest neighbor","text":"<p>One of the commonly used baseline technique is <code>k-nearest neighbors</code>.</p> <ul> <li>A simple non-parametric supervised learning method</li> <li>Assigns the value of the nearest neighbor(s) to the unseen data point</li> <li>Prediction is computationally expensive, while training is trivial (because there is no training at all)</li> <li>Generally performs poorly at high dimensions, because of <code>cursive dimensionality</code>, where the notion of distance gets distorted at very high dimensions, which also leads to poor performance.</li> </ul> <p><code>k</code> is the parameter you tune here. It could be 1 or 5 or anything that works best for the use case.</p> <p>An optimization to reduce computation is <code>approximate nearest neighbors</code>, where you bucket the data points. Now when a new data point comes in, you first figure out the bucket it falls into, and then calculate the nearest neighbor(s) and label in that bucket.</p> <p>At the end of the day, predictions is an expensive process because you have to compute distances. Distance can be anything including <code>euclidean</code> or <code>manhattan</code> or custom distance as long as it follows the norms of the distance metric, but it is still a computation.</p>"},{"location":"machine-learning/nuggets/03-supervised-learning/#hyperparameter-tuning","title":"Hyperparameter tuning","text":""},{"location":"machine-learning/nuggets/03-supervised-learning/#optimal-model-training","title":"Optimal model training","text":""},{"location":"machine-learning/nuggets/03-supervised-learning/#model-evaluation","title":"Model evaluation","text":""},{"location":"machine-learning/nuggets/03-supervised-learning/#model-deployment","title":"Model deployment","text":"<p>Note: Measure confidence around your prediction.</p>"},{"location":"machine-learning/nuggets/decision-trees/","title":"Decision Trees","text":"<p>Tree-based algorithms are widely used in the data science world. Decision trees, a supervised learning approach, are the fundamental building blocks of any tree-based model. Just like any ML algorithm, a decision tree helps predict the outcome given the values of input variables.</p> <p>Given any ML problem, you are going to start by asking what are the input variables (<code>Features</code> or <code>Predictors</code>) that you are going to use. For example: 1. In <code>Regression</code>, you may want to ask which input variable is going to help you predict the output in the best way. 2. If you want to <code>classify</code> an email as spam, what features of the email really help you to classify it as spam email.</p> <p>Start by asking: 1. Factors that affect the outcome 2. Order of importance of each factor, as some of these predictors are more important than others.</p> <p>Think of a flowchart with a yes or no outcome at each step. This is a decision tree.</p>"},{"location":"machine-learning/nuggets/decision-trees/#white-box","title":"White Box","text":"<p>If you want to classify articles as tech articles or something, you could use <code>Support Vector Machines</code> (SVM). SVMs provide you a plane (in like a 3-D space, n-D space), and based on which side of the plane the article falls, you'll decide the type of article. SVM is like a <code>black box</code>. It is hard to understand or visualize what's happening inside.</p> <p>A decision tree is like a <code>white box</code>. An intuitive and rule-like way to explain the classification algorithm. It's like you have a comprehensive set of rules and just by following the rules you arrive at the outcome.</p>"},{"location":"machine-learning/nuggets/decision-trees/#types-of-variables","title":"Types of Variables","text":"<p>Input variables represent <code>nodes</code> in a decision trees, where you will decide which way to go. A <code>leaf</code> would be the end goal/outcome in this case.</p> <p>Note: These types apply to input variables as well as output variables.</p>"},{"location":"machine-learning/nuggets/decision-trees/#categorical-variables","title":"Categorical variables","text":"<p>Variables that take value from a specific set of values. Example: gender, color, spam email. Here, since decision trees are used for categorical outcome, they can be used for <code>Classification</code>, while building a classification tree. In a classification problem, outcomes / leaf nodes are called <code>Class Labels</code>.</p>"},{"location":"machine-learning/nuggets/decision-trees/#continuousnumeric-variables","title":"Continuous/Numeric variables","text":"<p>One that can take in a range of values. Example: temperature, sale price of a house. Here, since decision trees are used for range of outcome, they can be used for <code>Regression</code>, while building a regression tree.</p>"},{"location":"machine-learning/nuggets/decision-trees/#outcome","title":"Outcome","text":"<p>Your <code>Feature Vector</code> would be a list of values, one for each of the Features. You'll precompute all possible vectors, and when your input matches one particular form of feature vector, it is going to result in a known outcome. Example: (1, X, X, X), (0, 1, X X), etc.</p> <p>Decision tree also gives us the <code>Conditional Probability</code> of the outcome, given the values of the input variables. Hence in general, the outcomes are <code>Non Deterministic</code>, also called most likely outcome. At each leaf node, we'll know the <code>probability</code> of the outcome.</p> <p>TODO: Find out how these probabilities are calculated.</p>"},{"location":"machine-learning/nuggets/decision-trees/#building-the-tree","title":"Building the tree","text":"<p><code>Recursive partitioning</code> is the most common strategy, to take some training data and arrive at a decision tree. You: 1. Take the entire training data as a single set 2. Then recursively keep dividing it into subsets, based on the values of the input variables. 3. Each time you divide the data into subsets, a node of the decision tree is created.</p> <p>Some of the recursive partitioning algorithms are: <code>ID3</code>, <code>C4.5</code>, <code>CART</code>, <code>CHAID</code>. These algorithms use a different metrics for arriving at the best attribute.</p> <p>Training data needs to be represented in the form of tuples, a feature vector, that represents that particular data point and it's label/outcome.</p>"},{"location":"machine-learning/nuggets/decision-trees/#recursive-partitioning","title":"Recursive Partitioning","text":"<p>The basic idea is to continuously split the data into subsets, based on input variables. With each split, one attribute is chosen to be the basis of the split.</p> <p>Greedy algorithm for learning a decision tree. We build histograms to find patterns and probabilities.</p> <p>There is a <code>stopping condition</code> when building decision trees, that is: 1. when all our subsets are mostly homogeneous 2. we run out of attributes 3. the tree is too big</p> <p>Finding the <code>best split</code> for the continuous input variable, is done by building histogram and finding a shift in data.</p> <p>There are more algorithms based on this strategy to solve for decision trees.</p>"},{"location":"machine-learning/nuggets/decision-trees/#cons-of-decision-trees","title":"Cons of Decision Trees","text":"<p>One serious problem with them - they are prone to <code>Overfitting</code>. There are bunch of techniques to avoid the overfitting, including <code>Cross Validation</code>, <code>Regularization</code>, and <code>Ensemble learning</code>.</p>"},{"location":"machine-learning/nuggets/decision-trees/#random-forest","title":"Random Forest","text":"<p>This is one particular tree-based model that avoids overfitting in decision trees using Ensemble learning.</p>"},{"location":"machine-learning/nuggets/decision-trees/#information-gain","title":"Information Gain","text":"<p>The idea of <code>Information Gain</code> is to reduce <code>entropy</code> and maximize <code>information</code>. Information is a measure of the amount of information a statement contains. The lower the probability, the more information you get. Example: guess the card game. <code>Entropy</code> is the measure of uncertainty in an answer when you're trying to measure something. Entropy decreases with certainty in the process of figuring out the answer. <code>ID3</code> and <code>C4.5</code> uses information gain.</p>"},{"location":"machine-learning/nuggets/decision-trees/#references","title":"References","text":"<ul> <li>Kaggle Titanic challenge<ul> <li>Get the data</li> <li>Train a classifier on the training set</li> <li>Compute predictions on the test set</li> <li>Submit the predictions made, to obtain a score of how good the classifier was</li> </ul> </li> </ul>"},{"location":"machine-learning/nuggets/llm/","title":"Llm","text":""},{"location":"machine-learning/nuggets/llm/#large-language-model-llm","title":"Large Language Model (LLM)","text":"<p>This is a type of AI model used in natural language processing (NLP). These models, such as GPT-3 or GPT-4, are designed to understand and generate human-like text. They are trained on a large corpus of text data and can generate coherent and contextually relevant sentences. They are used in a variety of applications including translation, question answering, and text generation.  </p>"},{"location":"machine-learning/nuggets/llm/#number-of-parameters","title":"Number of parameters","text":"<p>A model trained on larger number of parameters can be more desirable than the one trained on comparatively lesser number of parameters, because:</p> <ul> <li> <p>Increased Model Capacity: More parameters generally mean greater model capacity, allowing the model to capture more complex patterns and relationships in the data. This can lead to improved performance on a wide range of natural language processing (NLP) tasks, including text generation, comprehension, translation, and summarization.</p> </li> <li> <p>Improved Generalization: Larger models can potentially generalize better to unseen data, as they have more capacity to learn from diverse and extensive training data. This can result in better performance across a broader range of inputs and contexts, making the model more versatile and applicable to real-world scenarios.</p> </li> <li> <p>Enhanced Representation Learning: With a larger parameter space, the model can learn more nuanced and detailed representations of language and semantics. This can lead to richer and more expressive embeddings, enabling the model to capture subtle nuances in meaning and context more effectively.</p> </li> <li> <p>Reduced Bias and Stereotypes: Larger models trained on diverse and extensive datasets have the potential to mitigate bias and stereotypes by learning more balanced and representative representations of language. This can help address concerns about fairness and equity in AI systems.</p> </li> <li> <p>Better Adaptation to Specific Tasks: Large models trained on generic datasets can be fine-tuned on smaller, task-specific datasets to achieve state-of-the-art performance on specific tasks. The increased capacity of the model allows for more effective transfer learning, where the model can leverage its pre-trained knowledge to adapt quickly to new tasks with minimal additional training data.</p> </li> </ul>"},{"location":"machine-learning/nuggets/llm/#trade-offs","title":"Trade-offs","text":"<ul> <li>It's essential to consider the trade-offs involved in training larger models, including increased computational requirements, longer training times, and higher resource costs.</li> <li>Larger models may also be more prone to overfitting, where the model memorizes training data rather than learning generalizable patterns.</li> </ul>"},{"location":"machine-learning/nuggets/llm/#training-a-llm","title":"Training a LLM","text":"<p>The exact methods for training may vary depending on the specific architecture and training objectives, however, here is a general overview of the training process for an LLM:</p> <ol> <li> <p>Data Collection: Gather a large dataset of text data. This dataset typically consists of diverse and representative samples of natural language text, such as books, articles, websites, social media posts, and other sources. The quality and size of the dataset are crucial for training a high-performing language model.</p> </li> <li> <p>Preprocessing: The dataset undergoes preprocessing to clean and standardize the text. This may involve tasks such as tokenization (splitting text into individual words or sub-word units), lowercasing, removing punctuation, and handling special characters. Additionally, the text may be segmented into smaller chunks or sequences to facilitate training.</p> </li> <li> <p>Tokenization and Embedding: Each word or sub-word token in the preprocessed text is mapped to a unique numerical representation called an embedding. These embeddings capture the semantic meaning and relationships between words in the text and are used as input to the neural network architecture.</p> </li> <li> <p>Architecture Selection: Choose an appropriate neural network architecture for the language model. Common architectures for LLMs include recurrent neural networks (RNNs), long short-term memory networks (LSTMs), gated recurrent units (GRUs), transformer architectures (such as BERT, GPT, and T5), and their variants. The architecture should be capable of learning complex patterns and dependencies in the text data.</p> </li> <li> <p>Model Initialization: The neural network parameters (weights and biases) are initialized randomly or using pre-trained embeddings (if available). Pretraining on large-scale language modeling tasks (such as predicting the next word in a sequence) may help initialize the model with useful features and representations.</p> </li> <li> <p>Training Objective: The training objective for an LLM is typically to maximize the likelihood of predicting the next word or token in a sequence given the previous context. This is often formulated as a maximum likelihood estimation (MLE) or cross-entropy loss function, which measures the discrepancy between the predicted probability distribution over tokens and the true distribution.</p> </li> <li> <p>Training Algorithm: The model is trained using an optimization algorithm such as stochastic gradient descent (SGD), Adam, or variants thereof. During training, the model iteratively updates its parameters to minimize the training loss on the dataset. This involves forward propagation (computing predictions), backward propagation (computing gradients), and parameter updates based on the gradients.</p> </li> <li> <p>Hyperparameter Tuning: Hyperparameters such as learning rate, batch size, sequence length, and model architecture hyperparameters are tuned to optimize performance and convergence speed. This may involve experimentation and validation on held-out data or using techniques such as grid search or random search.</p> </li> <li> <p>Regularization: Regularization techniques such as dropout, weight decay, and layer normalization may be applied to prevent overfitting and improve generalization performance on unseen data.</p> </li> <li> <p>Evaluation: The trained model is evaluated on a separate validation or test dataset to assess its performance on various language understanding and generation tasks. Metrics such as perplexity, accuracy, F1 score, and BLEU score are commonly used to evaluate LLMs.</p> </li> <li> <p>Fine-Tuning: After initial training on a large dataset, the LLM may be fine-tuned on task-specific datasets to adapt its learned representations to specific downstream tasks such as text classification, sentiment analysis, question answering, or language translation.</p> </li> </ol> <p>Overall, training an LLM is a complex and computationally intensive process that involves careful selection of data, preprocessing, architecture, optimization, and evaluation techniques to build a high-quality language model capable of understanding and generating natural language text.</p>"},{"location":"machine-learning/nuggets/rag/","title":"Retrieval Augmented Generation (RAG)","text":"<p>Motivation for RAGs is really focused around two key considerations: 1. Ability to feed information from external sources into LLMs for processing during text generation, to extend knowledge sources, such as private data, and leading to more contextual responses. 2. LLMs will be at the center of a new kind of operating system</p> <p>The main idea is that documents are indexed such that they can be retrieved based upon some heuristics relative to an input. And those relevant documents can be passed to an LLM, and the LLM can produce answers that are grounded in that retrieved information.</p>"},{"location":"machine-learning/nuggets/rag/#steps-involved","title":"Steps involved","text":"<ol> <li>Indexing of external data. Building a database, for example.</li> <li>Retrieval</li> <li>Generation</li> </ol>"},{"location":"machine-learning/nuggets/recommendation-systems/","title":"Recommendation Systems","text":"<p>Recommendations are for helping users to find something within a limited time, solving the problem of discovery.</p> <p>Recommendation engines perform tasks like:</p> <ul> <li>Filter relevant products<ul> <li>Similar to ones the user like</li> <li>Liked by similar users</li> <li>Products purchased along with the product</li> </ul> </li> <li>Predict what the user would choose<ul> <li>Clicked on</li> <li>Added to the cart</li> <li>Rated highly</li> </ul> </li> <li>Rank on relevance to the user</li> </ul> <p>Most recommendation systems use one or more of the techniques below.</p>"},{"location":"machine-learning/nuggets/recommendation-systems/#content-based-filtering","title":"Content-based Filtering","text":"<p>Comparing similarity of the objects. Filtering based on item attributes, description and/or content. Used to find the products similar to the ones that the user has already indicated that they \"like\".</p> <p>Normally used with text documents (books, articles, etc)</p> <p>Start by: 1. Identifying <code>attributes</code> that differentiate products, that may be influential in deciding if the user will like the product or not. Represent products in terms of descriptors or attributes. Represent all these attributes as <code>vectors</code> or <code>tuples</code>. 2. Create a user's profile based on the user's history, by quantifying what the user likes. Represent users using the same product descriptors or attributes. 4. Find products that match the user's profile based on distance between two vectors.</p> <p>Identifying the right set of attributes is the key challenge in content-based filtering, typically collection manually.</p> <p>Alternatively, you could use NLP for generating descriptors, based on presence, absence or frequency of words in the document. Once you have the frequency, you can then find the nearest neighbors for matching.</p> <p>Content-based filtering requires <code>manual data collection</code> phase, to map every product and user against the identified attributes. Hence pure content-based filtering are <code>less common</code>.</p> <p>The most successful example is the music genome project, owned by Pandora radio. They built descriptors for every song in their catalog. Every song is represented by 450 \"genes\" or factors or attributes. Trained musical analysts score each song on these attributes. The process normally takes 20-30 minutes per song. Using all this data, the radio keeps playing songs that match the user's preferences.</p>"},{"location":"machine-learning/nuggets/recommendation-systems/#collaborative-filtering","title":"Collaborative Filtering","text":"<p>Liked by similar users. Predicting what a user may like, without knowing anything about the product. This technique is <code>preferred a lot</code>. It does not even require you to know the description of the product.</p> <p>Ask a friend, who likes the same things as you do! So if two users have the same opinion about a bunch of products, then they are likely to have the same opinion about other products too. The algorithms rely on user behavior (browse clicks or searches and purchase history, ratings, similar users, etc). It <code>cannot</code> use product descriptions and user demographics. It's objective is to predict a user's score for a particular product.</p> <p>Two popular techniques are: 1. <code>Nearest Neighbor based methods</code> - tries to find users who are most similar to a particular user, based on some similarity metrics and using weighted average 2. <code>Latent Factor based methods</code> - uses attributes identified in content-based filtering and maps users and products against those factors. Inspired by content-based but you still cannot use product description or user demographics.</p>"},{"location":"machine-learning/nuggets/recommendation-systems/#nearest-neighbor","title":"Nearest Neighbor","text":"<p>The objective is to predict a user's rating for a product they haven't rated yet.</p> <p>Also known as <code>Memory based method</code>, because they usually involve in-memory based calculations or looping through a large part of the database, which is also a challenge for scalability reasons. Amazon and Barnes &amp; Noble uses memory-based collaborative filtering techniques for most of their recommendations.</p> <ol> <li>Find the <code>k-nearest neighbors</code> of the user</li> <li>Take a weighted average of their rating for the product</li> <li>Build a <code>rating matrix</code> between user and items. No ratings by the user for the item will stay blank.</li> <li>Compare ratings and return the weighted average rating for the item in question.</li> <li>This same metric can be used for the weight of each neighbor in the predicted rating.</li> </ol> <p><code>Similarity</code> (or <code>distance</code>) between two vectors or tuples can be measured in many ways. Popular ones being: 1. <code>Euclidean Distance</code> is simply a distance between two coordinates in a straight line, calculated using pythagoras theorem. The same concept can be applied to a 3-d space. 2. <code>Cosine similarity</code> is calculated by Cosine angle between two coordinates. Lower angle is a sign of high similarity. 3. <code>Pearson Correlation</code> is same as correlation used in linear regression. Analogous to cosine similarity, but after adjusting the vectors by the respective means. Users have a <code>natural bias</code>. Some naturally rate movies higher, while others rate everything low. One way to normalize this bias is by taking average rating. This is a mean of all ratings for the user, and then shift each number by their mean. Cosine similarity between these two new vectors is called Pearson correlation.</p> <p>Here in the equation, weight constant is the similarity between users. Or weighted average rating between users, for example.</p> <p>For <code>top picks</code>, predict ratings (using the formula for weighted average for n users) for the product not purchased or seen, and then pick the top N rated products.</p> <p>Instead of user-centric, this can be performed for item-centric as well. For example, for a 3rd part of the movie, find ratings for first two parts and rate 3rd part accordingly.</p>"},{"location":"machine-learning/nuggets/recommendation-systems/#latent-factor","title":"Latent Factor","text":"<p>Latent factor methods identify hidden factors that influence users from user history. <code>Matrix Factorization</code> is used to find these factors. This method was first used and then popularized for recommendations by the <code>Netflix</code> Prize winners. Many modern recommendation systems including Netflix, use some form of matrix factorization.</p>"},{"location":"machine-learning/nuggets/recommendation-systems/#association-rules","title":"Association Rules","text":"<p>Purchased along with the ones the user liked. The main idea is to find some kind of association between two products, if one is bought, then the other will also be bought. Example: batteries for camera.</p>"},{"location":"machine-learning/nuggets/recommendation-systems/#resources","title":"Resources","text":""},{"location":"message-queue/","title":"Message Queue","text":""},{"location":"message-queue/#todo","title":"TODO","text":"<ul> <li>ZeroMQ</li> <li>MQTT</li> <li>ActiveMQ</li> <li>RabbitMQ</li> <li>Kafka</li> <li>Blogs<ul> <li>Kafka V/S ZeroMQ V/S RabbitMQ: Your 15-Minute Architecture Guide</li> <li>MQTT vs ZeroMQ for IoT</li> <li>Free Public MQTT Broker</li> </ul> </li> </ul>"},{"location":"networking/","title":"Networking","text":""},{"location":"networking/#transmission-control-protocol-tcp","title":"Transmission Control Protocol (TCP)","text":"<p>The protocol sits at layer 4 of the OSI model, and it is responsible for delivering the data segment between two nodes in a reliable and error-checked manner. The TCP consists of a 160-bit header that contains, among other fields, source and destination ports, a sequence number, an acknowledgment number, control flags, and a checksum.</p> <p>TCP uses <code>datagram</code> sockets or ports to establish host-to-host communication, through designated ports such as port 80 for HTTP (web) and port 25 for SMTP (mail). The server listens on one of these well-known ports in order to receive communication requests from the client. The TCP connection is managed by the operating system with the socket representing the local endpoint for the connection.</p> <p>The protocol operation consists of a <code>state machine</code>, which the machine needs to keep track of when it is listening for an incoming connection during the communication session, as well as releasing resources once the connection is closed. Each TCP connection goes through a series of states such as <code>Listen</code>, <code>SYN-SENT</code>, <code>SYN-RECEIVED</code>, <code>ESTABLISHED</code>, <code>FIN-WAIT</code>, <code>CLOSE-WAIT</code>, <code>CLOSING</code>, <code>LAST-ACK</code>, <code>TIME-WAIT</code>, and <code>CLOSED</code>. The different states help in managing the TCP messages.</p> <p>TCP transmits data in an ordered and reliable fashion, thus guaranteeing delivery. It does this by first establishing a three-way handshake to synchronize the sequence number between the transmitter and the receiver, <code>SYN</code>, <code>SYN-ACK</code>, and <code>ACK</code>. The acknowledgment is used to keep track of subsequent segments in the conversation. Finally, at the end of the conversation, one side will send a <code>FIN</code> message, and the other side will <code>ACK</code>.</p>"},{"location":"networking/#user-datagram-protocol-udp","title":"User datagram protocol (UDP)","text":"<p>Unlike TCP, the header is only 64 bits, which only consists of a source and destination port, length, and checksum. The lightweight header makes it ideal for applications that prefer faster data delivery without setting up the session between two hosts or needing reliable data delivery. Perhaps it\u2019s hard to imagine with today\u2019s fast internet connections, but the lightweight header made a big difference to the speed of transmission in the early days of <code>X.21</code> and frame relay links. Also, not having to maintain various states, such as TCP, also saves computer resources.</p> <p>Multimedia video streaming benefits from a lighter header when the application just wants to deliver the datagram as quickly as possible. The fast Domain Name System (DNS) lookup process is also based on the UDP protocol. The tradeoff between accuracy and latency usually tips to the side of low latency.</p>"},{"location":"networking/#internet-protocol","title":"Internet protocol","text":""},{"location":"networking/#topics","title":"Topics","text":"<ul> <li>Local Area Network (LAN)</li> <li>Internet Service Provider (ISP)</li> <li>Internet of Things (IoT)</li> <li>Internet Protocol (IP)</li> <li>Dense Wavelength Division Multiplexing (DWDM)</li> <li>OSI Model</li> <li>Infrastructure as a Service (IaaS)</li> <li>Software-Defined Wide-Area-Networks (SD-WANs)</li> <li>International Organization for Standardization (ISO), now as Telecommunication Standardization Sector of the International Telecommunication Union (ITU-T)</li> <li>Advanced Research Projects Agency Network (ARPANET)</li> <li>Internet Assigned Numbers Authority (IANA)</li> </ul>"},{"location":"networking/#references","title":"References","text":"<ul> <li>TCP Guide</li> <li>UDP Protocol</li> <li>Understanding sockets concepts | IBM</li> <li>The difference between a Port and a Socket</li> </ul>"},{"location":"networking/components/","title":"Network Components","text":""},{"location":"networking/components/#ip-address","title":"IP Address","text":"<p>Internet Protocol address (IP address) is a unique string of numbers separated by periods that identifies each computer using the Internet Protocol to communicate over a network. (example, 255.255.255.255). There are two versions of IP addresses used today 32-bit (IPV4) or 128-bit (IPV6).</p> <p>The IP address space is managed globally by the Internet Assigned Numbers Authority (IANA), and by five regional Internet registries (RIRs) responsible in their designated territories for assignment to end users and local Internet registries, such as Internet service providers. IPv4 addresses have been distributed by IANA to the RIRs in blocks of approximately 16.8 million addresses each. Each ISP or private network administrator assigns an IP address to each device connected to its network. Such assignments may be on a static (fixed or permanent) or dynamic basis, depending on its software and practices.</p> <p>IP address has two principal functions</p> <ol> <li>host or network interface identification</li> <li>location addressing</li> </ol>"},{"location":"networking/components/#subnetsubnetwork","title":"Subnet/Subnetwork","text":"<p>A logical subdivision of an IP network.</p>"},{"location":"networking/components/#mac-address","title":"Mac Address","text":"<p>A unique identifier assigned to network interface controllers for communication.</p>"},{"location":"networking/components/#hardware","title":"Hardware","text":""},{"location":"networking/components/#modem","title":"Modem","text":"<p>Connects ISP to your home, translating analog data to digital</p>"},{"location":"networking/components/#router","title":"Router","text":"<p>Controls the flow of digital data between devices. tA router creates a local network for devices to communicate, just like appartment. These are the cheapest and fastest way to connect to the internet. Have their own operating system called firmware, where default has limited functionality since most want plug-n-play.</p>"},{"location":"networking/components/#switch","title":"Switch","text":"<p>Box that turns one ethernet connection into several, allowing multiple wired devices to connect to internet without overloading the router.</p>"},{"location":"networking/home-network______TBD/","title":"Home Network","text":"<p>A network with many devices becomes taxing on the router.</p> <p>Some core principles:</p> <ul> <li>Use wires over Wi-Fi as much as possible</li> <li>Eliminate bottlenecks (device is capable of handling the speed)</li> <li>Use switches to minimize traffic hitting the router</li> <li>Use a third-party firmware on your router to manually prioritize devices/services.</li> </ul>"},{"location":"networking/home-network______TBD/#performance","title":"Performance","text":"<pre><code>https://broadbandnow.com/report/design-supercharged-home-network/\n    https://www.howtogeek.com/51477/how-to-remove-advertisements-with-pixelserv-on-dd-wrt/\nhttps://broadbandnow.com/report/optimize-network-faster-speeds/\nhttps://www.blackhillsinfosec.com/home-network-design-part-1/\n    https://www.troyhunt.com/ubiquiti-all-the-things-how-i-finally-fixed-my-dodgy-wifi/\n    https://www.troyhunt.com/wiring-a-home-network-from-the-ground-up-with-ubiquiti/\nhttps://www.lifewire.com/home-network-diagrams-4064053\n</code></pre>"},{"location":"networking/home-network______TBD/#cables","title":"Cables","text":"<p>Cat5e/Cat6/Cat6e/Cat7</p> <pre><code>https://planetechusa.com/blog/ethernet-different-ethernet-categories-cat3-vs-cat5e-vs-cat6-vs-cat6a-vs-cat7-vs-cat8/\nhttps://www.cablewholesale.com/support/technical_articles/megahertz_madness.php\nhttps://www.ebay.com/gds/What-are-the-Different-Types-of-Ethernet-Cables-/10000000177629325/g.html\nhttps://www.howtogeek.com/70494/what-kind-of-ethernet-cat-5e6a-cable-should-i-use/\nhttp://www.flukenetworks.com/knowledge-base/applicationstandards-articles-copper/mhz-vs-mbits-and-encoding\n</code></pre>"},{"location":"networking/home-network______TBD/#useful-links","title":"Useful Links","text":"<pre><code>http://www.practicallynetworked.com/\nhttps://hackaday.io/project/20063-flashing-forceware-on-sb6141\nhttps://www.youtube.com/watch?v=7iSC9yXxVVc\n</code></pre>"},{"location":"networking/home-network______TBD/#network-stories","title":"Network Stories","text":"<pre><code>https://www.youtube.com/watch?v=dIFKmJ4wufc            # Eli the computer guy\n    https://www.youtube.com/watch?v=9yYqNqTNnqI            # Switches\n    https://www.youtube.com/watch?v=q4P4BjjXghQ            # VPN\n    https://www.youtube.com/watch?v=rL8RSFQG8do&amp;index=1&amp;list=PLF360ED1082F6F2A5\nhttps://www.youtube.com/watch?v=RGVPeB98zWI\n    https://www.youtube.com/watch?v=QuV__5P4Bxw\nhttps://www.youtube.com/watch?v=8J00rfZE17Y\nhttps://www.youtube.com/watch?v=00UTYN9j0FE\nhttps://www.youtube.com/watch?v=bUY7-tsdSsU            # My Home Network Diagram Explained\nhttps://www.youtube.com/watch?v=TNpEIqq77Fk            # Home Networking 101\nhttps://www.youtube.com/watch?v=saD_SFOYCWk            # Building a simple network\nhttps://www.youtube.com/watch?v=j41JveqPUQM            # Building a smart network\nhttps://www.youtube.com/watch?v=2wtBeeZyxqs            # Tour of home network\n    https://www.youtube.com/watch?v=PR7GBvc1zw0&amp;list=PLy4Ry1Cuuzt7BB2u023ZsKKE2WN_zsoYD    # Playlist\n\nhttps://www.youtube.com/watch?v=GagY5EeYFzs            # NAS\n    https://www.youtube.com/watch?v=tdSCwTChR48\n    https://www.bhphotovideo.com/c/product/1364346-REG/synology_ds418_diskstation_4_bay_nas.html\n    https://www.bhphotovideo.com/c/product/1242542-REG/synology_diskstation_ds416slim_4_bay_nas.html\n    https://www.bhphotovideo.com/c/product/1333824-REG/synology_ds1817_2gb_ds1817_8_bay_nas.html\nhttps://www.youtube.com/watch?v=SnLkrwMMxGA            # TechWiser - NAS\n\nhttps://www.youtube.com/watch?v=_RlxCc0M6Ko            # Home Media Server - Techquickie\n</code></pre>"},{"location":"networking/home-network______TBD/#network-design","title":"Network Design","text":"<pre><code>https://www.youtube.com/watch?v=J5QJb3O19zI            # Connect two routers\nhttps://www.youtube.com/watch?v=UXKZi6me-ew            # How to wire a home\n</code></pre>"},{"location":"networking/home-network______TBD/#network-monitoring","title":"Network Monitoring","text":"<pre><code>https://www.youtube.com/watch?v=WAw6w6IMRyM            # TechWiser\n    https://www.youtube.com/watch?v=AeGdWbUxYug            # Secure yourself on public WiFi\n    https://www.youtube.com/watch?v=NjwQPWDT9KY\n    https://www.youtube.com/watch?v=cCYU5EdlPtg            # Kick people off your internet\nhttps://www.youtube.com/watch?v=yLbKhwnc0HY            # Gadget Addict\n</code></pre>"},{"location":"networking/home-network______TBD/#network-troubleshooting","title":"Network Troubleshooting","text":"<pre><code>https://www.youtube.com/watch?v=5Tke_JD7-jk            # HackAttack\nhttps://www.youtube.com/watch?v=AimCNTzDlVo            # Sakitech\nhttps://www.youtube.com/watch?v=h43fmazlkoI            # Professor Messer\nhttps://www.youtube.com/watch?v=b-hXJ5xxk-M\nhttps://www.youtube.com/watch?v=PVOuff-p420            # Basic Linux Network troubleshooting\nhttps://www.youtube.com/watch?v=75lCgcXP4dc            # Basic Linux Network troubleshooting\n\nhttps://www.youtube.com/watch?v=IHS7LLFvDFE            # Cisco Network Troubleshooting\n    https://www.youtube.com/watch?v=onfPC19eFYg\n\nhttp://www.nirsoft.net/network_tools.html\n</code></pre>"},{"location":"networking/home-network______TBD/#vpn","title":"VPN","text":"<p>VPN creates a secure, encrypted connection, which can be thought of as a tunnel, and no one (even your ISP) can see your traffic until it exits the tunnel</p> <pre><code>https://www.youtube.com/watch?v=xGjGQ24cXAY            # Android Authority\n</code></pre>"},{"location":"networking/home-network______TBD/#security","title":"Security","text":"<pre><code>https://www.youtube.com/watch?v=ciNHn38EyRc            # Computerphile\n</code></pre>"},{"location":"networking/home-network______TBD/#captive-portal","title":"Captive Portal","text":"<pre><code>https://www.youtube.com/watch?v=LA4dowjsn1s\n</code></pre>"},{"location":"networking/home-network______TBD/#aws","title":"AWS","text":"<pre><code>https://www.youtube.com/watch?v=N89AffsxS-g            # Eli the computer guy\nhttps://www.youtube.com/watch?v=IT1X42D1KeA            # Edureka\n    https://www.youtube.com/watch?v=IT1X42D1KeA&amp;list=PL9ooVrP1hQOFWxRJcGdCot7AgJu29SVV3&amp;index=1\n</code></pre>"},{"location":"networking/home-network______TBD/#what-to-think-about-when-designing-own-home-network","title":"What to think about when designing own home network?","text":"<pre><code>Wall mount rack\nDedicated Space\nBackup power supply (UPS)\nCache drive\n\nHome Automation\n</code></pre>"},{"location":"networking/internet/","title":"Internet","text":"<p>The Internet is a global computer network consisting of a web of inter-networks connecting large and small networks together.</p> <p><code>Hosts</code> are end nodes/devices on the network that communicate with other nodes. All nodes need to have an address, be routed, and be managed. These nodes request for services online, and the services are provided by <code>servers</code>.</p> <p>The <code>network components</code> that connect nodes with servers fall in layer one to layer three of the 7-layer <code>OSI model</code>, sometimes even layer four. They are layer-two and layer-three routers and switches that direct traffic, as well as layer-one transports such as fiber optic cables, coaxial cables, twisted copper pairs, and some Dense Wavelength Division Multiplexing (DWDM) equipment, to name a few.</p> <p>Collectively, hosts, servers, storage, and network components make up the internet.</p>"},{"location":"networking/internet/#internet-service-provider-isp","title":"Internet Service Provider (ISP)","text":"<p>The ISP lets you get online. They do this by aggregating small networks into bigger networks that they maintain. Your ISP network often consists of many <code>edge nodes</code> that aggregate the traffic to their <code>core network</code>. The core network\u2019s function is to interconnect these edge networks via a high-speed network.</p> <p>At some of the more specialized edge nodes, called <code>Internet exchange points</code>, your ISP is connected to other ISPs to pass your traffic appropriately to your destination. The return path from your destination to your home device may or may not follow the same path through all these in-between networks back to your original device, while the source and destination remain the same. This asymmetrical behavior is designed to be <code>fault-tolerant</code> so that no one node can take down the whole connection.</p>"},{"location":"networking/internet/#data-centers","title":"Data Centers","text":"<p>Because of the higher hardware capacity that servers demand, all required network components are often put together in a central location to be managed more efficiently. We often refer to these locations as data centers. They can generally be classified into three broad categories:</p> <ul> <li>Enterprise data centers</li> <li>Cloud data centers</li> <li>Edge data centers</li> </ul> <p>Cloud data centers are so big and power-hungry that they are typically built close to power plants where they can get the cheapest power rate, without losing too much efficiency during the transportation of the power. Their cooling needs are so high that some are forced to be creative about where the data center is built. A typical network design would be a multi-staged Clos network.</p> <p>The most significant limitation in routing the request and session all the way back from the client to a large data center is the latency introduced in the transport. Significant latency is where the network becomes a bottleneck. We can try to be as closely connected to the end-user as possible, perhaps meeting the user at the edge where the requests enter our network. We can place enough resources at these edge locations to serve the request.</p> <p>This is especially common for servicing media content such as music and videos, by placing the media either inside or very near to the customer\u2019s ISP. Also, for redundancy and connection speed, the upstreaming of the video server farm would not just be connected to one or two ISPs, but all the ISPs that we can connect to reduce the hop count, thus reducing the number of devices we need to pass thru. All the connections would have as much bandwidth as needed to decrease peak-hour latency. This need gave rise to the peering exchange\u2019s edge data centers of big ISP and content providers.</p> <p>Self-driving cars and Software-Defined Wide-Area-Networks (SD-WANs) are also applications of edge nodes. The self-driving car needs to make split-second decisions based on its sensors. SD-WAN routers need to route packets locally, without the need to consult a central \u201cbrain.\u201d These are all concepts of intelligent edge nodes.</p>"},{"location":"networking/internet/#the-osi-model","title":"The OSI Model","text":"<p>Networking breaks the complexity by using layers to model the functions of its elements. OSI is a conceptual model that componentizes the telecommunication functions into different layers. The model defines seven layers, and each layer sits independently on top of another one with defined structures and characteristics.</p> <ul> <li>Layer 1 (Physical)    - Sends data on to the physical wire</li> <li>Layer 2 (Data Link)   - Reads the MAC address from the data packet</li> <li>Layer 3 (Network)     - Reads the IP address from the packet</li> <li>Layer 4 (Transport)   - Responsible for the transport protocol (e.g., TCP, UDP) and error handling</li> <li>Layer 5 (Session)     - Establishes/ends connection between two hosts (e.g., NetBIOS, PPTP)</li> <li>Layer 6 (Presentation) - Formats the data so it can be viewed by the user (JPG, HTTPS, SSL, TLS). Also encrypts and decrypts.</li> <li>Layer 7 (Application) - Services that are used with end-user applications (like SMTP)</li> </ul>"},{"location":"networking/internet/#references","title":"References","text":"<ul> <li>Book: Mastering Python Networking</li> <li>TCP Guide</li> <li>UDP Protocol</li> <li>OSI Model<ul> <li>Image | O'Reilly</li> <li>An OSI Model for Cloud | Cisco</li> <li>OSI Model Reference Guide | Livewire</li> <li>The OSI Model &amp; It\u2019s Importance to Cloud Engineers | Medium</li> <li>What is OSI Model? | AWS </li> </ul> </li> </ul>"},{"location":"networking/network-protocols/","title":"Network Protocols","text":"<p>There are three main types of network protocols:</p> <ol> <li>Communication protocols include basic data communication tools like <code>TCP/IP</code> and <code>HTTP</code>.</li> <li>Management protocols maintain and govern the network through protocols such as <code>ICMP</code> and <code>SNMP</code>.</li> <li>Security protocols include <code>HTTPS</code>, <code>SFTP</code>, and <code>SSL</code>.</li> </ol>"},{"location":"networking/network-protocols/#communication-protocols","title":"Communication Protocols","text":"<p>These protocols formally describe the formats and rules by which data is transferred over the network. This is a must-have for exchanging messages between your computing systems and in telecommunications, applying to both hardware and software. Communication protocols also handle authentication and error detection as well as the syntax, synchronization and semantics that both analog and digital communications must abide by to function.</p> <p><code>HTTP</code> \u2013 One of the most familiar protocols, hyper text transfer protocol (HTTP) is often referred to as the protocol of the internet. HTTP is an application layer protocol that allows the browser and server to communicate.</p> <p><code>TCP</code> \u2013 Transmission Control Protocol (TCP) separates data into packets that can be shared over a network. These packets can then be sent by devices like switches and routers to the designated targets.</p> <p><code>UDP</code> \u2013 User Datagram Protocol (UDP) works in a similar way to TCP, sending packets of data over the network. The key difference between the two is that TCP ensures a connection is made between the application and server, but UDP does not.</p> <p><code>IRC</code> \u2013 Internet Relay Chat (IRC) is a text-based communication protocol. Software clients are used to communicate with servers and send messages to other clients. This protocol works well on networks with a large number of distributed machines.</p>"},{"location":"networking/network-protocols/#management-protocols","title":"Management Protocols","text":"<p>Network management protocols help define the policies and procedures used to monitor, manage and maintain your computer network, and help communicate these needs across the network to ensure stable communication and optimal performance across the board.</p> <p><code>SNMP</code> \u2013 Simple Network Management Protocol (SNMP) is used to monitor and manage network devices. This TCP-based protocol allows administrators to view and modify endpoint information to alter behavior of devices across the network. SNMP relies on the use of agents to collect and send data to an overarching SMNP manager, which in turn queries agents and gets their responses.</p> <p><code>ICMP</code> \u2013 Internet Control Message Protocol (ICMP) is primarily used for diagnostic purposes. Managed devices on the network can use this protocol to send error messages, providing information regarding network connectivity issues between devices.</p>"},{"location":"networking/network-protocols/#security-protocols","title":"Security Protocols","text":"<p><code>SSL</code> \u2013 A Secure Socket Layer (SSL) is a network security protocol primarily used for ensuring secure internet connections and protecting sensitive data. This protocol can allow for server/client communication as well as server/server communication. Data transferred with SSL is encrypted to prevent it from being readable.</p> <p><code>SFTP</code> \u2013 Secure File Transfer Protocol (SFTP), as its name might suggest, is used to securely transfer files across a network. Data is encrypted and the client and server are authenticated.</p> <p><code>HTTPS</code> \u2013 Secure Hypertext Transfer Protocol is the secure version of HTTP. Data sent between the browser and server are encrypted to ensure protection.</p>"},{"location":"networking/network-protocols/#references","title":"References","text":"<ul> <li>https://www.cdw.com/content/cdw/en/articles/networking/types-of-network-protocols.html</li> </ul>"},{"location":"operating-system/","title":"Operating System","text":""},{"location":"operating-system/#references","title":"References","text":"<ul> <li>Process (computing)</li> <li>Core dump</li> </ul>"},{"location":"system-design-fundamentals/","title":"System Design Fundamentals","text":"<p>System design is a broad and complex field that encompasses many topics. Here are some fundamental topics:</p>"},{"location":"system-design-fundamentals/#1-scalability","title":"1. Scalability","text":"<ul> <li>Horizontal vs. Vertical Scaling: Understanding the differences and when to use each.</li> <li>Load Balancing: Techniques and tools to distribute traffic across servers (e.g., Round Robin, Least Connections, IP Hash).</li> <li>Sharding: Dividing a database into smaller, more manageable pieces.</li> <li>Partitioning: Techniques to split data across different databases or storage systems.</li> </ul>"},{"location":"system-design-fundamentals/#2-reliability","title":"2. Reliability","text":"<ul> <li>Redundancy: Using redundant components to eliminate single points of failure.</li> <li>Replication: Techniques for replicating data across multiple servers to ensure availability.</li> <li>Failover: Mechanisms to switch to a standby server in case of failure.</li> <li>Backup and Recovery: Strategies for backing up data and recovering from failures.</li> </ul>"},{"location":"system-design-fundamentals/#3-performance","title":"3. Performance","text":"<ul> <li>Caching: Implementing caches to speed up data retrieval (in-memory, distributed).</li> <li>Database Indexing: Using indexes to speed up database queries.</li> <li>Asynchronous Processing: Using message queues and background processing to handle long-running tasks.</li> <li>Content Delivery Networks (CDNs): Using CDNs to deliver content more quickly to users around the world.</li> </ul>"},{"location":"system-design-fundamentals/#4-data-management","title":"4. Data Management","text":"<ul> <li>Database Design: Choosing between SQL and NoSQL, understanding normalization and denormalization.</li> <li>Data Consistency: Implementing consistency models like eventual consistency, strong consistency, and CAP theorem.</li> <li>Data Storage: Techniques for storing and managing data (e.g., file storage, object storage, block storage).</li> </ul>"},{"location":"system-design-fundamentals/#5-security","title":"5. Security","text":"<ul> <li>Authentication and Authorization: Implementing secure access control mechanisms.</li> <li>Encryption: Encrypting data at rest and in transit.</li> <li>Securing APIs: Best practices for securing REST and GraphQL APIs.</li> <li>Network Security: Using firewalls, VPNs, and secure network protocols.</li> </ul>"},{"location":"system-design-fundamentals/#6-distributed-systems","title":"6. Distributed Systems","text":"<ul> <li>Microservices: Designing applications as a collection of loosely coupled services.</li> <li>Service Discovery: Mechanisms for services to find each other in a distributed system.</li> <li>Event-Driven Architecture: Using events to communicate between services.</li> <li>Consensus Algorithms: Implementing consensus in distributed systems (e.g., Paxos, Raft).</li> </ul>"},{"location":"system-design-fundamentals/#7-communication","title":"7. Communication","text":"<ul> <li>HTTP/HTTPS: Basics of web protocols.</li> <li>WebSockets: Real-time communication protocols.</li> <li>Message Queues: Using queues for asynchronous communication (e.g., RabbitMQ, Kafka).</li> <li>gRPC: A high-performance RPC framework.</li> </ul>"},{"location":"system-design-fundamentals/#8-monitoring-and-logging","title":"8. Monitoring and Logging","text":"<ul> <li>Metrics: Collecting and analyzing performance metrics.</li> <li>Logging: Best practices for logging and log management.</li> <li>Alerting: Setting up alerts for monitoring system health.</li> <li>Tracing: Using distributed tracing to diagnose issues in microservices.</li> </ul>"},{"location":"system-design-fundamentals/#9-devops-and-automation","title":"9. DevOps and Automation","text":"<ul> <li>CI/CD Pipelines: Implementing continuous integration and continuous deployment.</li> <li>Infrastructure as Code: Managing infrastructure with code (e.g., Terraform, Ansible).</li> <li>Containerization: Using containers to package applications (e.g., Docker, Kubernetes).</li> <li>Deployment Strategies: Techniques for deploying updates (e.g., blue-green deployments, canary releases).</li> </ul>"},{"location":"system-design-fundamentals/#10-design-patterns","title":"10. Design Patterns","text":"<ul> <li>Singleton: Ensuring a class has only one instance.</li> <li>Factory: Creating objects without specifying the exact class.</li> <li>Observer: Allowing objects to be notified of changes.</li> <li>Circuit Breaker: Preventing calls to a failing service to allow it to recover.</li> </ul>"},{"location":"system-design-fundamentals/#11-api-design","title":"11. API Design","text":"<ul> <li>RESTful Services: Designing REST APIs using best practices.</li> <li>GraphQL: Designing APIs with GraphQL.</li> <li>Rate Limiting: Protecting APIs from abuse by limiting the number of requests.</li> </ul>"},{"location":"system-design-fundamentals/#12-networking-basics","title":"12. Networking Basics","text":"<ul> <li>IP Addressing: Understanding IPv4 and IPv6.</li> <li>DNS: Domain Name System basics and caching.</li> <li>Load Balancers: Types and strategies.</li> </ul>"},{"location":"system-design-fundamentals/#13-concurrency-and-parallelism","title":"13. Concurrency and Parallelism","text":"<ul> <li>Threads and Processes: Understanding the difference and use cases.</li> <li>Synchronization: Techniques to avoid race conditions (e.g., mutexes, semaphores).</li> <li>Asynchronous Programming: Using async/await, futures, and promises.</li> </ul>"},{"location":"system-design-fundamentals/#14-storage-systems","title":"14. Storage Systems","text":"<ul> <li>File Systems: Basics of file storage.</li> <li>Block Storage: Used for databases and virtual machines.</li> <li>Object Storage: Used for unstructured data (e.g., AWS S3).</li> </ul>"},{"location":"system-design-fundamentals/#15-api-gateways","title":"15. API Gateways","text":"<ul> <li>Functions: Routing, rate limiting, security.</li> <li>Tools: Implementing with tools like Kong, NGINX.</li> </ul> <p>Understanding these fundamental topics will help in designing scalable, reliable, and efficient systems. Each topic has its own set of best practices, tools, and techniques, which are crucial for building robust applications.</p>"},{"location":"system-design-fundamentals/#additional-topics-to-cover","title":"Additional topics to cover","text":"<ul> <li>API gateway</li> <li>Consistent Hashing</li> <li>Map Reduce</li> <li>Quorum</li> <li>GeoHash, Quad Tree</li> <li>Leaky bucket, Token bucket</li> <li>federation, denormalization</li> <li>Message broker, message queue, pub-sub</li> <li>Saga pattern, event sourcing, lambda arch, kappa arch, ddd</li> </ul>"},{"location":"system-design-fundamentals/dynamodb/","title":"DynamoDB","text":""},{"location":"system-design-fundamentals/dynamodb/#references","title":"References","text":"<ul> <li>DynamoDB Guide</li> <li></li> </ul>"},{"location":"system-design-fundamentals/protobuf/","title":"Protobuf","text":""},{"location":"system-design-fundamentals/protobuf/#references","title":"References","text":"<ul> <li>https://protobuf.dev/reference/protobuf/google.protobuf/</li> <li>https://docs.solo.io/gloo-edge/1.7.23/reference/api/github.com/solo-io/protoc-gen-ext/external/google/protobuf/wrappers.proto.sk/</li> </ul>"},{"location":"system-design-fundamentals/quorum/","title":"Quorum","text":"<p>A quorum is a concept commonly used in distributed systems to ensure consistency, fault tolerance, and reliable decision-making in the presence of network partitions or node failures. In distributed databases and consensus algorithms, a quorum represents the minimum number of votes or acknowledgments needed for an operation to be considered successful or for a system to make progress. The idea is to have a sufficient number of nodes agree on a decision to ensure the integrity and correctness of the distributed system.</p> <p>Here are a few common scenarios where the concept of quorum is applied:</p> <ol> <li>Read and Write Operations:</li> <li>In distributed databases, especially those using techniques like eventual consistency or consensus algorithms, a quorum is often involved in read and write operations.</li> <li> <p>For example, in a system with replication, a write may be considered successful only when a certain number of replicas acknowledge the write.</p> </li> <li> <p>Consensus Algorithms:</p> </li> <li>In consensus algorithms like Paxos or Raft, a quorum is used to determine when a decision or agreement has been reached.</li> <li> <p>For instance, in Paxos, a majority of nodes must agree on a value for it to be chosen as the consensus value.</p> </li> <li> <p>Voting in Fault-Tolerant Systems:</p> </li> <li>In distributed systems designed for fault tolerance, such as a replicated file system or a distributed key-value store, a quorum is used to ensure that a certain number of replicas are available and operational.</li> <li> <p>For example, a system might be configured to continue operation as long as a majority of nodes are functioning.</p> </li> <li> <p>Avoiding Split-Brain Scenarios:</p> </li> <li>Quorums are also employed to prevent split-brain scenarios, where network partitions can cause parts of the system to operate independently.</li> <li>By requiring a quorum for certain operations, the system ensures that at least a majority of nodes agree on the state of the system, reducing the risk of inconsistent data.</li> </ol> <p>The concept of quorum is often expressed in terms of a fraction or a majority of nodes. For example, if a system has 5 nodes, a quorum might be defined as 3 nodes. This means that as long as three nodes are available and agree on an operation, the system can make progress.</p> <p>Quorums play a crucial role in achieving consistency and fault tolerance in distributed systems by providing a robust mechanism for decision-making that can withstand failures and network partitions.</p>"},{"location":"system-design-fundamentals/quorum/#learn-more","title":"Learn more","text":"<p>Learning about quorum and its applications in distributed systems involves understanding consensus algorithms, distributed databases, and fault-tolerance mechanisms. Here are some resources to deepen your knowledge:</p> <ol> <li>Consensus Algorithms Papers:</li> <li> <p>Original papers on consensus algorithms provide in-depth insights into the principles behind quorum-based decision-making. For example:</p> <ul> <li>Paxos:<ul> <li>Title: \"The Part-Time Parliament\"</li> <li>Authors: Leslie Lamport</li> <li>Link to the paper</li> </ul> </li> <li>Raft:<ul> <li>Title: \"In Search of an Understandable Consensus Algorithm\"</li> <li>Authors: Diego Ongaro, John Ousterhout</li> <li>Link to the paper</li> </ul> </li> </ul> </li> <li> <p>Books on Distributed Systems:</p> </li> <li> <p>Books that cover distributed systems often include detailed explanations of quorum-based approaches. Recommended books include:</p> <ul> <li>\"Distributed Systems\" by Andrew S. Tanenbaum and Maarten van Steen.</li> <li>\"Designing Data-Intensive Applications\" by Martin Kleppmann.</li> </ul> </li> <li> <p>Online Articles and Tutorials:</p> </li> <li> <p>Numerous articles and tutorials provide accessible explanations of quorums, consensus, and distributed systems. Search for \"quorum in distributed systems\" or \"consensus algorithms explained\" on platforms like Medium, Towards Data Science, or other tech blogs.</p> </li> <li> <p>Consensus Algorithm Implementations:</p> </li> <li> <p>Exploring open-source implementations of consensus algorithms like Paxos and Raft can provide practical insights. You can find these implementations on GitHub and other code repositories.</p> </li> <li> <p>Lecture Videos and Online Courses:</p> </li> <li>Platforms like YouTube, Coursera, edX, and others may have lectures or courses on distributed systems that cover quorum-based approaches.</li> <li> <p>Search for relevant keywords such as \"consensus algorithms\" or \"distributed systems\" to find educational content.</p> </li> <li> <p>Documentation of Distributed Databases:</p> </li> <li>Documentation for distributed databases that utilize quorum-based techniques can be valuable. Examples include Apache Cassandra, Amazon DynamoDB, and other distributed databases.</li> </ol> <p>Remember to complement your theoretical knowledge with practical implementation and experimentation. Building a simple distributed system or consensus algorithm simulator will help solidify your understanding of quorum-based decision-making in distributed environments.</p>"},{"location":"system-design-fundamentals/retry-on-failure/","title":"Retry on failure","text":"<p>In distributed systems and networked applications, retry strategies are crucial for handling transient errors and network instability effectively.</p> <p>There are several strategies to handle retries when an operation fails. Here are a few common ones:</p> <ol> <li> <p>Simple Retry: In this strategy, the operation is simply retried after it fails. This is the most basic form of retry logic.</p> </li> <li> <p>Exponential Backoff: This is a more sophisticated form of retry logic where the time between retries increases exponentially. This can be useful to prevent overloading a system that might be struggling to handle requests.</p> </li> <li> <p>Incremental Backoff: Similar to exponential backoff, but the delay between retries increases incrementally.</p> </li> <li> <p>Randomized Exponential Backoff: This is a variant of exponential backoff where a random factor is introduced to prevent a phenomenon known as \"thundering herd problem\".</p> </li> <li> <p>Circuit Breaker: This is a design pattern used in modern software development which is used to detect failures and encapsulates the logic of preventing a failure from constantly recurring.</p> </li> <li> <p>Linear Backoff: This is a retry strategy where the delay between retries increases linearly. For example, if the initial delay is 2 seconds, after the first retry, the delay might increase to 4 seconds, then 6 seconds, and so on. This strategy is simpler than exponential backoff but can still help to prevent overloading a system that might be struggling to handle requests.  </p> </li> <li> <p>Linear Jitter Backoff: This is a variant of linear backoff where a random factor is introduced to the delay between retries. This can help to prevent a large number of operations from being retried at exactly the same time, which could potentially overload the system. The delay is usually calculated as a random value within a range that increases linearly.</p> </li> <li> <p>Exponential Jitter Backoff combines exponential backoff with a random <code>jitter</code> or variation. This strategy is used to spread out the retry attempts over time, which can help to prevent a large number of operations from being retried at exactly the same time, potentially overloading the system.</p> </li> </ol> <p>Exponential backoff (with or without jitter) is often used when you're dealing with temporary issues like network instability, while linear backoff might be more appropriate for rate limiting scenarios where you want to spread out your requests more evenly over time.</p>"},{"location":"system-design-fundamentals/retry-on-failure/#example","title":"Example","text":"<p>Here's an example of a simple retry mechanism in Python:</p> <pre><code>import time\nimport random\n\ndef retry(operation, attempts):\n    for _ in range(attempts):\n        try:\n            return operation()\n        except Exception as e:\n            print(f\"Exception caught: {e}, retrying...\")\n            time.sleep(1)  # wait for 1 second before retrying\n\ndef operation_that_can_fail():\n    # This is a placeholder for an operation that can fail\n    # In this example, it randomly raises an exception\n    if random.randint(0, 1) == 0:\n        raise Exception(\"Failed!\")\n    else:\n        return \"Success!\"\n\nretry(operation_that_can_fail, 5)\n</code></pre> <p>In this example, <code>retry</code> is a function that takes an operation and a number of attempts. It tries to execute the operation, and if it fails (an exception is raised), it waits for a second and then tries again, up to a maximum number of attempts.</p> <p>Remember, the best retry strategy depends on the specific use case and requirements of your application.</p>"},{"location":"system-design-fundamentals/retry-on-failure/#circuit-breaker","title":"Circuit Breaker","text":"<p>A circuit breaker is a design pattern used in software development that allows a system to handle failures gracefully and prevent a failure from constantly recurring. It's particularly useful when the system is interacting with a remote service or resource and there's a chance that calls to this service could fail, hang, or have high latency.</p> <p>Here's a simple way to implement a circuit breaker in Python:</p> <ol> <li>Define a <code>CircuitBreaker</code> class.</li> <li>The class should have a state, which can be \"CLOSED\", \"OPEN\", or \"HALF-OPEN\".</li> <li>In the \"CLOSED\" state, requests are allowed through. If a request fails, the failure count is incremented. If the failure count exceeds a certain threshold, the state is changed to \"OPEN\".</li> <li>In the \"OPEN\" state, all requests fail immediately without calling the remote service. After a certain timeout, the state is changed to \"HALF-OPEN\".</li> <li>In the \"HALF-OPEN\" state, a limited number of requests are allowed through. If those requests succeed, the state is changed back to \"CLOSED\". If any request fails, the state is changed back to \"OPEN\".</li> </ol> <p>Here's a simple implementation of a circuit breaker in Python:</p> <pre><code>import time\n\nclass CircuitBreaker:\n    def __init__(self, failure_threshold, recovery_timeout, retry_state_threshold):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.retry_state_threshold = retry_state_threshold\n        self.state = \"CLOSED\"\n        self.failure_count = 0\n        self.last_failure_time = None\n\n    def call(self, func, *args, **kwargs):\n        if self.state == \"OPEN\":\n            if time.time() - self.last_failure_time &gt; self.recovery_timeout:\n                self.state = \"HALF-OPEN\"\n                self.failure_count = 0\n            else:\n                raise Exception(\"Circuit is OPEN; calls are failing fast\")\n\n        try:\n            result = func(*args, **kwargs)\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            if self.failure_count &gt; self.failure_threshold:\n                self.state = \"OPEN\"\n            raise e\n\n        if self.state == \"HALF-OPEN\" and self.failure_count &lt; self.retry_state_threshold:\n            self.state = \"CLOSED\"\n\n        return result\n</code></pre> <p>In this example, <code>CircuitBreaker</code> is a class that wraps a function call. It maintains a count of consecutive failures (<code>failure_count</code>), and if this count exceeds a certain threshold (<code>failure_threshold</code>), it changes its state to \"OPEN\" and starts failing fast. After a certain timeout (<code>recovery_timeout</code>), it allows a limited number of requests through (\"HALF-OPEN\" state). If those requests succeed, it changes its state back to \"CLOSED\". If any request fails, it changes its state back to \"OPEN\".</p>"},{"location":"system-design-fundamentals/cache/","title":"Cache","text":""},{"location":"system-design-fundamentals/cache/#what-is-a-cache","title":"What is a Cache?","text":"<p>A cache is a high-speed data storage layer that stores a subset of data, typically transient, so that future requests for that data can be served faster than retrieving the same data from a primary storage location, which is usually slower. By temporarily storing copies of data, caches help to reduce data access latency, increase data retrieval speed, and improve the overall performance of applications and systems.</p>"},{"location":"system-design-fundamentals/cache/#key-concepts","title":"Key Concepts","text":"<ol> <li> <p>Latency Reduction: Caches reduce the time it takes to access data by keeping frequently accessed data closer to the application. This minimizes the need to retrieve data from slower storage mediums.</p> </li> <li> <p>Efficiency: By storing data that is repeatedly accessed, caches reduce the workload on the primary storage and the network, leading to more efficient resource utilization.</p> </li> <li> <p>Data Locality: Caches often leverage the principle of data locality, which states that data that was recently accessed or modified will likely be accessed or modified again in the near future.</p> </li> </ol>"},{"location":"system-design-fundamentals/cache/#how-caches-work","title":"How Caches Work","text":"<ul> <li> <p>Data Retrieval: When an application requests data, the cache is checked first. If the data is found in the cache (a cache hit), it is returned immediately. If the data is not found (a cache miss), it is fetched from the primary storage, stored in the cache for future requests, and then returned to the application.</p> </li> <li> <p>Data Invalidation: To ensure that the cache does not serve stale data, caches must invalidate or update entries based on certain rules or expiration times. This can be done using various invalidation strategies like TTL (Time-To-Live) or based on changes in the underlying data source.</p> </li> </ul>"},{"location":"system-design-fundamentals/cache/#types-of-caches","title":"Types of Caches","text":"<ol> <li> <p>Memory Cache: Stores data in the RAM of the server to provide fast access to frequently accessed data. Examples include in-memory databases like Redis and Memcached.</p> </li> <li> <p>Disk Cache: Uses a dedicated portion of disk storage to cache data. This type of cache is slower than memory cache but can store larger datasets. Examples include HTTP disk cache and page cache in operating systems.</p> </li> <li> <p>Database Cache: A cache layer specifically designed to store frequently accessed database query results, reducing the load on the database. Examples include query caching in MySQL or Oracle.</p> </li> <li> <p>Web Cache: Caches web content like HTML pages, images, and API responses to reduce latency and bandwidth usage. Examples include browser caches and Content Delivery Networks (CDNs) like Cloudflare and Akamai.</p> </li> </ol>"},{"location":"system-design-fundamentals/cache/#caching-strategies","title":"Caching Strategies","text":"<ol> <li> <p>Cache Aside (Lazy Loading): The application is responsible for loading data into the cache. If a cache miss occurs, the application loads the data from the primary source and then stores it in the cache.</p> </li> <li> <p>Read Through: The cache sits in front of the data source. When data is requested, the cache retrieves it from the data source on a miss and serves it to the application, also storing it in the cache.</p> </li> <li> <p>Write Through: Data is written to both the cache and the primary storage simultaneously, ensuring that the cache always has the most up-to-date data.</p> </li> <li> <p>Write Behind (Write Back): Data is written to the cache first and then asynchronously written to the primary storage, improving write performance but requiring careful management to ensure data consistency.</p> </li> <li> <p>Write-Around: Data is written directly to the primary storage, bypassing the cache. The cache is only populated when data is read, which can reduce cache pollution in write-heavy applications.</p> </li> </ol>"},{"location":"system-design-fundamentals/cache/#cache-eviction-strategies","title":"Cache Eviction Strategies","text":"<ol> <li> <p>Least Recently Used (LRU): Evicts the least recently accessed items first when the cache reaches its capacity.</p> </li> <li> <p>Most Recently Used (MRU): Evicts the most recently accessed items first, useful for certain access patterns where older items are more likely to be reused.</p> </li> <li> <p>Least Frequently Used (LFU): Evicts items that are accessed least frequently, keeping the more frequently accessed data in the cache.</p> </li> <li> <p>Time-To-Live (TTL): Each cache entry has an expiration time after which it is invalidated, ensuring that stale data is eventually purged.</p> </li> </ol>"},{"location":"system-design-fundamentals/cache/#benefits-of-caching","title":"Benefits of Caching","text":"<ul> <li>Improved Performance: Significantly reduces data access time, improving the responsiveness of applications.</li> <li>Reduced Load on Primary Storage: Decreases the number of direct requests to the primary data source, which can improve its performance and scalability.</li> <li>Cost Efficiency: Reduces bandwidth and resource usage, potentially lowering operational costs.</li> </ul>"},{"location":"system-design-fundamentals/cache/#challenges-of-caching","title":"Challenges of Caching","text":"<ul> <li>Data Consistency: Ensuring that the cache does not serve outdated or stale data requires careful invalidation and update strategies.</li> <li>Cache Management: Deciding what data to cache, how long to keep it, and when to evict it involves complex decision-making and can affect performance.</li> <li>Scalability: Managing caches in distributed systems can be challenging, especially when ensuring consistency and synchronization across multiple cache nodes.</li> </ul> <p>In summary, caching is a powerful technique for optimizing data access and improving the performance of applications. By understanding and implementing the appropriate caching and eviction strategies, you can significantly enhance the efficiency and scalability of your systems.</p>"},{"location":"system-design-fundamentals/cache/cache-eviction-strategies/","title":"Cache Eviction Strategies","text":"<p>Cache eviction strategies determine how the cache decides which items to remove when it reaches its capacity. These strategies help manage the limited space in the cache efficiently.</p>"},{"location":"system-design-fundamentals/cache/cache-eviction-strategies/#1-least-recently-used-lru","title":"1. Least Recently Used (LRU)","text":"<ul> <li>Description: The cache evicts the least recently used items when it reaches its maximum capacity. This strategy assumes that recently accessed data will likely be accessed again soon.</li> <li>Use Cases: General-purpose caching where usage patterns are unpredictable.</li> <li>Pros: Effective for a wide range of access patterns, straightforward to implement.</li> <li>Cons: May not be optimal for all scenarios, especially with irregular access patterns.</li> </ul>"},{"location":"system-design-fundamentals/cache/cache-eviction-strategies/#2-most-recently-used-mru","title":"2. Most Recently Used (MRU)","text":"<ul> <li>Description: The cache evicts the most recently used items first. This can be useful when the most recently accessed items are less likely to be accessed again.</li> <li>Use Cases: Specific scenarios where recent access indicates less future access.</li> <li>Pros: Useful for specific access patterns, such as stack algorithms.</li> <li>Cons: Not generally applicable, may lead to suboptimal caching for most use cases.</li> </ul>"},{"location":"system-design-fundamentals/cache/cache-eviction-strategies/#3-least-frequently-used-lfu","title":"3. Least Frequently Used (LFU)","text":"<ul> <li>Description: The cache evicts the least frequently accessed items. This strategy keeps items that are accessed more often.</li> <li>Use Cases: Scenarios where access frequency is a good indicator of future access.</li> <li>Pros: Retains frequently accessed data, potentially higher hit rates.</li> <li>Cons: Can be complex to implement and manage, may not adapt well to changing access patterns.</li> </ul>"},{"location":"system-design-fundamentals/cache/cache-eviction-strategies/#4-time-to-live-ttl","title":"4. Time-To-Live (TTL)","text":"<ul> <li>Description: Each cache entry has an expiration time after which the entry is invalidated. This ensures that stale data is eventually purged from the cache.</li> <li>Use Cases: Scenarios where data freshness is more important than avoiding cache misses.</li> <li>Pros: Simple to implement, ensures periodic refresh of data.</li> <li>Cons: Can lead to cache misses and increased load on the data source if TTL is too short.</li> </ul>"},{"location":"system-design-fundamentals/cache/cache-eviction-strategies/#5-hybrid-approaches","title":"5. Hybrid Approaches","text":"<ul> <li>Description: Combines multiple eviction strategies to leverage the benefits of each. For instance, using both LRU and LFU together.</li> <li>Use Cases: Complex systems with diverse caching requirements.</li> <li>Pros: Flexible and can be optimized for specific use cases.</li> <li>Cons: Increased complexity, requires fine-tuning and monitoring.</li> </ul>"},{"location":"system-design-fundamentals/cache/cache-eviction-strategies/#choosing-the-right-strategy","title":"Choosing the Right Strategy","text":"<p>Choosing the appropriate caching and eviction strategies depends on the specific requirements and characteristics of your application, such as read/write ratio, data volatility, access patterns, and consistency requirements. Often, a combination of these strategies is used to achieve the best performance and efficiency.</p> <p>By understanding and implementing the appropriate caching and eviction strategies, you can significantly enhance the performance and scalability of your applications, ensuring efficient use of resources and improved user experience.</p>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/","title":"Hybrid Caching Strategies","text":"<p>A hybrid caching strategy combines multiple caching techniques to leverage the strengths of each and mitigate their weaknesses. This can optimize performance, scalability, and data consistency for complex systems with diverse requirements. Here are some examples:</p>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#scenarios-for-hybrid-caching-strategies","title":"Scenarios for Hybrid Caching Strategies","text":""},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#1-high-read-write-workloads","title":"1. High Read-Write Workloads","text":"<ul> <li>Scenario: An e-commerce platform experiences high read-write workloads, especially during peak times like sales events.</li> <li>Hybrid Approach: </li> <li>Read Through for frequently accessed product details.</li> <li>Write Behind for handling large volumes of transaction logs and inventory updates.</li> <li>Benefits: Ensures quick access to product details while efficiently handling write operations without overloading the primary database.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#2-mixed-consistency-requirements","title":"2. Mixed Consistency Requirements","text":"<ul> <li>Scenario: A social media application where user profile data needs strong consistency, but post feeds can tolerate eventual consistency.</li> <li>Hybrid Approach:</li> <li>Write Through for user profile updates to maintain strong consistency.</li> <li>Write Around for post feeds to reduce cache pollution from frequent updates.</li> <li>Benefits: Guarantees consistent and up-to-date profile information while optimizing the cache for high read performance of post feeds.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#3-predictable-access-patterns-with-volatile-data","title":"3. Predictable Access Patterns with Volatile Data","text":"<ul> <li>Scenario: A news website with predictable access patterns for trending articles but with frequent updates.</li> <li>Hybrid Approach:</li> <li>Refresh Ahead for trending articles to keep them up-to-date.</li> <li>Cache Aside for less frequently accessed archives.</li> <li>Benefits: Ensures trending content is always fresh while managing the cache efficiently for less frequently accessed data.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#examples-of-hybrid-caching-strategies","title":"Examples of Hybrid Caching Strategies","text":""},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#1-web-content-delivery","title":"1. Web Content Delivery","text":"<ul> <li>Scenario: A content delivery network (CDN) that serves a large amount of static content (images, videos) and dynamic content (user-specific data).</li> <li>Hybrid Approach:</li> <li>Cache Aside for static content to reduce the load on the origin server and ensure content is cached only when needed.</li> <li>Read Through for dynamic content to ensure user-specific data is always retrieved from the cache if available.</li> <li>Benefits: Reduces latency for static content delivery while ensuring dynamic content is efficiently served.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#2-database-query-optimization","title":"2. Database Query Optimization","text":"<ul> <li>Scenario: A financial application that handles a mix of read-heavy queries (e.g., account balances) and write-heavy transactions (e.g., trade executions).</li> <li>Hybrid Approach:</li> <li>Read Through for caching account balances to reduce load on the database.</li> <li>Write Behind for logging transactions to maintain performance during high write activity.</li> <li>Benefits: Enhances read performance and manages write operations without causing significant delays.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#3-iot-data-management","title":"3. IoT Data Management","text":"<ul> <li>Scenario: An IoT platform where sensors generate a continuous stream of data, and historical data needs to be queried frequently.</li> <li>Hybrid Approach:</li> <li>Write Around for sensor data to avoid flooding the cache with transient data.</li> <li>Time-To-Live (TTL) for historical data to ensure it remains available in the cache for a defined period.</li> <li>Benefits: Optimizes the use of cache space while ensuring relevant historical data is readily accessible.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-hybrid-strategies/#implementation-considerations","title":"Implementation Considerations","text":"<ul> <li>Monitoring and Analytics: Use monitoring tools to analyze access patterns and performance metrics. This helps in fine-tuning the hybrid strategy for optimal results.</li> <li>Consistency and Latency Trade-offs: Balance the need for data consistency with acceptable latency levels for your application. Choose strategies that align with your consistency requirements.</li> <li>Scalability: Ensure the hybrid caching strategy can scale with the growth of your application. Test the strategy under different load conditions to verify its effectiveness.</li> </ul> <p>In conclusion, a hybrid caching strategy can be highly effective in optimizing performance for complex systems with diverse requirements. By combining multiple caching techniques, you can leverage their strengths to handle different access patterns and data consistency needs efficiently.</p>"},{"location":"system-design-fundamentals/cache/caching-strategies/","title":"Caching Strategies","text":"<p>Caching strategies focus on how and when data is loaded into the cache and how write operations are handled. These strategies ensure that data retrieval is efficient and that the cache remains coherent with the data source.</p> <p>Caching strategies are essential for improving the performance and efficiency of systems by temporarily storing copies of data. Here are some commonly used caching strategies:</p>"},{"location":"system-design-fundamentals/cache/caching-strategies/#1-cache-aside-lazy-loading","title":"1. Cache Aside (Lazy Loading)","text":"<ul> <li>Description: The application code is responsible for loading data into the cache. If a request for data is made and the data is not in the cache (cache miss), the application loads it from the data source and stores it in the cache for future requests.</li> <li>Use Cases: Read-heavy applications where data does not change frequently.</li> <li>Pros: Simple implementation, reduced load on the data source.</li> <li>Cons: Potential for stale data if the data source changes.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-strategies/#2-read-through","title":"2. Read Through","text":"<ul> <li>Description: The cache sits in front of the data source. When data is requested, the cache checks if the data is present. If not, it loads the data from the data source and returns it to the requester, simultaneously populating the cache.</li> <li>Use Cases: Systems where cache coherence with the data source is critical.</li> <li>Pros: Simplifies application logic, consistent data access.</li> <li>Cons: Can increase load on the data source during cache misses.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-strategies/#3-write-through","title":"3. Write Through","text":"<ul> <li>Description: Writes go to the cache and the data source simultaneously. This ensures that the cache is always in sync with the data source.</li> <li>Use Cases: Applications requiring strong consistency between the cache and the data source.</li> <li>Pros: Simple consistency model, cache is always up-to-date.</li> <li>Cons: Slower write operations due to dual writes.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-strategies/#4-write-behind-write-back","title":"4. Write Behind (Write Back)","text":"<ul> <li>Description: Writes are initially written to the cache and later asynchronously written to the data source. This reduces write latency but can risk data loss if the cache fails before the data is written back.</li> <li>Use Cases: Write-intensive applications where latency is critical, and eventual consistency is acceptable.</li> <li>Pros: Improved write performance, reduced write load on the data source.</li> <li>Cons: Risk of data loss, increased complexity for ensuring eventual consistency.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-strategies/#5-refresh-ahead","title":"5. Refresh Ahead","text":"<ul> <li>Description: The cache proactively refreshes data before it expires, ensuring that data is up-to-date when requested. This strategy is useful for scenarios where data is frequently accessed and must be current.</li> <li>Use Cases: Applications with predictable access patterns and a need for fresh data.</li> <li>Pros: Reduces cache misses, ensures data freshness.</li> <li>Cons: Increased load on the data source due to pre-fetching, potentially fetching unused data.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-strategies/#5-write-around","title":"5. Write-Around","text":"<ul> <li>Description: Writes data directly to the underlying data source, bypassing the cache. The data is fetched from the data source into the cache only when it is read.</li> <li>Use Cases: Write-heavy applications with infrequent reads, minimizing cache pollution.</li> <li>Pros: Reduced write load on the cache, better cache utilization.</li> <li>Cons: Increased read latency on cache misses, potential for cache inconsistency.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-strategies/#6-hybrid-approaches","title":"6. Hybrid Approaches","text":"<ul> <li>Description: Combines multiple caching strategies to leverage the benefits of each. For instance, using both LRU and LFU together.</li> <li>Use Cases: Complex systems with diverse caching requirements.</li> <li>Pros: Flexible and can be optimized for specific use cases.</li> <li>Cons: Increased complexity, requires fine-tuning and monitoring.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-strategies/#choosing-the-right-strategy","title":"Choosing the Right Strategy","text":"<p>Choosing the appropriate caching strategy depends on the specific requirements and characteristics of your application, such as read/write ratio, data volatility, access patterns, and consistency requirements. Often, a combination of these strategies is used to achieve the best performance and efficiency.</p>"},{"location":"system-design-fundamentals/cache/caching-write-around/","title":"Caching: Write Around","text":"<p>The write-around strategy involves writing data directly to the underlying data source, bypassing the cache. This means that the cache is not updated on write operations. Instead, the data is fetched from the data source into the cache only when it is read.</p>"},{"location":"system-design-fundamentals/cache/caching-write-around/#use-cases","title":"Use Cases","text":"<ul> <li>Write-Heavy Applications: Applications with a high volume of write operations but relatively infrequent reads.</li> <li>Minimizing Cache Pollution: Scenarios where written data is not likely to be read soon, thus avoiding unnecessary cache fills and evictions.</li> <li>Data Consistency: Situations where it is crucial to ensure that read operations always retrieve the most up-to-date data directly from the data source.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-write-around/#pros","title":"Pros","text":"<ul> <li>Reduced Write Load on Cache: Since write operations do not involve the cache, this reduces the load on the cache and helps prevent cache pollution with data that might not be read soon.</li> <li>Simplified Cache Management: Easier management of the cache since it only needs to handle read operations.</li> <li>Better Cache Utilization: The cache is populated with data that is actually read, which can lead to more efficient use of the cache space.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-write-around/#cons","title":"Cons","text":"<ul> <li>Increased Read Latency on Cache Misses: If the data is not in the cache and needs to be fetched from the data source, this can lead to higher latency for read operations.</li> <li>Potential for Cache Inconsistency: If the cache is not carefully managed, there might be inconsistencies between cached data and the underlying data source.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-write-around/#example-scenario","title":"Example Scenario","text":"<p>Consider a logging system where logs are continuously written but read infrequently:</p> <ol> <li>Write Operation: A new log entry is written directly to the persistent storage (e.g., a database) without updating the cache.</li> <li>Read Operation:</li> <li>If a log entry is requested, the system first checks the cache.</li> <li>If the entry is not found in the cache (cache miss), it fetches the entry from the database and then stores it in the cache for future reads.</li> </ol>"},{"location":"system-design-fundamentals/cache/caching-write-around/#comparison-with-other-strategies","title":"Comparison with Other Strategies","text":"<ul> <li>Write-Through vs. Write-Around:</li> <li>Write-through writes data to both the cache and the data source, ensuring that the cache is always up-to-date but increasing write latency.</li> <li> <p>Write-around bypasses the cache on writes, which can reduce write latency but might result in cache misses on subsequent reads.</p> </li> <li> <p>Write-Behind (Write-Back) vs. Write-Around:</p> </li> <li>Write-behind writes data to the cache first and asynchronously writes it to the data source, improving write performance but potentially risking data loss if the cache fails.</li> <li>Write-around avoids this risk by writing directly to the data source, but does not benefit from the write performance improvements of write-behind.</li> </ul>"},{"location":"system-design-fundamentals/cache/caching-write-around/#implementation-tips","title":"Implementation Tips","text":"<ul> <li>Cache Invalidation: Implement robust cache invalidation strategies to handle cases where the underlying data might change outside the cache\u2019s knowledge.</li> <li>Monitoring and Analytics: Use monitoring tools to analyze read and write patterns, helping to fine-tune the cache strategy for optimal performance.</li> <li>Hybrid Approach: Consider combining write-around with other strategies, such as read-through, to balance between write efficiency and read performance.</li> </ul> <p>In summary, the write-around caching strategy is useful for write-heavy applications with infrequent reads, helping to reduce the load on the cache and prevent cache pollution. However, it requires careful consideration of read latency and potential cache inconsistencies.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/","title":"Bloom filters","text":"<p>A Bloom filter is a space-efficient probabilistic data structure designed to test whether a given element is a member of a set. It provides a fast and memory-efficient way to perform set membership tests, but with a small probability of false positives.</p> <p>Here's how a Bloom filter works:</p> <ol> <li>Initialization:</li> <li>The Bloom filter is initialized with a fixed-size array of bits, typically all set to 0.</li> <li> <p>Also, it uses k independent hash functions (usually more than one) that generate hash values.</p> </li> <li> <p>Adding Elements:</p> </li> <li> <p>To add an element to the set, the element is hashed using each of the k hash functions, and the corresponding bits in the array are set to 1.</p> </li> <li> <p>Membership Test:</p> </li> <li>To check if an element is in the set, the element is hashed using each of the k hash functions.</li> <li> <p>If all the corresponding bits in the array are set to 1, the element is considered possibly in the set.</p> </li> <li> <p>False Positives:</p> </li> <li> <p>Due to the nature of hashing and the fixed-size array, there is a possibility of false positives. If an element hashes to the same bit positions as some other elements, it might incorrectly appear as a member of the set.</p> </li> <li> <p>False Negatives:</p> </li> <li>Bloom filters do not produce false negatives. If an element is not in the set, all the corresponding bits will be 0.</li> </ol> <p>Bloom filters are particularly useful in scenarios where memory is limited, and you can tolerate a small probability of false positives. Common use cases include:</p> <ul> <li> <p>Caching: To quickly check if a particular item is present in a cache before looking it up in a slower data store.</p> </li> <li> <p>Spell Checking: To quickly check if a word is in a dictionary.</p> </li> <li> <p>Network Routing Tables: To determine if a destination IP address is likely to be in a routing table.</p> </li> <li> <p>Duplicate Elimination: In databases or distributed systems, to quickly eliminate duplicate items.</p> </li> </ul> <p>However, it's essential to be aware of the trade-off involved. As more elements are added, the probability of false positives increases. The size of the Bloom filter and the number of hash functions used impact this trade-off, and careful consideration is needed to choose appropriate parameters for a given use case.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#multiple-hash-functions","title":"Multiple-hash functions","text":"<p>Multiple hash functions in a Bloom filter serve two important purposes: reducing the probability of collisions and distributing elements more uniformly across the filter's bits. Here's why multiple hash functions are beneficial:</p> <ol> <li> <p>Reducing Collisions:</p> <ul> <li>A hash function takes an input (an element to be inserted or queried) and produces a fixed-size hash value. With a single hash function, there's a possibility of collisions, where different elements map to the same hash value.</li> <li>By using multiple independent hash functions, the likelihood of different elements colliding at all hash positions is reduced. This helps in spreading out the bits that represent different elements, minimizing false positives.</li> </ul> </li> <li> <p>Distributing Elements Uniformly:</p> <ul> <li>If only a single hash function were used, the distribution of elements in the Bloom filter could be skewed. This could lead to certain bits being set more frequently than others, which may increase the probability of false positives.</li> <li>Multiple hash functions help distribute elements more uniformly across the bits of the filter, promoting a balanced distribution.</li> </ul> </li> <li> <p>Enhancing Independence:</p> <ul> <li>The effectiveness of a Bloom filter depends on the independence of the hash functions. Using multiple hash functions with diverse properties enhances this independence.</li> <li>Independence ensures that collisions for one hash function don't correlate strongly with collisions for another, providing a better approximation of randomness.</li> </ul> </li> <li> <p>Controlling False Positive Rate:</p> <ul> <li>The probability of a false positive in a Bloom filter is influenced by the number of hash functions used, the size of the filter, and the number of elements inserted.</li> <li>By using multiple hash functions, you can control and manage the false positive rate based on the desired trade-off between memory usage and accuracy.</li> </ul> </li> </ol> <p>In summary, multiple hash functions in a Bloom filter contribute to its effectiveness in terms of reducing collisions, promoting a more uniform distribution of elements, enhancing independence, and allowing for better control over the false positive rate. The choice of the number of hash functions depends on the desired level of accuracy and the trade-offs acceptable for the specific use case.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#number-of-hash-functions","title":"Number of hash functions","text":"<p>The number of hash functions needed in a Bloom filter depends on several factors, including the desired false positive rate, the size of the Bloom filter, and the number of elements to be inserted. The optimal number of hash functions can be determined by considering the following factors:</p> <ol> <li>False Positive Rate (FPR):</li> <li> <p>The false positive rate is the probability that a query incorrectly reports the presence of an element not in the set. It is influenced by the number of hash functions. As a general guideline, the false positive rate (FPR) can be estimated using the formula:      [ FPR = (1 - e^{-kn/m})^k ]    where:</p> <ul> <li>( k ) is the number of hash functions,</li> <li>( n ) is the number of elements in the set,</li> <li>( m ) is the number of bits in the Bloom filter.</li> </ul> </li> <li> <p>Desired Accuracy:</p> </li> <li> <p>The desired accuracy level plays a role in determining the number of hash functions. If a lower false positive rate is acceptable, fewer hash functions may be sufficient. Conversely, if a high level of accuracy is required, a higher number of hash functions might be necessary.</p> </li> <li> <p>Size of the Bloom Filter:</p> </li> <li> <p>The size of the Bloom filter, represented by the number of bits ( m ), impacts the accuracy. A larger filter provides more bits for distribution, potentially reducing collisions and improving accuracy. However, it also increases memory requirements.</p> </li> <li> <p>Empirical Testing:</p> </li> <li> <p>In some cases, practical experimentation with different numbers of hash functions may be necessary. Depending on the specific characteristics of the data and use case, you can observe the trade-offs between accuracy and memory usage.</p> </li> <li> <p>Rule of Thumb:</p> </li> <li>A common rule of thumb is to use approximately ( ( m / n ) * ln(2) ) hash functions, where ( m ) is the number of bits in the Bloom filter and ( n ) is the expected number of elements to be inserted.</li> </ol> <p>Keep in mind that the choice of the number of hash functions involves trade-offs. Using too few hash functions may result in a higher false positive rate, while using too many may increase computational overhead and memory usage.</p> <p>It's often beneficial to start with a reasonable estimate based on the factors mentioned above and then adjust through testing and analysis, considering the specific requirements and constraints of your application.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#hash-function-complexity-criteria","title":"Hash function complexity criteria","text":"<p>The complexity of hash functions in the context of a Bloom filter is typically evaluated in terms of time complexity and collision resistance. Here's how you can assess the complexity:</p> <ol> <li>Time Complexity:</li> <li>The time complexity of a hash function is evaluated based on the computation time required to produce the hash value for an input.</li> <li> <p>In practice, hash functions used in Bloom filters should have a constant or near-constant time complexity, meaning the time it takes to compute the hash is not significantly affected by the size of the input or the size of the Bloom filter.</p> </li> <li> <p>Collision Resistance:</p> </li> <li>Collision resistance is a crucial property of a hash function. A hash function is considered collision-resistant if it is computationally infeasible to find two different inputs that produce the same hash value.</li> <li> <p>In the context of a Bloom filter, collisions can lead to false positives. Therefore, the hash function should provide a high level of collision resistance.</p> </li> <li> <p>Independence:</p> </li> <li> <p>The independence of hash functions is important in the context of Bloom filters. Multiple hash functions are used to provide a more uniform distribution of elements across the filter. Each hash function should behave independently, meaning that the occurrence of collisions in one hash function should not significantly impact the likelihood of collisions in another.</p> </li> <li> <p>Hash Function Quality:</p> </li> <li> <p>The quality of a hash function is also assessed based on its ability to produce uniformly distributed hash values for a wide range of inputs. A good hash function minimizes clustering, where multiple inputs map to the same hash value, and provides a relatively even distribution across the hash space.</p> </li> <li> <p>Cryptographic Hash Functions (Optional):</p> </li> <li>While not always necessary for Bloom filters, if security is a concern, using cryptographic hash functions that meet specific security properties, such as being resistant to collision attacks, may be appropriate.</li> </ol> <p>Keep in mind that for Bloom filters, the primary focus is on fast computation and achieving a good distribution of hash values across the filter. Cryptographic hash functions, while providing strong collision resistance, may introduce unnecessary computational overhead.</p> <p>Overall, a good hash function for a Bloom filter should exhibit constant or near-constant time complexity, provide collision resistance, and produce uniformly distributed and independent hash values. The specifics of the chosen hash function depend on the requirements and constraints of the application.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#commonly-used-functions","title":"Commonly used functions","text":"<p>In the context of Bloom filters, commonly used hash functions are those that provide a good balance between computational efficiency, low collision probability, and a reasonably uniform distribution of hash values. Here are some examples of hash functions commonly used in practice:</p> <ol> <li> <p>MurmurHash:    MurmurHash is a non-cryptographic hash function known for its speed and good distribution properties. It comes in various versions, such as MurmurHash2 and MurmurHash3. MurmurHash3, in particular, is widely used for hash-based algorithms, including Bloom filters.</p> </li> <li> <p>Jenkins Hash Function:    The Jenkins hash functions, such as Bob Jenkins' one-at-a-time hash, are simple and fast non-cryptographic hash functions. They are often used in situations where speed is critical, making them suitable for Bloom filters.</p> </li> <li> <p>FNV-1a (Fowler-Noll-Vo):    FNV-1a is a non-cryptographic hash function known for its simplicity and ease of implementation. It is often used in scenarios where a quick and reasonably distributed hash function is needed.</p> </li> <li> <p>CityHash:    CityHash, developed by Google, is designed for hash-based algorithms and is known for its speed and good distribution properties. It's not cryptographic but is suitable for applications like Bloom filters.</p> </li> <li> <p>xxHash:    xxHash is a fast, non-cryptographic hash function known for its simplicity and speed. It is often used in situations where a quick hash function is needed, making it suitable for Bloom filters.</p> </li> <li> <p>SHA-256 (optional):    While cryptographic hash functions like SHA-256 can be used in Bloom filters, they are generally overkill unless security properties are a strict requirement. Cryptographic hash functions are computationally more expensive than non-cryptographic ones.</p> </li> </ol> <p>It's important to note that the choice of hash function may depend on the specific requirements of your application, including the desired trade-off between computational efficiency and hash distribution. The hash function should be suitable for the characteristics of the data and the constraints of the system where the Bloom filter is deployed. Additionally, the independence of multiple hash functions used in a Bloom filter is often more critical than the specific hash function chosen.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#filter-size","title":"Filter size","text":"<p>Calculating the needed size of a Bloom filter involves considering factors such as the expected number of elements to be inserted, the desired false positive rate, and the number of hash functions used. Here's a step-by-step guide:</p> <ol> <li> <p>Determine False Positive Rate (FPR):</p> <ul> <li>Decide on the acceptable false positive rate (FPR). The FPR is the probability that a query will incorrectly report the presence of an element not in the set.</li> </ul> </li> <li> <p>Choose Number of Hash Functions ( k ):</p> <ul> <li>Select the number of hash functions to be used in the Bloom filter. The number of hash functions affects the probability of false positives. Commonly used values are 2 or 3.</li> </ul> </li> <li> <p>Estimate the Number of Elements ( n ):</p> <ul> <li>Estimate the total number of elements you expect to insert into the Bloom filter.</li> </ul> </li> <li> <p>Calculate Bloom Filter Size ( m ):</p> <ul> <li>Use the formula to calculate the optimal size ( m ) of the Bloom filter:  [ m = - (n  * ln(FPR)) / (ln(2))^2 ]</li> <li>Where:<ul> <li>( n ) is the estimated number of elements.</li> <li>( FPR ) is the desired false positive rate.</li> </ul> </li> </ul> </li> <li> <p>Calculate Optimal Number of Bits Per Element ( b ):</p> <ul> <li>Use the formula to calculate the optimal number of bits per element ( b ):  [ b = m / n ]</li> <li>This represents the number of bits in the Bloom filter allocated per element.</li> </ul> </li> <li> <p>Round Up to the Nearest Integer:</p> <ul> <li>The calculated (m) and (b) may not be integers, but the Bloom filter size must be a whole number. Round up the values to the nearest integer to ensure that the Bloom filter is implemented with whole bits.</li> </ul> </li> <li> <p>Consider Memory Constraints:</p> <ul> <li>Take into account any memory constraints or limitations imposed by the system or application environment. Ensure that the chosen size is feasible within these constraints.</li> </ul> </li> </ol> <p>Keep in mind that these calculations provide an estimate, and the actual performance may vary based on factors like hash function quality and the distribution of data. Also, choosing an appropriate number of hash functions is crucial; too few may increase the probability of false positives, while too many may increase computational overhead.</p> <p>In summary, calculating the needed size of a Bloom filter involves estimating the number of elements, choosing a false positive rate, and using the formulas to determine the optimal size and number of bits per element. Adjustments may be needed based on specific requirements and constraints.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#persistence","title":"Persistence","text":"<p>A traditional Bloom filter is an in-memory data structure. It doesn't inherently support persistence. There are variations and extensions of the basic Bloom filter concept that aim to provide some level of persistence or durability. These include:</p> <ol> <li> <p>Counting Bloom Filters:    Counting Bloom Filters maintain a counter at each hash position instead of a simple binary value. This allows for removal of items from the filter and enables limited persistence.</p> </li> <li> <p>Bloom Filters with Disk Storage:    Some implementations use external storage, such as disk or a database, to store the Bloom filter. This allows the filter to persist across program executions.</p> </li> <li> <p>Bloom Filters in Databases:    Some databases, especially those designed for high-performance and large-scale data storage, may integrate Bloom filters as part of their indexing or query optimization mechanisms. In such cases, the database itself provides persistence.</p> </li> <li> <p>Disk-Based Bloom Filters:    Techniques like \"Disk-Based Bloom Filters\" involve using a combination of in-memory and on-disk data structures to create a Bloom filter that can handle larger datasets that don't fit entirely in memory.</p> </li> </ol> <p>While these variations may offer a degree of persistence or the ability to handle larger datasets, they often come with trade-offs such as increased complexity, higher storage requirements, or slower performance compared to traditional, in-memory Bloom filters.</p> <p>If persistence is a critical requirement for your use case, you might want to explore alternative data structures or databases designed for persistent storage, such as traditional databases, key-value stores, or other probabilistic data structures that inherently support persistence.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#storing-on-disk","title":"Storing on Disk","text":"<p>Combining in-memory and on-disk data structures to create a Bloom filter involves using a hybrid approach where the core Bloom filter resides in memory for fast lookups, while additional components are stored on disk to handle larger datasets or to provide persistence. This hybrid design is often used when dealing with datasets that exceed available memory.</p> <p>Here's a conceptual overview of how you might combine in-memory and on-disk structures for a Bloom filter:</p> <ol> <li> <p>In-Memory Bloom Filter:</p> <ul> <li>The primary Bloom filter structure is kept in memory for fast and efficient set-membership queries. This in-memory component allows for quick lookups and insertions.</li> </ul> </li> <li> <p>On-Disk Component:</p> <ul> <li>Additional data structures or components, such as a disk-backed storage or a secondary index, are used to handle the larger dataset that cannot fit entirely in memory.</li> <li>This on-disk component might store elements that are not present in the in-memory Bloom filter or maintain a representation of the entire dataset.</li> </ul> </li> <li> <p>Synchronization Mechanism:</p> <ul> <li>There needs to be a mechanism for synchronizing updates between the in-memory Bloom filter and the on-disk component. This ensures that changes made in memory are eventually persisted to disk.</li> </ul> </li> <li> <p>On-Disk Storage Structure:</p> <ul> <li>The on-disk storage structure can vary based on requirements. It might be a traditional database, a key-value store, or a custom file format designed for efficient disk-based lookups.</li> </ul> </li> <li> <p>Loading and Eviction Strategies:</p> <ul> <li>Loading and eviction strategies are implemented to manage the transition of elements between the in-memory Bloom filter and the on-disk component. This includes loading elements into memory when needed and evicting elements to disk to make space for new ones.</li> </ul> </li> <li> <p>Persistence Mechanism:</p> <ul> <li>To ensure durability, a persistence mechanism is required to store the on-disk component. This may involve periodic snapshots, write-ahead logging, or other methods to recover the state in case of failures.</li> </ul> </li> </ol> <p>By combining in-memory and on-disk components, this hybrid design allows for the benefits of fast set-membership queries provided by the Bloom filter in memory, while also addressing the limitations of memory size for large datasets. The approach is often used in scenarios where the dataset is too large to fit entirely in memory but the performance benefits of an in-memory Bloom filter are still desired.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#storing-in-db","title":"Storing in DB","text":"<p>Storing Bloom filters directly in databases is less common because Bloom filters are typically used as auxiliary structures to enhance query performance rather than as primary data storage. However, in certain cases, databases or storage systems may provide facilities for managing and storing Bloom filters. Here are a few examples:</p> <ol> <li> <p>Redis:    Redis, an in-memory data structure store, allows users to create and manage Bloom filters using the <code>BF.ADD</code> and <code>BF.MEXISTS</code> commands. Bloom filters in Redis are explicitly created and managed by the user within the Redis data store.</p> </li> <li> <p>RocksDB:    RocksDB, a high-performance embedded key-value storage engine, allows users to use the <code>rocksdb::FilterPolicy</code> interface to specify custom Bloom filters for their databases. Users can define and configure Bloom filters to be associated with specific column families in RocksDB.</p> </li> <li> <p>LevelDB:    LevelDB, a key-value storage library, provides an interface for users to specify a custom <code>FilterPolicy</code> when opening a database. Users can implement and use their own Bloom filters through the <code>FilterPolicy</code> interface.</p> </li> <li> <p>Apache Cassandra:    Apache Cassandra, a distributed NoSQL database, uses Bloom filters internally as part of its SSTable (Sorted String Table) storage format. While users do not interact directly with these internal Bloom filters, Cassandra provides configuration options related to their usage.</p> </li> </ol> <p>In general, when Bloom filters are used in databases, they are often part of the internal mechanisms used for optimization, indexing, or reducing the need for unnecessary disk reads. Users typically configure and manage Bloom filters indirectly through database-specific interfaces or APIs rather than directly interacting with them as standalone entities.</p> <p>It's worth noting that databases often integrate Bloom filters as part of their internal optimizations, and users may not have direct access or control over these Bloom filters. If you specifically need a data structure that is primarily a Bloom filter and is stored as such, you might consider standalone Bloom filter libraries or custom implementations within your application rather than relying on database-specific features.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#persistent-probabilistic-data-structures","title":"Persistent Probabilistic Data structures","text":"<p>Many probabilistic data structures are designed to be memory-efficient and provide fast approximate answers to various queries. While most of them are inherently designed for memory usage optimization, persistence is typically not a primary feature. However, there are certain variations and extensions of probabilistic data structures or techniques that introduce persistence. Here are a few examples:</p> <ol> <li> <p>Persistent Bloom Filters:    Persistent Bloom Filters are an extension of traditional Bloom filters with a focus on supporting persistence. They allow you to efficiently maintain previous versions of the Bloom filter over time, enabling queries and modifications to be performed on historical states.</p> </li> <li> <p>Count-Min Sketch with Aging:    Count-Min Sketch is a probabilistic data structure used for approximate counting. Introducing an aging mechanism allows you to gradually reduce the influence of older elements, effectively achieving a form of persistence.</p> </li> <li> <p>Counting Bloom Filters with Snapshots:    By periodically creating snapshots of a Counting Bloom Filter and persisting those snapshots, you can achieve a form of persistence. Each snapshot represents a historical state of the Counting Bloom Filter.</p> </li> <li> <p>Time-Decaying Bloom Filters:    Similar to Count-Min Sketch with aging, Time-Decaying Bloom Filters incorporate a decay factor for elements over time, allowing the filter to provide approximate answers to queries for historical periods.</p> </li> <li> <p>Retroactive Data Structures:    Retroactive data structures, though not probabilistic by nature, allow you to efficiently modify the past state of a data structure. By combining a probabilistic data structure with retroactive techniques, you can achieve a form of persistence.</p> </li> </ol> <p>It's important to note that introducing persistence to probabilistic data structures often involves trade-offs. Techniques like snapshots, retroactive modifications, or aging mechanisms may add complexity and memory overhead. Additionally, the persistence mechanisms themselves might impact the probabilistic nature of the data structure.</p> <p>When considering probabilistic data structures with persistence, it's crucial to carefully evaluate the specific requirements of your application, including the desired level of accuracy, memory constraints, and the need for historical data. Depending on your use case, a combination of probabilistic data structures with persistence features or other specialized techniques might be the most suitable solution.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#learn-more","title":"Learn more","text":"<p>To learn more about Bloom filters and deepen your understanding, you can explore various resources ranging from papers and articles to books and practical implementations. Here are some recommended resources:</p> <ol> <li> <p>Original Bloom Filter Paper:</p> <ul> <li>Title: \"Space/Time Trade-offs in Hash Coding with Allowable Errors\"</li> <li>Authors: Burton H. Bloom</li> <li>This is the original paper that introduced the concept of Bloom filters.</li> <li>Link to the paper</li> </ul> </li> <li> <p>Online Articles and Tutorials:</p> <ul> <li>Several online articles and tutorials provide explanations and practical examples of Bloom filters. Search for \"Bloom filter tutorial\" or \"Bloom filter explained\" on platforms like Medium, Towards Data Science, or others.</li> </ul> </li> <li> <p>Books on Algorithms and Data Structures:</p> <ul> <li>Books that cover algorithms and data structures often include sections on Bloom filters. Some recommended books include:<ul> <li>\"Introduction to Algorithms\" by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.</li> <li>\"Data Structures and Algorithms in Python\" by Michael T. Goodrich, Roberto Tamassia, and Michael H. Goldwasser.</li> </ul> </li> </ul> </li> <li> <p>GitHub Repositories:</p> <ul> <li>Explore open-source projects and implementations of Bloom filters. Reading through the code can provide practical insights.</li> <li>Search on GitHub for repositories related to Bloom filters, hashing, or data structures.</li> </ul> </li> <li> <p>Lecture Videos and Online Courses:</p> <ul> <li>Platforms like YouTube, Coursera, edX, and others may have lectures or courses on algorithms and data structures that cover Bloom filters.</li> <li>Search for relevant keywords on these platforms to find educational content.</li> </ul> </li> <li> <p>Documentation of Libraries and Databases:</p> <ul> <li>Documentation of certain libraries and databases may include information on the use of Bloom filters. For example, Redis, Cassandra, and other systems use Bloom filters in their implementations.</li> </ul> </li> </ol> <p>Remember to experiment with Bloom filters through practical implementations. Building a simple Bloom filter and testing its behavior with different parameters will enhance your understanding of how they work and when to use them effectively.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#db-using-bloom-filters","title":"DB using Bloom Filters","text":"<p>Several databases and storage systems integrate Bloom filters as part of their indexing or query optimization mechanisms. Bloom filters are often used to quickly exclude unnecessary lookups, reducing the overall workload and improving query performance. Here are some databases and storage systems that incorporate Bloom filters:</p> <ol> <li> <p>Cassandra:    Apache Cassandra, a distributed NoSQL database, uses Bloom filters to reduce the number of disk seeks during query execution. Bloom filters are employed in Cassandra's SSTable (Sorted String Table) file format to determine whether a particular SSTable might contain data for a given query.</p> </li> <li> <p>HBase:    Apache HBase, a distributed and scalable NoSQL database built on top of Hadoop, uses Bloom filters to speed up lookups in its storage structure. Bloom filters help in skipping unnecessary reads for queries that can be quickly determined to have no matching data.</p> </li> <li> <p>LevelDB and RocksDB:    LevelDB, an embedded key-value storage library, and its fork RocksDB both use Bloom filters to improve the efficiency of key lookups. Bloom filters help avoid unnecessary disk reads by quickly determining the absence of a key.</p> </li> <li> <p>Redis:    Redis, an in-memory data structure store, introduced the HyperLogLog probabilistic data structure, which can be considered an alternative to Bloom filters. HyperLogLog is used for approximate cardinality estimation and set-membership queries.</p> </li> <li> <p>Elasticsearch:    Elasticsearch, a distributed search and analytics engine, uses Bloom filters as part of its inverted index structure. Bloom filters help optimize queries by quickly identifying shards that may not contain relevant data.</p> </li> <li> <p>Couchbase:    Couchbase, a NoSQL database, uses Bloom filters to reduce the number of unnecessary disk reads during query execution. Bloom filters are employed in the storage engine to quickly identify whether a key may exist in a specific data block.</p> </li> <li> <p>Amazon DynamoDB:    DynamoDB, a managed NoSQL database service by Amazon Web Services (AWS), uses Bloom filters to reduce the number of unnecessary read operations. Bloom filters help quickly determine whether an item might exist in a partition.</p> </li> <li> <p>Google Bigtable:    Google Cloud Bigtable, a distributed NoSQL database service, leverages Bloom filters to optimize key lookups. Bloom filters are used to determine whether a specific column family or column qualifier may exist in a given row.</p> </li> </ol> <p>These databases integrate Bloom filters to optimize various aspects of their storage and retrieval mechanisms, resulting in improved query performance and reduced I/O operations. The integration of Bloom filters helps these systems quickly eliminate irrelevant data during query processing, making them suitable for use in scenarios where quick exclusion of non-matching records is crucial.</p>"},{"location":"system-design-fundamentals/nuggets/bloom-filters/#alternatives","title":"Alternatives","text":"<p>While Bloom filters are useful for certain scenarios, there are alternative data structures and techniques that serve similar purposes or address specific limitations of Bloom filters. Here are some Bloom filter alternatives:</p> <ol> <li> <p>Counting Bloom Filter:    A variation of the traditional Bloom filter where each hash function maintains a counter instead of a binary value. This allows removal of elements from the filter and supports counting occurrences.</p> </li> <li> <p>Cuckoo Filter:    Cuckoo Filters are space-efficient data structures that provide similar functionality to Bloom filters but with lower false positive rates. They use a cuckoo hashing scheme, allowing for constant-time lookups and deletions.</p> </li> <li> <p>Quotient Filter:    Quotient Filters are designed to be space-efficient and support set-membership queries. They are based on a combination of hashing and division, allowing for efficient operations with low false positive rates.</p> </li> <li> <p>Bipartite Tables:    Bipartite Tables, like Count-Min Sketch, are used for approximate counting of events in data streams. They offer advantages in terms of space efficiency and performance for certain types of queries.</p> </li> <li> <p>Roaring Bitmaps:    Roaring Bitmaps are compressed bitmap data structures that efficiently represent sets of integers. They are particularly useful for scenarios where the set of elements is dense or when the range of possible values is limited.</p> </li> <li> <p>HyperLogLog:    HyperLogLog is a probabilistic data structure designed for approximating the cardinality (distinct count) of a multiset. It is often used for large-scale cardinality estimation in streaming data scenarios.</p> </li> <li> <p>Bloomier Filters:    Bloomier Filters are a generalization of Bloom filters that provide a mapping from keys to values. They allow for efficient and space-efficient storage of key-value pairs with a similar probabilistic nature.</p> </li> <li> <p>MinHash and HyperMinHash:    MinHash is a technique for estimating the Jaccard similarity between sets. HyperMinHash extends this concept and provides improvements in terms of accuracy and space efficiency.</p> </li> <li> <p>Xor Filters:    Xor Filters are space-efficient filters that use XOR operations to determine membership. They offer advantages in terms of memory usage compared to Bloom filters.</p> </li> <li> <p>Bloom Filters with Delta-Encoding:    This involves combining Bloom filters with delta-encoding techniques to efficiently represent evolving sets, where elements are inserted or deleted over time.</p> </li> </ol> <p>The choice of an alternative to Bloom filters depends on the specific requirements of the application, such as the desired accuracy, memory constraints, and the types of operations needed. Each alternative has its own strengths and weaknesses, and the selection should be based on the characteristics of the data and the use case.</p>"},{"location":"system-design-fundamentals/nuggets/cdn/","title":"CDN","text":"<p>A Content Delivery Network (CDN) is a distributed network of servers strategically positioned to deliver web content, such as images, videos, stylesheets, and scripts, to users based on their geographic location. The primary purpose of a CDN is to enhance the performance, reliability, and scalability of content delivery.</p> <p>It's key functions include:</p> <ol> <li>Content Distribution</li> <li>Reduced Latency</li> <li>Load Balancing</li> <li>Security</li> <li>Scalability</li> <li>Bandwidth Optimization</li> </ol> <p>When a user makes a request for a specific piece of content, the CDN uses <code>intelligent routing algorithms</code> to determine the optimal server to fulfill the request. CDNs <code>cache</code> content on their servers, storing copies of frequently accessed files. This reduces the need to fetch content from the origin server for each user request. CDNs handle both <code>static content</code> and <code>dynamic content</code>.</p> <p>CDNs play a crucial role in enhancing the speed, reliability, and security of web applications, making them a fundamental component for modern online services. Popular CDN providers include <code>Akamai</code>, <code>Cloudflare</code>, and <code>Amazon CloudFront</code>.</p>"},{"location":"system-design-fundamentals/nuggets/consistent-hashing/","title":"Consistent Hashing","text":"<p>Consistent hashing is a distributed hashing technique used in distributed systems to efficiently distribute data among nodes while minimizing the need for rehashing when the number of nodes changes. It was introduced to address the challenges associated with traditional hashing methods in distributed environments, where the number of nodes can vary dynamically.</p> <p>In traditional hashing, data is hashed to determine which node should store or handle that data. However, when the number of nodes changes (nodes being added or removed), most of the hashed values become invalid, leading to the need for remapping and redistribution of data, which can be resource-intensive and time-consuming.</p> <p>Consistent hashing solves this problem by ensuring that the majority of the hash values remain consistent even when the number of nodes changes. Here's how it works:</p> <ol> <li> <p>Hash Space: Imagine a hash space represented as a circle. The output of the hash function maps onto this circle, and each node is assigned a range or segment on this circle.</p> </li> <li> <p>Node Assignment: Nodes are assigned segments of the hash space based on their own hash values. This assignment is done in such a way that the distribution of data is balanced among the nodes.</p> </li> <li> <p>Data Placement: When data needs to be stored or retrieved, it is hashed, and its hash value is mapped onto the hash space. The corresponding node responsible for that segment of the hash space is then responsible for handling that data.</p> </li> <li> <p>Node Additions or Removals: When a node is added or removed, only a small portion of the hash space is affected. The majority of the data remains mapped to the same nodes. This minimizes the need for rehashing and redistributing data.</p> </li> </ol> <p>The benefits of consistent hashing include:</p> <ul> <li> <p>Load Balancing: The distribution of data across nodes tends to be more even, preventing hotspots where certain nodes are overwhelmed with data while others are underutilized.</p> </li> <li> <p>Scalability: The system can easily scale by adding or removing nodes without requiring a complete remapping of data.</p> </li> <li> <p>Efficiency: Consistent hashing reduces the amount of data that needs to be moved or remapped when the number of nodes changes, making it more efficient in dynamic distributed environments.</p> </li> </ul> <p>Consistent hashing is widely used in distributed storage systems, content delivery networks (CDNs), and other distributed applications to achieve a balance between efficient data distribution and ease of scaling.</p>"},{"location":"system-design-fundamentals/nuggets/consistent-hashing/#learning-more","title":"Learning more","text":"<p>Learning about consistent hashing involves understanding the underlying concepts, its applications, and practical implementations. Here are some resources that can help you delve into consistent hashing:</p> <ol> <li>Original Paper:</li> <li>Title: \"Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web\"</li> <li>Authors: David Karger, Eric Lehman, Tom Leighton, Matthew Levine, Daniel Lewin</li> <li>This is the original paper that introduced consistent hashing. Reading the paper will provide you with a deep understanding of the concept.</li> <li> <p>Link to the paper</p> </li> <li> <p>Online Articles and Tutorials:</p> </li> <li>Several online articles and tutorials explain consistent hashing with examples and practical insights. Websites like Medium, Towards Data Science, and others often feature such articles.</li> <li> <p>Search for \"consistent hashing tutorial\" or \"consistent hashing explained\" on your preferred search engine.</p> </li> <li> <p>Distributed Systems Books:</p> </li> <li> <p>Books on distributed systems often cover consistent hashing as part of their content. Some recommended books include:</p> <ul> <li>\"Distributed Systems\" by Andrew S. Tanenbaum and Maarten van Steen.</li> <li>\"Designing Data-Intensive Applications\" by Martin Kleppmann.</li> </ul> </li> <li> <p>GitHub Repositories:</p> </li> <li>Explore open-source projects and implementations that utilize consistent hashing. Reading through the code can provide practical insights.</li> <li> <p>Search on GitHub for repositories related to distributed systems, distributed databases, or consistent hashing.</p> </li> <li> <p>Lecture Videos and Online Courses:</p> </li> <li>Platforms like YouTube, Coursera, edX, and others may have lectures or courses on distributed systems that cover consistent hashing.</li> <li> <p>Search for relevant keywords on these platforms to find educational content.</p> </li> <li> <p>Documentation of Distributed Systems:</p> </li> <li>Documentation of distributed databases and storage systems often includes information on how they use consistent hashing for data distribution.</li> <li>Examples include Apache Cassandra, Amazon DynamoDB, and others.</li> </ol> <p>Remember to supplement your theoretical understanding with practical implementation and experimentation. Building a simple consistent hashing system or exploring existing implementations will deepen your comprehension of the topic.</p>"},{"location":"system-design-fundamentals/nuggets/gossip-protocol/","title":"Gossip protocol","text":"<p>Gossip protocols are a class of distributed communication algorithms used to disseminate information across a network of nodes. These protocols are often employed in large-scale distributed systems, such as peer-to-peer networks, cloud computing environments, and decentralized databases. The primary goal of gossip protocols is to achieve eventual consistency and fault tolerance in a scalable and decentralized manner.</p> <p>Here's a high-level explanation of how a gossip protocol works:</p> <ol> <li>Node Initialization:</li> <li> <p>Each node in the network maintains a local state, which can include information about the node itself and data it wants to share with others.</p> </li> <li> <p>Gossip Rounds:</p> </li> <li> <p>In each round or iteration, a node randomly selects one or more peers to exchange information with. This random selection is a fundamental aspect of gossip protocols.</p> </li> <li> <p>Information Exchange:</p> </li> <li> <p>During a gossip round, the selected nodes exchange information. This information could be updates, state changes, or other relevant data.</p> </li> <li> <p>Propagation:</p> </li> <li> <p>The exchanged information is then disseminated to other nodes through subsequent gossip rounds. This process continues iteratively.</p> </li> <li> <p>Local State Updates:</p> </li> <li>Nodes update their local state based on the received information. This helps maintain a consistent view of the distributed system over time.</li> </ol> <p>The key characteristics of gossip protocols include:</p> <ul> <li> <p>Probabilistic Dissemination: The random selection of nodes ensures that information spreads throughout the network without relying on a central coordinator. This helps achieve fault tolerance and adaptability in dynamic environments.</p> </li> <li> <p>Scalability: Gossip protocols are well-suited for large-scale distributed systems, as the communication load is distributed among nodes, and the protocol naturally scales as the network grows.</p> </li> <li> <p>Eventual Consistency: Gossip protocols aim to ensure that, over time, all nodes converge to a consistent state by continuously exchanging information. However, they don't guarantee instantaneous consistency.</p> </li> <li> <p>Resilience to Node Failures: The random nature of communication helps in maintaining the system's resilience to node failures. If a node goes down, its information can still propagate through other nodes.</p> </li> <li> <p>Low Overhead: Gossip protocols typically have low communication overhead, making them efficient for use in networks with varying latencies.</p> </li> </ul> <p>Gossip protocols are applied in various distributed systems, including distributed databases (e.g., Apache Cassandra), peer-to-peer networks, and decentralized blockchain networks. They provide a decentralized and scalable approach to information dissemination and system coordination in dynamic and fault-tolerant environments.</p>"},{"location":"system-design-fundamentals/nuggets/gossip-protocol/#learn-more","title":"Learn more","text":"<p>To learn more about gossip protocols and deepen your understanding, you can explore a variety of resources including research papers, articles, books, and practical implementations. Here are some recommended resources:</p> <ol> <li>Original Gossip Protocol Papers:</li> <li> <p>There are several papers that have introduced and discussed gossip protocols. A notable one is:</p> <ul> <li>Title: \"Epidemic Algorithms for Replicated Database Maintenance\"</li> <li>Authors: Anne-Marie Kermarrec, Ayalvadi J. Ganesh, Laurent Massoulie, and Christophe Diot</li> <li>Link to the paper</li> </ul> </li> <li> <p>Online Articles and Tutorials:</p> </li> <li> <p>Many online articles and tutorials provide explanations and practical insights into gossip protocols. Search for \"gossip protocol tutorial\" or \"gossip protocol explained\" on platforms like Medium, Towards Data Science, or others.</p> </li> <li> <p>Books on Distributed Systems:</p> </li> <li> <p>Books that cover distributed systems often include sections on gossip protocols. Some recommended books include:</p> <ul> <li>\"Distributed Systems\" by Andrew S. Tanenbaum and Maarten van Steen.</li> <li>\"Designing Data-Intensive Applications\" by Martin Kleppmann.</li> </ul> </li> <li> <p>GitHub Repositories:</p> </li> <li>Explore open-source projects and implementations of gossip protocols. Reading through the code can provide practical insights.</li> <li> <p>Search on GitHub for repositories related to gossip protocols, distributed systems, or decentralized networks.</p> </li> <li> <p>Lecture Videos and Online Courses:</p> </li> <li>Platforms like YouTube, Coursera, edX, and others may have lectures or courses on distributed systems that cover gossip protocols.</li> <li> <p>Search for relevant keywords on these platforms to find educational content.</p> </li> <li> <p>Documentation of Distributed Systems:</p> </li> <li>Documentation of certain distributed systems, especially those that use gossip protocols, may include information on their implementation and usage.</li> <li>Examples include Apache Cassandra, Riak, and other distributed databases.</li> </ol> <p>Remember to experiment with gossip protocols through practical implementations. Building a simple gossip protocol simulator or exploring existing implementations will enhance your understanding of how they work in real-world scenarios.</p>"},{"location":"system-design-fundamentals/nuggets/load-balancer/","title":"Load Balancer","text":"<p>A Load balancer distributes incoming network traffic across multiple servers. Its primary purpose is to enhance the availability and reliability of applications, ensuring that no single server becomes overwhelmed with too much traffic.</p> <p>It's key functions include:</p> <ol> <li>Distribution of Traffic</li> <li>Improving Scalability</li> <li>Enhancing Redundancy</li> <li>Session Persistence</li> <li>Health Monitoring</li> <li>Security</li> </ol> <p>Load balancers can be implemented at various layers of the OSI model, including application layer (Layer 7), transport layer (Layer 4), or network layer (Layer 3), depending on the specific requirements of the application or network architecture.</p> <p>In summary, load balancers play a crucial role in optimizing resource utilization, improving system reliability, and ensuring high availability for applications and services by distributing incoming traffic across multiple servers.</p>"},{"location":"system-design-fundamentals/nuggets/load-balancer/#layer-4-transport-layer-lb","title":"Layer 4 (Transport Layer) LB","text":"<p>A TCP load balancer. Distributes traffic based on TCP protocol information (IP address and port). Example: HAProxy, which balances TCP traffic.</p>"},{"location":"system-design-fundamentals/nuggets/load-balancer/#layer-7-application-layer-lb","title":"Layer 7 (Application Layer) LB","text":"<p>An HTTP load balancer. Makes decisions based on content of the HTTP header, such as URL or cookie information. Example: NGINX, which can balance HTTP/HTTPS traffic.</p>"},{"location":"system-design-fundamentals/nuggets/load-balancer/#layer-4-7-combined-lb","title":"Layer 4-7 Combined LB","text":"<p>An Application Delivery Controller (ADC). Balances traffic based on both network and application layer information. Example: F5 BIG-IP, offering advanced features like SSL termination and content switching.</p>"},{"location":"system-design-fundamentals/nuggets/reverse-proxy/","title":"Reverse Proxy","text":"<p>A reverse proxy is a server that sits between client devices (such as web browsers) and a web server. Unlike a traditional forward proxy, which acts on behalf of clients to access resources from other servers, a reverse proxy handles requests on behalf of servers from clients. It serves as an intermediary between users and the backend servers, providing several benefits</p> <ol> <li>Load Balancing</li> <li>Web Acceleration</li> <li>Security</li> <li>SSL Termination:</li> <li>Compression</li> <li>Application Firewall</li> </ol> <p>The reverse proxy receives the client request and determines the optimal backend server to handle the request based on factors like load balancing algorithms. The reverse proxy receives the response from the backend server, performs any necessary processing (e.g., compression, SSL termination), and then sends the response back to the client.</p> <p>A common use case for reverse proxies is in front of web servers to optimize performance, enhance security, and streamline the management of backend servers. Popular reverse proxy software includes Nginx, Apache HTTP Server, and HAProxy.</p>"},{"location":"system-designs/test/","title":"Test page","text":"Tab 1Tab 2 <p>Markdown content.</p> <p>Multiple paragraphs.</p> <p>More Markdown content.</p> <ul> <li>list item a</li> <li>list item b</li> </ul> <p></p>"},{"location":"text-processing/","title":"Text Processing","text":"<p>NOTE: THIS SECTION NEEDS TO MERGE INTO COMMUNICATION WHEN PARTLY READY. DOESN'T DESERVE IT's OWN SECTION JUST YET.</p> <p>Assumption: Text can be in form of a string or a file. Text does not mean it is readable, and can be in any encoding format.</p> <p>Things we do with text:</p> <ul> <li>encryption</li> <li>encoding</li> <li>compression</li> <li>chunking</li> <li>hashing</li> <li>joining</li> </ul> <p>Note: There is a lot we do we text, but this space is still raw. And I haven't yet decided the scope of the content here.</p>"},{"location":"text-processing/character-encoding-scheme/","title":"Character Encoding Scheme","text":"<p>Most modern character encoding subsets are based on the ASCII character encoding scheme, and support several additional characters.</p>"},{"location":"text-processing/character-encoding-scheme/#references","title":"References","text":"<ul> <li>Character encoding schemes | IBM</li> <li>Character Encoding: What Is It and Why Is It Important?</li> <li>Types of Encoding Techniques | JavaPoint</li> </ul>"},{"location":"text-processing/character-encoding/","title":"Character Encoding","text":"<p>The process of converting data from one form to another is called <code>Encoding</code>. It is used to transform the data so that data can be supported and used by different systems. Encoding works similarly to converting temperature from centigrade to Fahrenheit, as it just gets converted in another form, but the original value always remains the same.</p> <p>Character \"A\" in a way is a graphical image. When you read it, it make sense to you because you map it with a sound you speak and what it means to you.</p> <p>Character encoding is the process of assigning numbers to graphical characters, especially the written characters of human language, allowing them to be stored, transmitted, and transformed using digital computers.</p>"},{"location":"text-processing/character-encoding/#fundamentals","title":"Fundamentals","text":"<p>A <code>bit</code> is either a <code>0</code> or a <code>1</code>. A <code>byte</code> is a set of 8 bits. Characters are stored in a computer as one or more bytes. Each character is mapped to a fixed number, which can be converted into a binary number and stored in a computer. The mapping of the character to the number is stored in ASCII or something else.</p> <p>These characters can be an English character, a Latin letter, a Chinese ideograph, any a character in another language.</p> <p>Characters that are needed for a specific purpose are grouped into a character set (also called a repertoire).</p> <p>There are two concepts of importance here: the <code>character repertoire</code> and the <code>encoding</code>. The term <code>charset</code> has been used for both of these, which can lead to some confusion. The character repertoire is the total set of available characters. For HTML this is defined to be <code>ISO/IEC 10646</code>, which to all intents and purposes is equivalent to Unicode. You cannot change this; it\u2019s built into HTML.</p> <p>What you can vary is the encoding, which is how the characters in a given repertoire are represented numerically, i.e., in a form that computers can understand.</p> <p>Words and sentences in text are created from characters.</p>"},{"location":"text-processing/character-encoding/#character-encoding-scheme-ces","title":"Character Encoding Scheme (CES)","text":"<p>You may have heard of UTF-8 encoding. What is it?</p> <p>Simple Character encoding scheme</p> <ul> <li>UTF-8</li> <li>UTF-16BE</li> <li>UTF-16LE</li> <li>UTF-32BE</li> <li>UTF-32LE</li> </ul> <p>Compound Character encoding scheme</p> <ul> <li>UTF-16</li> <li>UTF-32</li> <li>ISO/IEC 2022</li> </ul>"},{"location":"text-processing/character-encoding/#references","title":"References","text":"<ul> <li>Types of Encoding Techniques</li> </ul>"},{"location":"text-processing/character-set/","title":"Character Set","text":"<ul> <li>US-ASCII</li> <li>ISO-10646</li> <li>ISO-8859-1</li> </ul> <p>Standard Charsets</p> Charset Description US-ASCII Seven-bit ASCII, a.k.a. ISO646-US, a.k.a. the Basic Latin block of the Unicode character set ISO-8859-1 ISO Latin Alphabet No. 1, a.k.a. ISO-LATIN-1 UTF-8 Eight-bit UCS Transformation Format UTF-16BE Sixteen-bit UCS Transformation Format, big-endian byte order UTF-16LE Sixteen-bit UCS Transformation Format, little-endian byte order UTF-16 Sixteen-bit UCS Transformation Format, byte order identified by an optional byte-order mark Paragraph Text"},{"location":"text-processing/character-set/#references","title":"References","text":"<ul> <li>RFC-2978 - IANA Charset Registration Procedures</li> <li>Character Sets | IANA</li> </ul>"},{"location":"text-processing/data-storage/","title":"Data Storage","text":"<p>In the context of computers, data storage is storing information (data) in an electronic storage medium. This information can be human language or electronic data from another computer.</p> <p>Barcodes and magnetic ink character recognition (MICR) are two ways of recording machine-readable data on paper.</p> <p>All data gets transformed into bits, and is stored as bits <code>0</code>s and <code>1</code>s either in RAM or DISK. Decimal number can be easily converted to their binary equivalent. For characters (English and other languages), emojis, etc, there is a collectively agreed mapping to convert these units into specific numeric values, which then gets converted into binary equivalents, and stored.</p> <p>One of the most popular mapping is ASCII. ASCII can support 127 character set. To convert a word like \"HELLO\" in ASCII, you look up the relevant decimal value of each character, convert that value to binary, and concatenate them all together to get the storage ready version. In this case, each character becomes 8-bit or 1 byte of binary data.</p> <p>The process of converting the characters into binary equivalent using the character set is called encoding the string. For decoding, you do everything in reverse.</p> <p>There are over 10,000 characters in the Chinese language. This is where the <code>Unicode</code> standard comes in, which encompses over 100,000 unique characters in over 100 languages, including emojis. Unicode has several encoding strategies with different pros and cons, like UTF-32.</p> <p>The word <code>Character</code> is ambiguous. Use the word <code>Grapheme</code> instead, which is a single unit of human writing system.</p>"},{"location":"text-processing/data-storage/#references","title":"References","text":"<ul> <li>YT - Unicode, in friendly terms: ASCII, UTF-8, code points, character encodings, and more | Studying With Alex</li> <li>YT - ASCII and Unicode Character Sets</li> <li>YT - Character Encoding and Unicode Tutorial</li> <li>What is UTF-8? UTF-8 Character Encoding Tutorial | FreeCodeCamp</li> <li>Charset for Spanish language site<ul> <li>The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)</li> <li>UTF-8 History</li> </ul> </li> <li>Character encodings for beginners | W3C</li> <li>Types of Encoding Techniques | JavaPoint</li> <li>Java 7<ul> <li>https://docs.oracle.com/javase/7/docs/api/java/lang/Character.html#unicode</li> <li>https://docs.oracle.com/javase/7/docs/api/java/nio/charset/Charset.html</li> </ul> </li> <li>Java 9<ul> <li>https://docs.oracle.com/javase/9/docs/api/java/lang/Character.html#unicode</li> <li>https://docs.oracle.com/javase/9/docs/api/java/nio/charset/Charset.html</li> </ul> </li> </ul>"},{"location":"text-processing/human-computer-interaction/","title":"Human-Computer Interaction","text":"<p>Humans speak many different languages. Each of those language have their own unique representation when writing on paper. Each language has a <code>character set</code>, which can help them construct any word they speak and help write it on paper. When this paper is passed on to someone who speaks and reads the same language, they can understand well what that text means.</p> <p>If that text is passed on the someone who does not understand the language, there is no way that text is going to make sense to them.</p> <p>Computers read and write in <code>binary</code> language.</p> <p>To make computers understand what humans speak, and vice versa, <code>character encoding</code> exists. Humans use a dictionary to encoded their language to binary when storing text in the computer, and decode the text stored back, in their language, using the same dictionary, to understand what was stored.</p> <p>This communication wouldn't be possible without this dictionary. If someone speaks multiple languages, they'll need to know which dictionary to use, to be able to encode and decode the communication.</p> <p>Example: ASCII</p> <pre><code>c ---&gt; 67 ---&gt; 1000011\n</code></pre> <p>This dictionary that humans use is what is called the Character Set.</p>"}]}